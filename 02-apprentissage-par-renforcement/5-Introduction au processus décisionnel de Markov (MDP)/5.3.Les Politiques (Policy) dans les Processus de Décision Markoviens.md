# Les Politiques (Policy) dans les Processus de Décision Markoviens (MDP)**

---
## **1 - C'est quoi une Policy ?**
---

Une **policy** (ou politique) est une **stratégie ou une règle** qui détermine **comment un agent doit choisir ses actions** en fonction de l'état dans lequel il se trouve.  

C'est la **méthode principale** qui guide le comportement de l'agent pour **atteindre ses objectifs** dans un environnement donné.  

---

## **2 - Définition de la Policy**
---

### **Policy ($$\pi$$) :**  
- Une policy est une **fonction mathématique** qui associe chaque état à une action ou à un ensemble d'actions possibles.  
- Elle peut être :  
  - **Déterministe :** Chaque état est associé **à une action précise**.  
  - **Stochastique :** Chaque état est associé **à une distribution de probabilités sur les actions possibles**.  

---

## **3 - Optimal Policy ($$\pi^*$$)**  
---

Une **politique optimale** est celle qui **maximise la récompense cumulative attendue** pour l'agent sur le long terme.  
L'objectif est de trouver une **politique optimale ($$\pi^*$$)** qui permet à l'agent d'obtenir le plus de récompenses possibles.  

---

## **4 - Utilité des Policies**  
---

Les politiques sont essentielles car elles permettent :  

- **Guidage :** Savoir quoi faire à chaque état.  
- **Optimisation :** Maximiser les récompenses cumulées.  
- **Cohérence :** Prendre des décisions efficaces au lieu d'agir au hasard.  

---

## **5 - Illustration sur le Labyrinthe**  

---

Le robot doit se déplacer de **l'état 7 (R)** jusqu'à **l'état 25 (★)**.  

```
+----+----+----+----+----+
|  1 |  2 |  3 |  4 |  5 |
+----+----+----+----+----+
|  6 |  R |  8 |  9 | 10 |
+----+----+----+----+----+
| 11 | 12 | 13 | 14 | 15*|
+----+----+----+----+----+
| 16 | 17 | 18 | 19 | 20 |
+----+----+----+----+----+
| 21 | 22 | 23 | 24 | 25★|
+----+----+----+----+----+
```

---

## **6 - Application de deux policies différentes :**
---

### **Politique 1 : Politique Déterministe (Optimale)**
L'agent suit toujours le même chemin pour atteindre la sortie de manière optimale.

#### **Déplacements :**
1. **État 7 → D (Droite)** → **État 8** (Récompense : -1).  
2. **État 8 → D (Droite)** → **État 9** (Récompense : -1).  
3. **État 9 → B (Bas)** → **État 14** (Récompense : -1).  
4. **État 14 → D (Droite)** → **État 15 (Indice précieux)** (Récompense : +50).  
5. **État 15 → B (Bas)** → **État 20** (Récompense : -1).  
6. **État 20 → B (Bas)** → **État 25 (Sortie)** (Récompense : +100).  

---

#### **Total des récompenses obtenues :**  
- Mouvement inutile : **-1 -1 -1 -1 = -4 points**.  
- Indice précieux : **+50 points**.  
- Sortie : **+100 points**.  

**Score total : 50 + 100 - 4 = 146 points.**  
**Conclusion :** Cette politique est très efficace. Elle atteint la sortie avec un bon score en passant par l'indice précieux.  

---

### **Politique 2 : Politique Stochastique (Aléatoire)**  
L'agent choisit ses actions de manière aléatoire à chaque étape, sans stratégie précise.

#### **Déplacements (Exemple simulé) :**
1. **État 7 → H (Haut)** → **Mur (État 2)** (Récompense : -10).  
2. **État 7 → D (Droite)** → **État 8** (Récompense : -1).  
3. **État 8 → H (Haut)** → **Mur (État 3)** (Récompense : -10).  
4. **État 8 → G (Gauche)** → **État 7** (Récompense : -1).  
5. **État 7 → D (Droite)** → **État 8** (Récompense : -1).  
6. **État 8 → D (Droite)** → **État 9** (Récompense : -1).  
7. **État 9 → B (Bas)** → **État 14** (Récompense : -1).  
8. **État 14 → G (Gauche)** → **État 13** (Récompense : -1).  
9. **État 13 → G (Gauche)** → **État 12** (Récompense : -1).  
10. **État 12 → D (Droite)** → **État 13** (Récompense : -1).  
11. **État 13 → D (Droite)** → **État 14** (Récompense : -1).  
12. **État 14 → D (Droite)** → **État 15 (Indice précieux)** (Récompense : +50).  
13. **État 15 → B (Bas)** → **État 20** (Récompense : -1).  
14. **État 20 → B (Bas)** → **État 25 (Sortie)** (Récompense : +100).  

---

#### **Total des récompenses obtenues :**  
- Mouvements inutiles : **-1 -10 -10 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 = -32 points**.  
- Indice précieux : **+50 points**.  
- Sortie : **+100 points**.  

**Score total : 50 + 100 - 32 = 118 points.**  
**Conclusion :** Cette politique aléatoire obtient un score moins bon que la politique optimale. L'agent perd beaucoup de points en se trompant souvent de chemin.  

---

## **7 - Conclusion sur l'Utilité des Policies**  
---

L'exemple montre qu'une **politique bien définie** permet à l'agent d'obtenir une **meilleure récompense cumulée** en évitant les mouvements inutiles et en maximisant ses gains.  

Une **policy optimale ($$\pi^*$$)** est essentielle pour obtenir les meilleurs résultats dans un environnement complexe.  
