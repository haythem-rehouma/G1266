# Concepts de l'utilité en Apprentissage par Renforcement (RL)**

---

## **1 - Qu'est-ce que l'apprentissage par renforcement orienté utilité ?**  
Redéfinissons l'apprentissage par renforcement en introduisant maintenant le **concept de l'utilité**.  

L'apprentissage par renforcement (**Reinforcement Learning - RL**) est une technique d'apprentissage automatique où un **agent** apprend à interagir avec un environnement pour **maximiser une récompense cumulative**.  

Cependant, lorsqu'on parle d'un apprentissage **orienté utilité**, il s'agit pour l'agent de maximiser non seulement les récompenses immédiates, mais surtout la **somme pondérée des récompenses futures**.  

En d'autres termes, l'agent cherche à **maximiser une fonction d'utilité** qui prend en compte **l'impact de ses décisions sur le long terme**.  

> **Citation vulgarisée :**   
> "Imaginez un robot qui explore un labyrinthe. Il ne doit pas seulement penser à gagner quelques points maintenant, mais à accumuler le maximum de points jusqu'à la sortie. Ce n'est pas la récompense immédiate qui compte, mais la somme totale de toutes les récompenses jusqu'à la fin du parcours."  

---

## **2 - Points clés sur l'utilité de l'apprentissage par renforcement**  

### **a. Optimisation des performances**  
- L'objectif principal est d'améliorer les performances de l'agent en **maximisant les récompenses cumulées à long terme**.  
- Un agent orienté utilité ne se contente pas d'une récompense immédiate, il cherche à obtenir la meilleure récompense totale sur une longue période.  

> **Citation vulgarisée :**  
> "Un bon joueur de basket ne cherche pas seulement à marquer un point immédiatement, il réfléchit à comment construire une stratégie qui mènera son équipe à la victoire sur tout le match."  

---

### **b. Prise de décision séquentielle**  
- L'apprentissage par renforcement orienté utilité est particulièrement utile dans les situations où **les décisions doivent être prises de manière séquentielle**.  
- Chaque action influence non seulement la récompense immédiate, mais aussi les **récompenses futures**.  
- Exemple : Dans un jeu vidéo, choisir de prendre un chemin dangereux peut rapporter de gros points si l'agent parvient à survivre.  

> **Citation vulgarisée :**  
> "C'est comme jouer aux échecs : chaque mouvement compte, mais ce qui est vraiment important, c'est de planifier plusieurs coups à l'avance pour remporter la partie."  

---

### **c. Exploration et exploitation**  
- L'agent doit trouver un équilibre entre :  
  - **Exploration :** Tester de nouvelles actions pour découvrir des récompenses potentielles.  
  - **Exploitation :** Choisir les actions connues qui offrent les meilleures récompenses.  
- Un agent orienté utilité doit apprendre à explorer efficacement, tout en exploitant ses connaissances pour maximiser son utilité.  

> **Citation vulgarisée :**  
> "C'est comme être dans une nouvelle ville. Si vous visitez toujours les mêmes restaurants, vous savez que vous allez bien manger, mais vous ratez peut-être quelque chose d'incroyable ailleurs. Parfois, il faut essayer de nouvelles choses pour trouver ce qui est vraiment optimal."  

---

## **3 - Récompense cumulative et Utilité**  
### **a. Valeur cumulative (ou Utilité Totale)**  
En apprentissage par renforcement orienté utilité, l'agent cherche à maximiser une **fonction d'utilité** qui représente la **somme pondérée des récompenses futures**.  

**Formule générale :**  
$$
U(s) = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots
$$  

- **$$U(s)$$** : L'utilité d'un état **$$s$$**.  
- **$$\gamma$$** : Le **facteur d'actualisation** (**0 < $$\gamma$$ < 1**) qui détermine l'importance des récompenses futures.  
- **$$R_t$$** : La récompense obtenue au temps **$$t$$**.  

> **Citation vulgarisée :**  
> "C'est comme investir de l'argent : vous pouvez gagner un petit montant maintenant, ou faire des choix intelligents qui rapporteront beaucoup plus sur le long terme."  

---

### **b. Pourquoi utiliser un facteur d'actualisation ($$\gamma$$) ?**  
- **Réduire l'impact des récompenses futures :** Plus une récompense est lointaine, moins elle a de valeur pour l'agent.  
- **Assurer une valeur cumulative finie :** Sans actualisation, l'utilité pourrait devenir infinie si l'agent continue à obtenir des récompenses.  
- **Favoriser l'atteinte rapide des objectifs :** Plus $$\gamma$$ est petit, plus l'agent privilégie les récompenses immédiates.  

> **Citation vulgarisée :**  
> "Si vous êtes affamé maintenant, un repas qui sera servi dans deux heures n'a pas la même valeur qu'un repas servi tout de suite."  

---

## **4 - Problèmes avec les utilités infinies (Infinite Utilities)**  
L'un des problèmes majeurs de l'apprentissage par renforcement est la possibilité d'obtenir une **utilité infinie** si l'environnement permet d'accumuler des récompenses à l'infini.  

### **a. Problème**  
- Si un agent reste piégé dans une boucle infinie qui rapporte constamment des récompenses positives, la **valeur cumulative devient infinie**.  
- Exemple : Un robot qui tourne en rond en gagnant **1 point** à chaque mouvement.  

> **Citation vulgarisée :**  
> "C'est comme un joueur qui gagne toujours des petites sommes au casino sans jamais quitter. Il accumule de l'argent, mais ne gagne jamais vraiment le gros lot."  

---

### **b. Solutions possibles**  

#### **1. Horizon fini :**  
- Fixer une limite de temps (nombre maximal de pas) après laquelle l'épisode se termine automatiquement.  
- Exemple : Le labyrinthe se termine après **20 mouvements maximums**.  

#### **2. Actualisation (Discounting) :**  
- Utiliser un facteur d’actualisation ($$\gamma$$) pour réduire progressivement l'importance des récompenses futures.  
- Cela assure une valeur cumulative **finie** même si l'environnement est complexe.  

#### **3. État absorbant (Terminal State)**  
- Ajouter un état terminal obligatoire où l’agent doit finir par arriver.  
- Exemple : Dans notre labyrinthe, atteindre l’état **25 (★)** est considéré comme un état terminal.  

> **Citation vulgarisée :**  
> "C'est comme un jeu vidéo où vous devez atteindre un boss final pour terminer le niveau. Sinon, vous pouvez toujours gagner des points, mais vous n'accomplissez jamais vraiment votre mission."  

---

## **5 - Conclusion**  
L'apprentissage par renforcement **orienté utilité** vise à maximiser l'utilité totale d'un agent en prenant en compte **l'ensemble des récompenses futures pondérées par un facteur d'actualisation**.  
- L'objectif est de trouver une **politique optimale ($$\pi^*$$)** qui permet d'obtenir la meilleure utilité possible.  
- Il est essentiel de gérer correctement les **problèmes d'utilités infinies** pour garantir que l'agent apprend efficacement.  
