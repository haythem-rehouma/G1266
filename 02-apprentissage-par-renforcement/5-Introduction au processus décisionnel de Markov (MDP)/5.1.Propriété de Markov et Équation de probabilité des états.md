----------------------------------------------------------
# 1 - MDP (processus de décision markoviens)
----------------------------------------------------------

- Les **processus de décision markoviens** (MDP) sont un cadre mathématique utilisé pour modéliser la prise de décision dans des environnements ***stochastiques***. Ils sont fondamentaux pour l'apprentissage par renforcement. Voici une introduction aux concepts clés des MDP :

----------------------------------------------------------
# 2 - Composants d'un MDP
----------------------------------------------------------

1. **États (S)** : Représentent les différentes situations possibles dans lesquelles l'agent peut se trouver.

2. **Actions (A)** : Ensemble des actions possibles que l'agent peut entreprendre à partir d'un état donné.

3. **Probabilités de transition (P)** : Définissent la probabilité de passer d'un état à un autre après avoir pris une action spécifique. Cela capture la nature stochastique de l'environnement.

4. **Récompenses (R)** : Valeurs reçues après la transition d'un état à un autre, en fonction de l'action entreprise. Elles guident l'agent vers les comportements souhaités.

5. **Politique ($$\pi$$)** : Stratégie que l'agent utilise pour choisir ses actions en fonction des états. L'objectif est de trouver la politique optimale $$\pi^*$$ qui maximise la récompense cumulative attendue.

----------------------------------------------------------
# 3 - Propriété de Markov
----------------------------------------------------------

La propriété de Markov stipule que le futur état dépend uniquement de l'état actuel et de l'action choisie, et non des états ou actions précédents. Cela simplifie le processus de modélisation en ne nécessitant que l'état courant pour prendre des décisions.

## **Exemple d'application**

Imaginez un robot naviguant dans un labyrinthe :

- **États** : Chaque position possible dans le labyrinthe.
- **Actions** : Se déplacer vers le haut, le bas, la gauche ou la droite.
- **Récompenses** : Points positifs pour atteindre la sortie, pénalités pour heurter un mur.
- **Transition** : Probabilité que le robot se déplace avec succès dans la direction choisie.

En explorant le labyrinthe et en recevant des récompenses ou pénalités, le robot apprend progressivement la meilleure stratégie pour atteindre son objectif, illustrant ainsi l'application pratique des MDP dans l'apprentissage par renforcement.

Les MDP sont essentiels pour résoudre des problèmes où les décisions doivent être prises dans des environnements incertains et dynamiques, comme les véhicules autonomes ou les systèmes de recommandation.



-------------------------------------------------------
# 4 - *Propriété de Markov*
-------------------------------------------------------

- Dans un processus de décision markovien (MDP), la **propriété de Markov** est essentielle.
- Elle stipule que ***la probabilité de transition vers un nouvel état dépend uniquement de l'état actuel et de l'action entreprise, et non des états précédents***. Cela simplifie l'analyse et la modélisation des systèmes dynamiques.
- Pour vilgariser, on peut dire que la propriété de Markov stipule que ***le futur état dépend uniquement de l'état actuel et de l'action choisie, et non des états ou actions précédents. Cela simplifie le processus de modélisation en ne nécessitant que l'état courant pour prendre des décisions***.

-------------------------------------------------------
# 5 - **Équation de probabilité des états**
-------------------------------------------------------

![image](https://github.com/user-attachments/assets/d15de363-7c21-49d8-a87a-3aa7fd941eac)



L'équation de probabilité pour la transition entre les états dans un MDP est généralement formulée comme suit :

$$
P(s' | s, a)
$$

où :
- $$s$$ est l'état actuel.
- $$a$$ est l'action entreprise.
- $$s'$$ est le nouvel état après la transition.
- $$P(s' | s, a)$$ est la probabilité de passer à l'état $$s'$$ en prenant l'action $$a$$ depuis l'état $$s$$.

Cette équation illustre comment les transitions d'états sont déterminées dans un environnement stochastique, en se basant uniquement sur l'état actuel et l'action choisie, conformément à la propriété de Markov.
