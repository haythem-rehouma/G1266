# Propriété de Markov et Équation de probabilité des états
---

# 1 - MDP (Processus de Décision Markoviens)
----------------------------------------------------------

Les **processus de décision markoviens (MDP)** sont un **cadre mathématique puissant** utilisé pour modéliser des situations où un **agent** doit prendre des décisions **séquentielles** dans un environnement incertain ou **stochastique**.  

Les MDP sont au cœur de l'**apprentissage par renforcement**, où un agent apprend **par essais et erreurs** en interagissant avec son environnement pour **maximiser une récompense cumulée au fil du temps**.  

> **Exemple :**  
> Imaginez un jeu vidéo où un personnage doit traverser une série de pièces pour atteindre un trésor caché.  
> À chaque pièce, il peut choisir d'aller à gauche, à droite, en haut ou en bas.  
> Certaines pièces contiennent des pièges qui lui retirent des points, tandis que d'autres contiennent des indices précieux qui lui en donnent.  
> Le but est de **trouver le meilleur chemin** qui lui rapporte le plus de points en un minimum de déplacements.  
> C'est exactement ce que les MDP tentent de modéliser.  

---

# 2 - Composants d'un MDP
----------------------------------------------------------

Un MDP est défini par cinq éléments principaux que nous allons détailler un par un avec des exemples concrets.  

### 2.1 - **États (S)**  
Les **États** représentent toutes les situations possibles dans lesquelles l’agent peut se trouver.  
Chaque état contient toutes les informations nécessaires pour prendre une décision.  

> **Exemple :**  
> Dans notre jeu vidéo, chaque pièce où le personnage se trouve est un **état**.  
> Si le personnage est actuellement dans la pièce numéro 5, c'est son état actuel.  

---

### 2.2 - **Actions (A)**  
Les **Actions** sont les choix disponibles pour l’agent à partir d'un état donné.  
Chaque action entreprise par l'agent influence son état futur.  

> **Exemple :**  
> Dans le jeu vidéo, les actions possibles sont :  
> - Aller en haut  
> - Aller en bas  
> - Aller à gauche  
> - Aller à droite  
> **L'agent doit choisir judicieusement parmi ces actions pour progresser vers le trésor.**  

---

### 2.3 - **Probabilités de transition (P)**  
Les **Probabilités de transition** déterminent la probabilité de passer d'un état actuel à un nouvel état après avoir pris une action.  
Cela capture l'incertitude de l'environnement : **L'action entreprise ne garantit pas toujours le résultat attendu.**  

> **Exemple :**  
> Imaginons un robot qui se déplace dans un labyrinthe.  
> **Étant donné que le robot est dans la pièce numéro 7 et décide de prendre l'action "Aller en haut" :**  
> - Il a **80 % de chances** d'arriver dans la pièce numéro 8 qui est au-dessus.  
> - Il a **20 % de chances** de glisser et se retrouver dans une pièce différente (par exemple, la pièce numéro 6 qui est à gauche).  
> Ces probabilités permettent de modéliser les erreurs possibles dans les actions prises par l'agent.  

---

### 2.4 - **Récompenses (R)**  
Les **Récompenses** sont des valeurs numériques attribuées après chaque transition. Elles indiquent si l'action entreprise est bénéfique ou non.  
Les récompenses sont les **indices principaux qui permettent à l'agent d'apprendre à faire de bons choix.**  

> **Exemple :**  
> Imaginons que notre robot navigue toujours dans le labyrinthe.  
> **Étant donné qu'il est dans la pièce numéro 8 :**  
> - **S'il atteint la sortie en montant encore vers le haut : +100 points.**  
> - **S'il heurte un mur en tentant d'aller à gauche : -10 points.**  
> - **S'il décide de marcher sans objectif précis : -1 point par mouvement.**  
> - **S'il trouve un indice précieux en allant à droite : +50 points.**  
> Ces récompenses permettent au robot de comprendre quelles actions sont bénéfiques et lesquelles sont inutiles ou nuisibles.  

---

### 2.5 - **Politique ($$\pi$$)**  
La **Politique** est la stratégie que l'agent adopte pour choisir ses actions en fonction de l'état dans lequel il se trouve.  
L'objectif est de trouver la **politique optimale** $$\pi^*$$ qui maximise les récompenses cumulées sur le long terme.  

> **Exemple :**  
> La politique optimale pour notre robot consiste à trouver **le chemin le plus sûr et le plus rapide** pour atteindre la sortie, tout en évitant les pièges et en minimisant les déplacements inutiles.  
> **La politique optimale pourrait dire : "Étant donné que le robot est dans la pièce numéro 7, il devrait aller vers le haut car c'est l'action qui lui rapporte le plus de récompenses."**  

---

# 3 - Propriété de Markov
----------------------------------------------------------

La **Propriété de Markov** stipule que **l'avenir dépend uniquement de l'état actuel et de l'action prise, et non de la manière dont cet état a été atteint**.  

> **Exemple :**  
> **Étant donné que le robot est dans la pièce numéro 7 et décide d'aller en haut :**  
> - Le résultat de cette action dépend uniquement de **l'endroit où il est maintenant (pièce numéro 7)** et de **l'action entreprise (monter)**.  
> - Pas besoin de savoir comment il est arrivé dans la pièce numéro 7.  

---

# 4 - Équation de probabilité des états
----------------------------------------------------------

![image](https://github.com/user-attachments/assets/7bf9784d-9466-4be5-9a71-9c74d1153d1c)


L'équation qui décrit la transition entre les états est la suivante :  


$$
P(s' | s, a)
$$  

**Interprétation :**  
- **$$s$$** : État actuel (par exemple, pièce numéro 7).  
- **$$a$$** : Action entreprise (par exemple, monter).  
- **$$s'$$** : Nouvel état après la transition (par exemple, pièce numéro 8).  
- **$$P(s' | s, a)$$** : Probabilité que l'agent passe à l'état **$$s'$$** en prenant l'action **$$a$$** depuis l'état **$$s$$**.  

> **Exemple précis :**  
> **Étant donné que le robot est dans la pièce numéro 7 et décide d'aller en haut (a) :**  
> - **$$P(8 | 7, a) = 0.8$$** signifie qu'il a **80 % de chances d'arriver dans la pièce numéro 8**.  
> - **$$P(6 | 7, a) = 0.2$$** signifie qu'il a **20 % de chances de glisser et arriver dans la pièce numéro 6 par erreur**.  

---

# 5 - Conclusion
----------------------------------------------------------

Les **MDP** permettent de modéliser des environnements où les actions ne produisent pas toujours les résultats attendus.  
Comprendre ces concepts est essentiel pour créer des systèmes capables d'apprendre à partir de leurs erreurs et à maximiser leurs récompenses.  



---
# 6 - Annexe : Exemple de Labyrinthe
---

##  **Illustration :**

```
+----+----+----+----+----+
|  1 |  2 |  3 |  4 |  5 |
+----+----+----+----+----+
|  6 |  7 |  8 |  9 | 10 |
+----+----+----+----+----+
| 11 | 12 | 13 | 14 | 15 |
+----+----+----+----+----+
| 16 | 17 | 18 | 19 | 20 |
+----+----+----+----+----+
| 21 | 22 | 23 | 24 | 25 |
+----+----+----+----+----+
```

---



## 1. **Les États (S) :**  
Chaque pièce numérotée est un **état unique**. Par exemple :  
- **État 7** représente la pièce située en **deuxième ligne, deuxième colonne**.  
- **État 25** représente la **sortie** située en **dernière ligne, dernière colonne**.  



## 2. **Les Actions (A) :**  
Depuis chaque pièce, le robot peut choisir parmi **quatre actions principales** :  
- **Aller en haut (H)**  
- **Aller en bas (B)**  
- **Aller à gauche (G)**  
- **Aller à droite (D)**  

Exemple :  
- Depuis l’état **7**, si le robot choisit l’action **H (monter vers le haut)**, il essaiera d’atteindre l’état **2**.  



## 3. **Les Probabilités de Transition (P) :**  
Étant donné qu'il est dans un état particulier et qu'il prend une action spécifique, le résultat est **probabiliste** :  

Imaginons que le robot est dans l'état **7** et qu'il décide de :  
- **Prendre l'action H (monter vers le haut) :**  
  - **80 % de chances** d'arriver en **état 2** (action réussie).  
  - **10 % de chances** de glisser à gauche et arriver en **état 1**.  
  - **10 % de chances** de glisser à droite et arriver en **état 3**.  



## 4. **Les Récompenses (R) :**  
Chaque transition d’un état à un autre produit une récompense, qui aide le robot à apprendre.  
Voici comment les récompenses sont attribuées :  

- **Atteindre la sortie (état 25)** : **+100 points**.  
- **Heurter un mur (par exemple essayer d'aller en haut depuis l'état 2)** : **-10 points**.  
- **Se déplacer inutilement** : **-1 point par mouvement**.  
- **Découvrir un indice précieux (par exemple, l'état 15)** : **+50 points**.  



## 5. **Politique ($$\pi$$) :**  
La politique est la stratégie que le robot apprend pour se déplacer de manière optimale.  
**Exemple :**  
- **Étant donné qu'il est dans l'état 7**, il doit choisir l'action qui maximise sa récompense future.  
- **Si l'objectif est d'atteindre l'état 25**, la meilleure politique pourrait être : **“Aller à droite (D), puis descendre (B) jusqu'à atteindre l'état 25.”**  



## 6.**Visualisation du Déplacement Exemple :**
Le robot commence en **état 7**. Il veut atteindre **l'état 25** (sortie).

1. **Étant donné qu'il est dans l'état 7**, il choisit l'action **D (aller à droite)**.  
   - **80 % de chances d'arriver en état 8.**  
   - **10 % de chances de rester en état 7 (glisser).**  
   - **10 % de chances d'arriver en état 6 (glisser en arrière).**  
   - Récompense : **-1 point (mouvement inutile)**.  

2. **Étant donné qu'il est maintenant en état 8**, il choisit l'action **D (aller à droite)**.  
   - **80 % de chances d'arriver en état 9.**  
   - **10 % de chances d’arriver en état 7.**  
   - **10 % de chances d’arriver en état 10.**  
   - Récompense : **-1 point (mouvement inutile)**.  

3. **Étant donné qu'il est en état 9**, il choisit l'action **B (aller en bas)**.  
   - **80 % de chances d'arriver en état 14.**  
   - **10 % de chances d’arriver en état 8.**  
   - **10 % de chances d’arriver en état 10.**  
   - Récompense : **-1 point (mouvement inutile)**.  

4. **Enfin, il décide d'aller à droite depuis l'état 14 (action D)** et poursuit sa stratégie jusqu'à atteindre l'état **25 (sortie)**.  
   - **Récompense finale : +100 points**.  


## 7.**Résumé :**  
Le robot doit **explorer** le labyrinthe, **apprendre de ses erreurs**, et **trouver la politique optimale ($$\pi$$)** qui maximise ses récompenses cumulées. Chaque état, chaque action, chaque récompense sont **pris en compte pour améliorer sa stratégie**.  



