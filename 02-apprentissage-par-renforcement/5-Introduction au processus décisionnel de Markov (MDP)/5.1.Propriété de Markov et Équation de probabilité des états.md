# Propriété de Markov et Équation de probabilité des états

---

# 1 - MDP (Processus de Décision Markoviens)
----------------------------------------------------------

Les **processus de décision markoviens (MDP)** sont un **cadre mathématique puissant** utilisé pour modéliser des situations où un **agent** doit prendre des décisions **séquentielles** dans un environnement incertain ou **stochastique**.  

Les MDP sont au cœur de l'**apprentissage par renforcement** où un agent apprend **par essais et erreurs** en interagissant avec son environnement pour **maximiser une récompense cumulée au fil du temps**.  

---

# 2 - Composants d'un MDP
----------------------------------------------------------

Un MDP est défini par cinq éléments principaux que nous allons détailler un par un avec des exemples concrets.  

### 2.1 - **États (S)**  
Les **États** représentent toutes les situations possibles dans lesquelles l’agent peut se trouver.  
**Exemple** : Pour un robot qui navigue dans un labyrinthe, chaque position dans le labyrinthe est un état.  

### 2.2 - **Actions (A)**  
Les **Actions** sont les choix disponibles pour l’agent à partir d'un état donné.  
**Exemple** : Pour le robot, cela peut être :  
- Aller en haut  
- Aller en bas  
- Aller à gauche  
- Aller à droite  

### 2.3 - **Probabilités de transition (P)**  
Les **Probabilités de transition** déterminent la probabilité de passer d'un état actuel à un nouvel état après avoir pris une action.  
Cela capture l'incertitude de l'environnement.  
**Exemple** : Si le robot tente d'aller à droite, il réussit 80 % du temps. Mais 20 % du temps, il peut glisser ou être dévié vers une autre direction.  

### 2.4 - **Récompenses (R)**  
Les **Récompenses** sont des valeurs numériques attribuées après chaque transition. Elles indiquent si l'action entreprise est bénéfique ou non.  
**Exemple** :  
- Atteindre la sortie du labyrinthe : +100 points.  
- Heurter un mur : -10 points.  
- Se déplacer inutilement : -1 point par mouvement.  

### 2.5 - **Politique ($$\pi$$)**  
La **Politique** est la stratégie que l'agent adopte pour choisir ses actions en fonction de l'état dans lequel il se trouve.  
**L'objectif principal** est de trouver la **politique optimale** $$\pi^*$$ qui maximise les récompenses cumulées sur le long terme.  
**Exemple** : Pour notre robot, une politique optimale est celle qui lui permet de trouver la sortie du labyrinthe avec le moins de mouvements possibles et en évitant les pénalités.  

---

# 3 - Exemple d'application de la propriété de Markov
----------------------------------------------------------

### **Propriété de Markov : Qu'est-ce que c'est ?**  
La **propriété de Markov** stipule que **l'avenir dépend uniquement du présent, et non du passé**.  
En d'autres termes, si l'on connaît l'état actuel et l'action choisie, on peut prédire ce qui va se passer **sans avoir besoin de connaître l'historique précédent**.  

### **Pourquoi est-ce utile ?**  
Cela simplifie considérablement la modélisation des problèmes complexes, car il suffit de prendre en compte **l'état actuel** au lieu de garder une trace de toutes les actions passées.  

### **Exemple d'application : Le robot dans le labyrinthe**  
Imaginez un robot qui se déplace dans un labyrinthe pour atteindre une sortie.  

- **États (S)** : Chaque position du labyrinthe.  
- **Actions (A)** : Haut, bas, gauche, droite.  
- **Récompenses (R)** :  
  - +100 points pour atteindre la sortie.  
  - -10 points pour heurter un mur.  
  - -1 point pour chaque mouvement inutile.  
- **Probabilités de transition (P)** :  
  - Le robot a 80 % de chances de se déplacer correctement.  
  - 20 % de chances de dévier en raison d'obstacles ou d'erreurs de mouvement.  

Le robot n'a pas besoin de savoir **comment il est arrivé dans une position particulière** ; il doit juste connaître sa position actuelle pour décider de l'action suivante.  

---

# 4 - Propriété de Markov
----------------------------------------------------------

En résumé, la **Propriété de Markov** indique que :  

- La **probabilité de transition vers un nouvel état** dépend uniquement de l'état actuel et de l'action entreprise, et **non des états précédents**.  
- Cela simplifie l'analyse en réduisant la complexité du modèle.  

Pour vulgariser :   
**"Pour savoir ce qui va se passer, regarde seulement où tu es maintenant et ce que tu fais. Pas besoin de te rappeler tout ce qui s'est passé avant."**  

---

# 5 - Équation de probabilité des états
----------------------------------------------------------

L'équation qui décrit la transition entre les états est la suivante :  

$$
P(s' | s, a)
$$  

**Interprétation :**  
- **$$s$$** : État actuel.  
- **$$a$$** : Action entreprise.  
- **$$s'$$** : Nouvel état après la transition.  
- **$$P(s' | s, a)$$** : Probabilité que l'agent passe à l'état **$$s'$$** en prenant l'action **$$a$$** depuis l'état **$$s$$**.  

**Exemple pratique :**  
Si le robot est dans la position (2,3) du labyrinthe et décide d’aller à droite (**Action : aller à droite**).  
- **$$P(s' | s, a) = 0.8$$** signifie qu'il a 80 % de chances d'arriver dans l'état voulu (3,3).  
- **$$P(s' | s, a) = 0.2$$** signifie qu'il peut se retrouver dans un état adjacent non désiré en raison d'une erreur.  

Cette équation illustre l'idée que l'environnement est incertain, mais que chaque action a une probabilité connue d'amener l'agent à un certain état.  

---

# 6 - Conclusion
----------------------------------------------------------

Les **MDP** sont un outil essentiel pour modéliser des problèmes complexes où l'incertitude est présente. Ils sont particulièrement utiles dans les domaines tels que :  
- La robotique (navigation autonome).  
- Les jeux vidéo (stratégies d'IA).  
- Les systèmes de recommandation.  
- Les véhicules autonomes.  

Comprendre ces concepts est fondamental pour tout domaine où des décisions doivent être prises de manière optimale dans un environnement incertain.  


