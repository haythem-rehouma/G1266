

# 1 - MDP (Processus de Décision Markoviens)
----------------------------------------------------------

Les **processus de décision markoviens (MDP)** sont un cadre mathématique essentiel pour modéliser la prise de décision dans des environnements incertains ou ***stochastiques***. Ils sont largement utilisés dans le domaine de l'**apprentissage par renforcement**, où un agent apprend à prendre des décisions optimales en interagissant avec son environnement.  

L'objectif principal d'un MDP est de fournir une méthode rigoureuse pour trouver une stratégie optimale permettant de maximiser une récompense cumulative attendue sur une période donnée.

---

# 2 - Composants d'un MDP
----------------------------------------------------------

Un MDP est défini par les cinq composants suivants :  

1. **États (S)** :  
   Représentent les différentes situations possibles dans lesquelles l'agent peut se trouver. Chaque état fournit toutes les informations nécessaires pour décider de l'action à entreprendre.  
   - Exemple : La position d'un robot dans un labyrinthe.

2. **Actions (A)** :  
   Ensemble des actions possibles que l'agent peut entreprendre à partir d'un état donné.  
   - Exemple : Se déplacer vers le haut, le bas, la gauche ou la droite dans un environnement.

3. **Probabilités de transition (P)** :  
   Définit la probabilité de passer d'un état à un autre après avoir pris une action spécifique.  
   Cela capture la nature stochastique de l'environnement en modélisant l'incertitude sur les résultats des actions.  
   - Exemple : Si un robot essaye de se déplacer vers la droite, il peut y parvenir avec une probabilité de 80 % et échouer en allant dans une autre direction avec une probabilité de 20 %.

4. **Récompenses (R)** :  
   Valeurs reçues suite à une transition d'un état à un autre en fonction de l'action entreprise.  
   Elles servent de signal d'apprentissage pour guider l'agent vers les comportements souhaités.  
   - Exemple : Recevoir une récompense positive en atteignant un objectif, ou une pénalité pour heurter un obstacle.

5. **Politique ($$\pi$$)** :  
   La politique est la stratégie que l'agent utilise pour choisir ses actions en fonction des états.  
   L'objectif est de trouver la **politique optimale** $$\pi^*$$ qui maximise la récompense cumulative attendue sur le long terme.  
   - Exemple : Pour un robot dans un labyrinthe, la politique optimale correspond à la série d'actions qui le mène le plus efficacement vers la sortie.  

---

# 3 - Exemple d'application de la propriété de Markov
----------------------------------------------------------

La **propriété de Markov** stipule que **l'état futur dépend uniquement de l'état actuel et de l'action choisie, et non de la manière dont l'état actuel a été atteint**.  

Cela simplifie considérablement la modélisation des systèmes dynamiques, car il suffit de connaître l'état courant pour prendre une décision.  

### Exemple d'application : Navigation d'un robot dans un labyrinthe  

Imaginons un robot qui doit se déplacer dans un labyrinthe pour atteindre une sortie.  

- **États (S)** : Chaque position possible dans le labyrinthe.  
- **Actions (A)** : Déplacements possibles vers le haut, le bas, la gauche ou la droite.  
- **Récompenses (R)** :  
  - Points positifs pour atteindre la sortie.  
  - Pénalités pour heurter un mur.  
- **Transitions (P)** :  
  - Probabilité que le robot se déplace avec succès dans la direction choisie (par exemple, 80 % de chance de succès, 20 % de chance de dévier).

Le robot explore le labyrinthe, apprend progressivement la meilleure stratégie pour atteindre son objectif en optimisant les récompenses reçues. Cela illustre comment les MDP peuvent être appliqués à des problèmes de prise de décision en environnement incertain.  

Les MDP sont particulièrement utiles dans des domaines comme les véhicules autonomes, les jeux vidéo, et les systèmes de recommandation où l'incertitude est un facteur majeur.

---

# 4 - Propriété de Markov
----------------------------------------------------------

Dans un processus de décision markovien (MDP), la **propriété de Markov** est un concept essentiel qui permet de simplifier l'analyse et la modélisation des systèmes dynamiques.  

Elle stipule que :  
- **La probabilité de transition vers un nouvel état dépend uniquement de l'état actuel et de l'action entreprise, et non des états précédents.**  
- En d'autres termes, l'historique complet n'est pas nécessaire pour prédire l'évolution du système ; seul l'état courant est pertinent.  

Pour vulgariser, on pourrait dire que **"le futur dépend uniquement du présent, pas du passé"**.  

---

# 5 - Équation de probabilité des états
----------------------------------------------------------

L'équation de probabilité qui décrit la transition entre les états dans un MDP est la suivante :  

$$
P(s' | s, a)
$$  

où :  
- **$$s$$** est l'état actuel.  
- **$$a$$** est l'action entreprise par l'agent.  
- **$$s'$$** est le nouvel état atteint après la transition.  
- **$$P(s' | s, a)$$** est la probabilité de passer à l'état **$$s'$$** en prenant l'action **$$a$$** depuis l'état **$$s$$**.  

Cette équation traduit l'incertitude présente dans un environnement stochastique. Elle démontre que le passage d'un état à un autre est déterminé par une probabilité qui dépend uniquement de l'état actuel et de l'action choisie, conformément à la propriété de Markov.  

