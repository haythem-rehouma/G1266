### **Quiz : Propri√©t√© de Markov et √âquation de probabilit√© des √©tats (10 Questions)**

---

# 1 - **Introduction et instructions pr√©l√©minaires**
---

Ci-bas, vous trouvez un exemple de labyrinthe simplifi√© o√π un robot doit atteindre la sortie (**‚òÖ**) en √©vitant de perdre trop de points.  

```
+----+----+----+----+----+
|  1 |  2 |  3 |  4 |  5 |
+----+----+----+----+----+
|  6 |  7 |  8 |  9 | 10 |
+----+----+----+----+----+
| 11 | 12 | 13 | 14 | 15 |
+----+----+----+----+----+
| 16 | 17 | 18 | 19 | 20 |
+----+----+----+----+----+
| 21 | 22 | 23 | 24 | 25‚òÖ|
+----+----+----+----+----+
```

---

# 2 - **R√®gles du labyrinthe :**
---

1. **Chaque pi√®ce est un √©tat (S).**  
2. **Le robot peut se d√©placer :**  
   - **H (Haut)**  
   - **B (Bas)**  
   - **G (Gauche)**  
   - **D (Droite)**  
3. **R√©compenses :**  
   - Atteindre la sortie (√©tat **25**) : **+100 points.**  
   - Heurter un mur (mouvement impossible) : **-10 points.**  
   - Se d√©placer inutilement : **-1 point par mouvement.**  
4. **Propri√©t√© de Markov :**  
   - Le futur d√©pend uniquement de **l'√©tat actuel** et de **l'action choisie**.  

---

# 3 - **Questions :**

---

#### **Question 1 : Compr√©hension de base (Vrai ou Faux)**  
**La Propri√©t√© de Markov stipule que l'avenir d√©pend √† la fois de l'√©tat actuel, de l'action prise, et des √©tats pr√©c√©dents.**  
- A) Vrai  
- B) Faux  

---

#### **Question 2 : D√©finition d'un √©tat (Choix multiple)**  
Un **√©tat (S)** dans un MDP repr√©sente :  
- A) Une action que l'agent peut entreprendre.  
- B) La strat√©gie optimale pour atteindre un objectif.  
- C) Une situation possible dans laquelle l‚Äôagent se trouve.  
- D) La r√©compense obtenue apr√®s chaque action.  

---

#### **Question 3 : D√©finition d'une politique (Choix multiple)**  
La politique **($$\pi$$)** est :  
- A) Une fonction qui d√©termine l'action optimale √† prendre pour chaque √©tat.  
- B) Un ensemble de r√©compenses cumul√©es.  
- C) La probabilit√© de transition entre deux √©tats.  
- D) Une formule pour calculer la r√©compense imm√©diate.  

---

#### **Question 4 : Probabilit√© de transition (Question √† trous)**  
Si un robot est dans l'√©tat **7** et d√©cide de prendre l'action **H (monter vers le haut)**, avec une probabilit√© de **0.8** d'arriver en √©tat **2** et **0.2** d'arriver en √©tat **1**, alors :  

- $$P(2 | 7, H) = \_\_\_\_$$
  
- $$P(1 | 7, H) = \_\_\_\_$$  




Ce message d'erreur est d√ª √† un probl√®me de formatage du LaTeX. Il manque une parenth√®se ouvrante apr√®s l'indicateur d'√©tat dans l'√©quation. Voici comment corriger cela :

---

### ‚úÖ **Correction des √©quations de probabilit√© de transition :**

#### **Question 4 : Probabilit√© de transition (Question √† trous)**  
Si un robot est dans l'√©tat **7** et d√©cide de prendre l'action **H (monter vers le haut)**, avec une probabilit√© de **0.8** d'arriver en √©tat **2** et **0.2** d'arriver en √©tat **1**, alors :  

- $$P(2 \mid 7, H) = \_\_\_\_$$  
- $$P(1 \mid 7, H) = \_\_\_\_$$  

---

### **Pourquoi cette correction ?**  
- **\mid** est la bonne syntaxe pour afficher la barre verticale indiquant une condition (au lieu de l'utilisation incorrecte de la barre verticale simple **|**).  
- La notation correcte est donc : $$P(2 \mid 7, H) = 0.8$$ et $$P(1 \mid 7, H) = 0.2$$.  

---

Est-ce que tu veux que je r√©vise **toutes les notations LaTeX** dans ton cours pour m'assurer qu'il n'y a plus d'erreurs ?  

Aussi, veux-tu que j'int√®gre ces modifications directement dans ton module Thinkific ? üòä

---

#### **Question 5 : R√©compense imm√©diate (Question ouverte)**  
Si le robot tente de se d√©placer vers la gauche depuis **l'√©tat 1**, quelle sera la r√©compense obtenue ? Expliquez pourquoi.  

---

#### **Question 6 : Propri√©t√© de Markov (Choix multiple)**  
La Propri√©t√© de Markov signifie que :  
- A) L'avenir d√©pend uniquement de l'√©tat actuel et de l'action prise.  
- B) L'avenir d√©pend uniquement des actions pass√©es.  
- C) L'avenir d√©pend de tous les √©tats pr√©c√©dents.  
- D) L'avenir ne d√©pend que des r√©compenses imm√©diates.  

---

#### **Question 7 : R√©compense cumulative (Question √† trous)**  
Pour un agent qui re√ßoit les r√©compenses suivantes : **-1, -1, +50, -1, +100**, sa r√©compense cumulative est de **\_\_\_\_**.  

---

#### **Question 8 : Calcul de l'utilit√© (Question de calcul)**  
Si le robot applique un facteur d‚Äôactualisation ($$\gamma = 0.9$$), quelle est l‚Äôutilit√© totale obtenue avec les r√©compenses suivantes : **-1, -1, +50, -1, +100** ?  
- Formule :  
$$U = -1 + 0.9(-1) + 0.9^2(50) + 0.9^3(-1) + 0.9^4(100)$$  

---

#### **Question 9 : Politique optimale (Question ouverte)**  
Quelle est la meilleure politique pour atteindre l'√©tat **25 (‚òÖ)** √† partir de l'√©tat **7** ? Justifiez votre r√©ponse.  

---

#### **Question 10 : Transition probabiliste (Question ouverte)**  
Si l‚Äôagent est dans l‚Äô√©tat **8** et d√©cide d'aller √† droite **(D)**, quelles sont les cons√©quences probables si :  

- $$P(9 | 8, D) = 0.8$$  
- $$P(7 | 8, D) = 0.1$$  
- $$P(10 | 8, D) = 0.1$$  

---

---

### **R√©ponses d√©taill√©es :**

---

#### **Question 1 :**   
R√©ponse : **B) Faux**  
**Explication :** La Propri√©t√© de Markov indique que l'avenir d√©pend uniquement de l'√©tat actuel et de l'action entreprise. Les √©tats pr√©c√©dents n'ont aucune influence sur l'avenir si l'√©tat actuel est connu.   

---

#### **Question 2 :**   
R√©ponse : **C) Une situation possible dans laquelle l‚Äôagent se trouve.**  
**Explication :** Un √©tat repr√©sente un instant pr√©cis dans le temps qui d√©crit compl√®tement la situation actuelle de l'agent.  

---

#### **Question 3 :**   
R√©ponse : **A) Une fonction qui d√©termine l'action optimale √† prendre pour chaque √©tat.**  
**Explication :** La politique est la strat√©gie de l'agent qui d√©finit quelle action entreprendre en fonction de l'√©tat o√π il se trouve.  

---

#### **Question 4 :**   
R√©ponses :  
- $$P(2 | 7, H) = 0.8$$ (Probabilit√© que le robot atteigne l'√©tat 2 en allant en haut depuis l'√©tat 7).  
- $$P(1 | 7, H) = 0.2$$ (Probabilit√© que le robot glisse et arrive √† l'√©tat 1).  

---

#### **Question 5 :**   
R√©ponse : **-10 points.**  
**Explication :** Si le robot essaie d'aller √† gauche depuis l'√©tat 1, il frappe un mur. Une p√©nalit√© est donc appliqu√©e.  

---

#### **Question 6 :**   
R√©ponse : **A) L'avenir d√©pend uniquement de l'√©tat actuel et de l'action prise.**  
**Explication :** La Propri√©t√© de Markov stipule que l'histoire n'a pas d'importance, seul l'√©tat actuel compte.  

---

#### **Question 7 :**   
R√©ponse : **146 points.**  
**Calcul :** -1 -1 + 50 -1 + 100 = 146  

---

#### **Question 8 :**   
R√©ponse :  
$$U = -1 + 0.9(-1) + 0.9^2(50) + 0.9^3(-1) + 0.9^4(100) = 103.481$$  

---

#### **Question 9 :**   
R√©ponse :  
- **Meilleur chemin :** **7 ‚Üí 8 ‚Üí 9 ‚Üí 14 ‚Üí 15 ‚Üí 20 ‚Üí 25**.  
- **Justification :** Ce chemin atteint l'objectif avec un minimum de mouvements et maximise la r√©compense en trouvant l'indice pr√©cieux (+50 points).  

---

#### **Question 10 :**  
R√©ponse :   
- **80 % de chances** d'arriver en √©tat **9** (objectif principal).  
- **10 % de chances** de glisser et revenir en √©tat **7**.  
- **10 % de chances** d'aller accidentellement en √©tat **10**.  

