### **Quiz : Propriété de Markov et Équation de probabilité des états (10 Questions)**

---

# 1 - **Introduction et instructions préléminaires**
---

Ci-bas, vous trouvez un exemple de labyrinthe simplifié où un robot doit atteindre la sortie (**★**) en évitant de perdre trop de points.  

```
+----+----+----+----+----+
|  1 |  2 |  3 |  4 |  5 |
+----+----+----+----+----+
|  6 |  7 |  8 |  9 | 10 |
+----+----+----+----+----+
| 11 | 12 | 13 | 14 | 15 |
+----+----+----+----+----+
| 16 | 17 | 18 | 19 | 20 |
+----+----+----+----+----+
| 21 | 22 | 23 | 24 | 25★|
+----+----+----+----+----+
```

---

# 2 - **Règles du labyrinthe :**
---

1. **Chaque pièce est un état (S).**  
2. **Le robot peut se déplacer :**  
   - **H (Haut)**  
   - **B (Bas)**  
   - **G (Gauche)**  
   - **D (Droite)**  
3. **Récompenses :**  
   - Atteindre la sortie (état **25**) : **+100 points.**  
   - Heurter un mur (mouvement impossible) : **-10 points.**  
   - Se déplacer inutilement : **-1 point par mouvement.**  
4. **Propriété de Markov :**  
   - Le futur dépend uniquement de **l'état actuel** et de **l'action choisie**.  

---

# 3 - **Questions :**

---

#### **Question 1 : Compréhension de base (Vrai ou Faux)**  
**La Propriété de Markov stipule que l'avenir dépend à la fois de l'état actuel, de l'action prise, et des états précédents.**  
- A) Vrai  
- B) Faux  

---

#### **Question 2 : Définition d'un état (Choix multiple)**  
Un **état (S)** dans un MDP représente :  
- A) Une action que l'agent peut entreprendre.  
- B) La stratégie optimale pour atteindre un objectif.  
- C) Une situation possible dans laquelle l’agent se trouve.  
- D) La récompense obtenue après chaque action.  

---

#### **Question 3 : Définition d'une politique (Choix multiple)**  
La politique **($$\pi$$)** est :  
- A) Une fonction qui détermine l'action optimale à prendre pour chaque état.  
- B) Un ensemble de récompenses cumulées.  
- C) La probabilité de transition entre deux états.  
- D) Une formule pour calculer la récompense immédiate.  

---

#### **Question 4 : Probabilité de transition (Question à trous)**  
Si un robot est dans l'état **7** et décide de prendre l'action **H (monter vers le haut)**, avec une probabilité de **0.8** d'arriver en état **2** et **0.2** d'arriver en état **1**, alors :  

- $$P(2 | 7, H) = \_\_\_\_$$
  
- $$P(1 | 7, H) = \_\_\_\_$$  

---

#### **Question 5 : Récompense immédiate (Question ouverte)**  
Si le robot tente de se déplacer vers la gauche depuis **l'état 1**, quelle sera la récompense obtenue ? Expliquez pourquoi.  

---

#### **Question 6 : Propriété de Markov (Choix multiple)**  
La Propriété de Markov signifie que :  
- A) L'avenir dépend uniquement de l'état actuel et de l'action prise.  
- B) L'avenir dépend uniquement des actions passées.  
- C) L'avenir dépend de tous les états précédents.  
- D) L'avenir ne dépend que des récompenses immédiates.  

---

#### **Question 7 : Récompense cumulative (Question à trous)**  
Pour un agent qui reçoit les récompenses suivantes : **-1, -1, +50, -1, +100**, sa récompense cumulative est de **\_\_\_\_**.  

---

#### **Question 8 : Calcul de l'utilité (Question de calcul)**  
Si le robot applique un facteur d’actualisation ($$\gamma = 0.9$$), quelle est l’utilité totale obtenue avec les récompenses suivantes : **-1, -1, +50, -1, +100** ?  
- Formule :  
$$U = -1 + 0.9(-1) + 0.9^2(50) + 0.9^3(-1) + 0.9^4(100)$$  

---

#### **Question 9 : Politique optimale (Question ouverte)**  
Quelle est la meilleure politique pour atteindre l'état **25 (★)** à partir de l'état **7** ? Justifiez votre réponse.  

---

#### **Question 10 : Transition probabiliste (Question ouverte)**  
Si l’agent est dans l’état **8** et décide d'aller à droite **(D)**, quelles sont les conséquences probables si :  

- $$P(9 | 8, D) = 0.8$$  
- $$P(7 | 8, D) = 0.1$$  
- $$P(10 | 8, D) = 0.1$$  

---

---

### **Réponses détaillées :**

---

#### **Question 1 :**   
Réponse : **B) Faux**  
**Explication :** La Propriété de Markov indique que l'avenir dépend uniquement de l'état actuel et de l'action entreprise. Les états précédents n'ont aucune influence sur l'avenir si l'état actuel est connu.   

---

#### **Question 2 :**   
Réponse : **C) Une situation possible dans laquelle l’agent se trouve.**  
**Explication :** Un état représente un instant précis dans le temps qui décrit complètement la situation actuelle de l'agent.  

---

#### **Question 3 :**   
Réponse : **A) Une fonction qui détermine l'action optimale à prendre pour chaque état.**  
**Explication :** La politique est la stratégie de l'agent qui définit quelle action entreprendre en fonction de l'état où il se trouve.  

---

#### **Question 4 :**   
Réponses :  
- $$P(2 | 7, H) = 0.8$$ (Probabilité que le robot atteigne l'état 2 en allant en haut depuis l'état 7).  
- $$P(1 | 7, H) = 0.2$$ (Probabilité que le robot glisse et arrive à l'état 1).  

---

#### **Question 5 :**   
Réponse : **-10 points.**  
**Explication :** Si le robot essaie d'aller à gauche depuis l'état 1, il frappe un mur. Une pénalité est donc appliquée.  

---

#### **Question 6 :**   
Réponse : **A) L'avenir dépend uniquement de l'état actuel et de l'action prise.**  
**Explication :** La Propriété de Markov stipule que l'histoire n'a pas d'importance, seul l'état actuel compte.  

---

#### **Question 7 :**   
Réponse : **146 points.**  
**Calcul :** -1 -1 + 50 -1 + 100 = 146  

---

#### **Question 8 :**   
Réponse :  
$$U = -1 + 0.9(-1) + 0.9^2(50) + 0.9^3(-1) + 0.9^4(100) = 103.481$$  

---

#### **Question 9 :**   
Réponse :  
- **Meilleur chemin :** **7 → 8 → 9 → 14 → 15 → 20 → 25**.  
- **Justification :** Ce chemin atteint l'objectif avec un minimum de mouvements et maximise la récompense en trouvant l'indice précieux (+50 points).  

---

#### **Question 10 :**  
Réponse :   
- **80 % de chances** d'arriver en état **9** (objectif principal).  
- **10 % de chances** de glisser et revenir en état **7**.  
- **10 % de chances** d'aller accidentellement en état **10**.  

