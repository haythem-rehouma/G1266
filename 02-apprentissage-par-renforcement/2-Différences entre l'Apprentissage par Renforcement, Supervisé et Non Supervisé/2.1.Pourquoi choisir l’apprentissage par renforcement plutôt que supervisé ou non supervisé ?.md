
# **Pourquoi choisir l’apprentissage par renforcement plutôt que supervisé ou non supervisé ?**  

## **1. Introduction**  

L’intelligence artificielle repose sur trois grandes familles d’apprentissage :  
- **L’apprentissage supervisé**, où un modèle apprend à partir de données étiquetées.  
- **L’apprentissage non supervisé**, où un modèle détecte des structures cachées dans des données non étiquetées.  
- **L’apprentissage par renforcement (RL)**, où un agent interagit avec un environnement et apprend à optimiser ses actions grâce aux récompenses reçues.  

Dans ce chapitre, nous allons explorer pourquoi et quand **l’apprentissage par renforcement** est le meilleur choix pour résoudre certains problèmes complexes.  

---

## **2. Rappel sur les types d’apprentissage automatique**  

### **2.1. Apprentissage supervisé**  
L’apprentissage supervisé repose sur des données pré-étiquetées. Le modèle apprend en associant **des entrées** à **des sorties connues**.  

**Exemple :**  
- **Reconnaissance d’images** : Un modèle apprend à identifier des chats et des chiens grâce à des images correctement étiquetées.  
- **Prédiction des prix immobiliers** : L’algorithme est entraîné avec un historique des prix et des caractéristiques des maisons.  

**Limite principale :**  
- Le modèle ne peut pas s’adapter aux **situations nouvelles ou imprévues** puisqu’il dépend entièrement des données sur lesquelles il a été entraîné.  

---

### **2.2. Apprentissage non supervisé**  
L’apprentissage non supervisé fonctionne sans étiquettes et cherche **des structures cachées** dans les données.  

**Exemple :**  
- **Segmentation de clientèle** : Un algorithme regroupe des clients selon leurs habitudes d’achat sans information préalable.  
- **Détection d’anomalies** : Identifier des fraudes bancaires en détectant des transactions inhabituelles.  

**Limite principale :**  
- Il identifie **des schémas**, mais **ne prend pas de décisions**. Il est donc inutilisable pour des tâches interactives.  

---

## **3. Pourquoi opter pour l’apprentissage par renforcement ?**  

L’apprentissage par renforcement (RL) est **indispensable** lorsqu’un agent doit **interagir avec un environnement dynamique**, prendre des décisions successives et s’adapter en temps réel.  

### **3.1. Interaction en temps réel avec l’environnement**  
Contrairement au supervisé ou non supervisé qui exploitent des **données fixes**, le RL **agit et apprend simultanément**.  

**Exemple :** Un robot doit naviguer dans un entrepôt en évitant des obstacles mouvants. Il ne peut pas se contenter de données pré-établies : il doit **réagir en temps réel**.  

---

### **3.2. Maximisation d’une récompense cumulative à long terme**  
L’apprentissage supervisé cherche **une réponse correcte immédiate**, tandis que le RL optimise **une stratégie sur la durée**.  

**Exemple :** Un algorithme de trading ne cherche pas simplement à **prédire** le prix d’une action, mais **à maximiser les profits** sur plusieurs transactions en fonction de l’évolution du marché.  

---

## **4. Exemples concrets : Comparaison RL vs Autres Méthodes**  

### **4.1. Conduite autonome**  
- **Supervisé** : Analyse les situations déjà observées (ex. images de conduite).  
- **RL** : **Apprend à réagir aux situations inédites** (ex. éviter un piéton qui surgit soudainement).  

### **4.2. Robotique industrielle**  
- **Non supervisé** : Peut regrouper les tâches similaires, mais **ne sait pas comment** exécuter les actions.  
- **RL** : **Optimise ses mouvements en temps réel** pour manipuler des objets de formes différentes.  

### **4.3. Optimisation de la publicité en ligne**  
- **Supervisé** : Se base uniquement sur les clics passés et affiche des annonces similaires.  
- **RL** : **Ajuste continuellement** la publicité selon le comportement en temps réel de l’utilisateur.  

---

## **5. Tableau comparatif : Supervisé, Non Supervisé et RL**  

| **Critère**               | **Supervisé**          | **Non supervisé**       | **Apprentissage par renforcement (RL)** |
|--------------------------|----------------------|----------------------|----------------------------------------|
| **Données**              | Étiquetées           | Non étiquetées       | Interaction dynamique                  |
| **Objectif principal**   | Classification, Prédiction | Regroupement, Détection de motifs | Maximisation d’une récompense à long terme |
| **Nature des décisions** | Statiques, basées sur l’historique | Pas de décision active | Décisions séquentielles et adaptatives |
| **Interaction avec l’environnement** | Aucune | Aucune | Interaction continue |
| **Exemple concret**      | Reconnaissance faciale | Segmentation clientèle | Conduite autonome, robotique |

---

## **6. Pourquoi le RL plutôt que supervisé ou non supervisé ?**  

### **6.1. Limites du supervisé**  
- Dépend entièrement **des données étiquetées**, donc ne peut pas gérer les **scénarios inconnus**.  
- Ne fonctionne pas pour les **décisions séquentielles** où **chaque action influence l’avenir**.  

_Exemple : Un modèle de reconnaissance faciale supervisé ne peut pas apprendre à **optimiser** son cadrage comme le ferait un RL._  

---

### **6.2. Limites du non supervisé**  
- Permet **de regrouper** les données, mais **ne sait pas comment réagir** à des événements.  
- Ne peut pas **optimiser une stratégie** basée sur des récompenses.  

_Exemple : Un clustering client en marketing ne permet pas d’**adapter** une campagne publicitaire en fonction des interactions en temps réel._  

---

### **6.3. Avantage décisif du RL**  
- **Apprend directement de l’expérience** sans nécessiter de données pré-étiquetées.  
- **Prend des décisions séquentielles** qui impactent l’avenir.  
- **S’adapte dynamiquement** aux nouvelles situations.  

_Exemple : Un robot RL apprend à **ouvrir une porte** en testant différentes stratégies et en améliorant son mouvement au fil du temps._  

---

## **7. Exemples d’environnements dynamiques adaptés au RL**  

Le RL est **particulièrement utile** lorsque les actions influencent l’environnement :  

- **Conduite autonome** → S’adapte aux conditions de la route et du trafic.  
- **Robotique industrielle** → Ajuste ses mouvements pour s’adapter aux variations des objets.  
- **Jeux vidéo complexes** → Apprend à réagir en temps réel aux adversaires.  
- **Trading algorithmique** → Prend des décisions financières en fonction des fluctuations des marchés.  
- **Gestion énergétique intelligente** → Optimise la consommation en fonction des conditions climatiques.  
- **Systèmes de recommandation interactifs** → Adapte les suggestions selon les interactions en direct des utilisateurs.  

---

## **8. Synthèse**  

- L’apprentissage par renforcement est **essentiel** dans les environnements dynamiques où les décisions influencent l’avenir.  
- Contrairement aux méthodes supervisées et non supervisées, le RL permet à un agent **d’apprendre en essayant**, d’**évaluer les résultats**, et **d’optimiser ses décisions à long terme**.  
- Cette capacité unique à **gérer la complexité et l’adaptation en temps réel** fait du RL **une solution incontournable** pour résoudre des problèmes pratiques avancés.  

---

## **9. Références complémentaires**  

- **DeepMind** : RL appliqué à des problèmes complexes.  
- **Stanford – RL Course** : Comparaison RL vs autres méthodes.  
- **MIT Technology Review** : Cas concrets RL dans l’industrie.  
