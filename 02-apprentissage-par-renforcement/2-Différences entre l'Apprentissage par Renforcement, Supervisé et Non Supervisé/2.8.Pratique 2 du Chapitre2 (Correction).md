# Pratique 2 – Pourquoi le RL ?

## 1. Objectifs d’apprentissage

À la fin de cet exercice, vous serez capable de :

- Expliquer pourquoi l’apprentissage par renforcement (RL) est la meilleure approche pour certaines applications.  
- Comparer le RL avec l’apprentissage supervisé et non supervisé dans des contextes réels.  
- Justifier l’utilisation du RL en tenant compte des contraintes et des objectifs d’optimisation.  

---

## 2. Exercices – Réponses complètes  

### **2.1. Mobilité intelligente et conduite autonome**  
**Problème à résoudre :**  
Les véhicules autonomes doivent gérer une multitude de situations imprévues : circulation dense, obstacles soudains, conditions météorologiques changeantes.  

**Pourquoi le RL ?**  
Le RL permet aux véhicules autonomes d’apprendre **à partir de l’expérience** et de s’adapter aux environnements dynamiques en temps réel. Contrairement à l’apprentissage supervisé, qui nécessiterait des **milliers de scénarios étiquetés**, le RL apprend par **essais et erreurs** en interagissant directement avec son environnement.  
- Il optimise les décisions **séquentielles** (freiner, accélérer, éviter un obstacle).  
- Il maximise une **récompense cumulative** (minimiser le temps de trajet tout en assurant la sécurité).  
- Il permet **une adaptation continue** aux conditions nouvelles et aux imprévus.  

---

### **2.2. Automatisation et robotique avancée**  
**Problème à résoudre :**  
Un robot domestique ou industriel doit accomplir des tâches variées (manipulation d’objets, navigation, interaction avec des humains), souvent dans un environnement en constante évolution.  

**Pourquoi le RL ?**  
Le RL est particulièrement adapté à la robotique, car il permet au robot d’**apprendre par l’expérience** plutôt que de suivre des règles fixes.  
- Il permet d’**adapter ses actions** aux changements de l’environnement (ex. éviter un humain en mouvement).  
- Il optimise ses actions pour maximiser une **récompense** (ex. efficacité énergétique, rapidité de la tâche).  
- Il est plus flexible que l’apprentissage supervisé, qui nécessiterait **des jeux de données gigantesques** et figés.  

---

### **2.3. Finance et optimisation des investissements**  
**Problème à résoudre :**  
Les marchés financiers sont imprévisibles et influencés par de nombreux facteurs externes. Un algorithme de trading doit décider quand acheter, vendre ou conserver un actif pour maximiser les profits.  

**Pourquoi le RL ?**  
Les décisions de trading doivent être prises **de manière dynamique et adaptative**, ce qui exclut l’apprentissage supervisé, basé sur des données historiques statiques.  
- Le RL permet **d’apprendre des tendances en temps réel** et de s’adapter aux fluctuations des marchés.  
- Il prend en compte **l’impact à long terme** des décisions sur le portefeuille.  
- Il peut **tester différentes stratégies** en simulation avant de les appliquer sur des marchés réels.  

---

### **2.4. Gestion énergétique et smart grids**  
**Problème à résoudre :**  
Les bâtiments intelligents et les réseaux électriques doivent ajuster leur consommation d’énergie en fonction de l’occupation des lieux, des conditions climatiques et de la demande sur le réseau.  

**Pourquoi le RL ?**  
Le RL permet d’optimiser **de manière continue** la consommation d’énergie en s’adaptant aux variations en temps réel.  
- Il ajuste **dynamiquement** le chauffage, la climatisation et l’éclairage en fonction des usages.  
- Il réduit les **pics de consommation** pour éviter la surcharge du réseau.  
- Il maximise l’efficacité énergétique en prenant **des décisions optimales à long terme**.  

---

### **2.5. Personnalisation des expériences utilisateur et marketing digital**  
**Problème à résoudre :**  
Les plateformes de contenu et de e-commerce doivent recommander des produits ou des vidéos de manière personnalisée pour maximiser l’engagement des utilisateurs.  

**Pourquoi le RL ?**  
Contrairement à un algorithme supervisé qui se base sur **les préférences passées**, le RL ajuste en permanence les recommandations **en fonction du comportement en temps réel des utilisateurs**.  
- Il **teste différentes suggestions** et mesure l’engagement (clics, durée de visionnage).  
- Il maximise une **récompense cumulative** (ex. fidélisation des utilisateurs).  
- Il permet une **adaptation dynamique**, ce qui est crucial pour le marketing digital.  

---

### **2.6. Soins de santé et traitements personnalisés**  
**Problème à résoudre :**  
Les traitements médicaux doivent être adaptés aux besoins spécifiques de chaque patient en fonction de leur réponse aux médicaments.  

**Pourquoi le RL ?**  
Le RL permet d’ajuster **de manière individualisée** les traitements médicaux en fonction de la réaction du patient.  
- Il teste **différentes doses** et apprend à **optimiser le traitement** sans intervention humaine.  
- Il minimise **les effets secondaires** en trouvant un équilibre optimal.  
- Il est plus performant que l’apprentissage supervisé, qui nécessiterait **des jeux de données statiques**, alors que les besoins d’un patient évoluent dans le temps.  

---

### **2.7. Gestion du trafic et optimisation des infrastructures urbaines**  
**Problème à résoudre :**  
Les villes doivent fluidifier la circulation et réduire les embouteillages en optimisant la gestion des feux de signalisation et du trafic en temps réel.  

**Pourquoi le RL ?**  
Les embouteillages résultent d’une **dynamique complexe** qui change constamment, ce qui rend les modèles supervisés inefficaces.  
- Le RL permet **d’adapter les cycles des feux** selon le trafic en temps réel.  
- Il apprend **par essais et erreurs** en testant différentes stratégies et en mesurant leur impact.  
- Il réduit les **temps d’attente globaux** et améliore **la fluidité du trafic** sur le long terme.  

---

## 3. Synthèse des réponses  

| **Cas** | **Pourquoi utiliser le RL ?** |
|---------|-------------------------------|
| **Conduite autonome** | Apprentissage dynamique, prise de décision séquentielle, adaptation aux imprévus. |
| **Robotique avancée** | Adaptation aux environnements en évolution, optimisation des tâches. |
| **Trading algorithmique** | Décisions en temps réel, prise en compte de l’impact futur. |
| **Gestion énergétique** | Optimisation en fonction des variations en temps réel, réduction des pics de consommation. |
| **Marketing digital** | Ajustement dynamique des recommandations, maximisation de l’engagement. |
| **Soins de santé** | Personnalisation des traitements, optimisation des doses en fonction du patient. |
| **Gestion du trafic** | Adaptation en temps réel, fluidification du trafic, réduction des temps d’attente. |

---

## 4. Conclusion  

L’apprentissage par renforcement est essentiel dans **les environnements dynamiques**, où les décisions doivent être **optimisées sur le long terme**. Contrairement aux méthodes supervisées et non supervisées, le RL permet d’**interagir avec l’environnement, d’apprendre par essais et erreurs, et d’adapter ses stratégies en fonction des résultats obtenus**.

Il est donc particulièrement adapté aux applications nécessitant :  
- **Une prise de décision séquentielle** (ex. conduite autonome).  
- **Une adaptation en temps réel** (ex. trading algorithmique, smart grids).  
- **L’optimisation sur le long terme** (ex. soins de santé, gestion du trafic).  
