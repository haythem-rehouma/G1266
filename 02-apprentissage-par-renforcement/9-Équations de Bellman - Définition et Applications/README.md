# Plan de la séance


<br/>
<br/>

# Partie 1 - rappel du processus de Décision de Markov (MDP)


# Composantes des MDP :
- **États (S)** : Représentent les différentes situations possibles dans lesquelles un agent peut se trouver.
- **Actions (A)** : Les différentes actions que l'agent peut entreprendre dans un état donné.
- **Transitions (P(s'|s,a) ou T(s,a,s'))** : La probabilité de passer d'un état à un autre, en fonction de l'action entreprise.
- **Récompenses (R(s,a,s') et facteur de réduction γ)** : Les récompenses obtenues par l'agent lorsqu'il effectue une transition entre états après avoir pris une action.
- **État de départ (s₀)** : L'état initial à partir duquel le processus commence.

# Quantités importantes :
- **Politique** : Une stratégie ou une fonction qui associe à chaque état une action à entreprendre.
- **Utilité** : La somme des récompenses actualisées, où les récompenses futures sont pondérées par le facteur de réduction (γ).


<br/>
<br/>
# Partie 2 - offline vs online learning



<br/>
<br/>

# Partie 3 - Équation de Bellman 




<br/>
<br/>

# Partie 4 - Démonstration 1


<br/>
<br/>

# Partie 5 - (Justification de la démo 1 et exercice): Comment utiliser l'équation de Bellman dans l'Apprentissage par Renforcement



<br/>
<br/>

# Partie 6 - Q-Learning



<br/>
<br/>

# Partie 7 - Démonstration 2


- Comment utiliser l'Équation de Bellman dans l'Apprentissage par Renforcement , exemple de calcul de la table q-value


<br/>
<br/>

# Partie 8 - Démo AWS


<br/>
<br/>

# Partie 9 - Exercices


