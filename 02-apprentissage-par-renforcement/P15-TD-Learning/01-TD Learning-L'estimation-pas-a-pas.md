# 1 - Introduction au TD Learning : une mÃ©thode d'estimation pas Ã  pas

- Dans cette nouvelle section, nous allons explorer une autre approche puissante appelÃ©e **TD Learning (Temporal Difference Learning)**. 
- Contrairement aux mÃ©thodes Monte Carlo qui attendent la fin dâ€™un Ã©pisode complet, les mÃ©thodes TD permettent de mettre Ã  jour les estimations **en temps rÃ©el**, au fur et Ã  mesure que lâ€™agent avance dans lâ€™environnement.

# 2 - Pourquoi le TD Learning ?

Les mÃ©thodes Monte Carlo ont leurs avantages, mais elles prÃ©sentent aussi certaines **limitations** :
1. **Elles doivent attendre la fin de lâ€™Ã©pisode** pour pouvoir estimer la valeur dâ€™un Ã©tat, ce qui peut Ãªtre long ou impossible si lâ€™Ã©pisode ne se termine jamais.
2. **Elles ne sont pas idÃ©ales pour les environnements continus ou trÃ¨s longs**, oÃ¹ lâ€™attente dâ€™une fin complique la mise Ã  jour.

Les mÃ©thodes TD (comme **TD(0)**, **TD(n)** ou encore **SARSA**) apportent des solutions efficaces :
-  Elles **n'ont pas besoin d'attendre la fin de l'Ã©pisode** : on peut commencer Ã  apprendre dÃ¨s la premiÃ¨re Ã©tape.
-  Elles permettent un **apprentissage en ligne**, parfait pour les environnements en continu ou Ã  durÃ©e indÃ©terminÃ©e.
-  Elles combinent les avantages des mÃ©thodes de Monte Carlo et de la programmation dynamique.

# 3 -  Qu'allons-nous faire ?

Nous allons commencer par la mÃ©thode **TD(0)**, la version la plus simple du TD Learning :
- Elle met Ã  jour la valeur dâ€™un Ã©tat **immÃ©diatement aprÃ¨s avoir observÃ© la rÃ©compense suivante et lâ€™Ã©tat suivant**.
  
Ensuite, nous Ã©tendrons la mÃ©thode avec :
1. **TD(n)**, qui permet de prendre en compte plusieurs Ã©tapes futures.
2. **SARSA** : une mÃ©thode de contrÃ´le basÃ©e sur le TD, qui apprend Ã  la fois les valeurs et la politique.
3. **Q-learning** : une autre mÃ©thode basÃ©e sur le TD, qui apprend la valeur optimale indÃ©pendamment de la politique suivie.

Nous appliquerons ces concepts dans un environnement concret (par exemple **FrozenLake**), oÃ¹ un agent apprend progressivement Ã  choisir les meilleures actions, sans attendre la fin du jeu.

<br/>

# 3 - RÃ©sumÃ© avec une explication plus simple

**TD Learning**, ou apprentissage par diffÃ©rence temporelle, est une mÃ©thode qui permet Ã  un agent dâ€™apprendre **au fil de ses expÃ©riences, sans attendre la fin du jeu**. 
- Imagine que tu joues Ã  un jeu vidÃ©o et quâ€™Ã  chaque action, tu obtiens un petit indice sur si tu vas gagner ou pas â€” tu nâ€™attends pas dâ€™avoir fini le jeu pour tirer des leÃ§ons : tu apprends tout de suite, action par action. Câ€™est Ã§a, le cÅ“ur du TD Learning.
- Si Monte Carlo consiste Ã  observer tout un film avant de donner son avis, TD Learning te permet de **donner une critique dÃ¨s la premiÃ¨re scÃ¨ne**, et de la corriger au fur et Ã  mesure que tu avances. Cela rend lâ€™apprentissage plus rapide, surtout quand le film est long ou infini.

<br/>

# 3 - TD Learning, câ€™est la science du "jâ€™apprends au fur et Ã  mesure" (lâ€™art dâ€™apprendre **au fil de lâ€™expÃ©rience**)



Avec le **TD Learning**, on nâ€™attend pas la fin dâ€™un Ã©pisode pour apprendre : **on ajuste ses estimations Ã  chaque Ã©tape, en temps rÃ©el**. Câ€™est lâ€™Ã©quivalent, dans la vie quotidienne, de goÃ»ter un plat pendant que tu le cuisines, plutÃ´t que dâ€™attendre de lâ€™avoir servi pour juger sâ€™il est bon.

- **TD Learning** : Ã€ chaque Ã©tape de la recette, tu goÃ»tes, tu ajustes le sel, tu modifies la cuisson. Tu apprends **pendant lâ€™action**.
- **Monte Carlo** : Tu prÃ©pares le plat entier sans y toucher, tu attends le verdict final Ã  la premiÃ¨re bouchÃ©eâ€¦ et si ce nâ€™est pas bon, tu recommences depuis le dÃ©but.

Autrement dit, **Monte Carlo apprend en observant des trajectoires complÃ¨tes**, alors que **TD Learning apprend pas Ã  pas, dÃ¨s quâ€™une information est disponible**. Cela en fait une mÃ©thode bien plus adaptÃ©e aux environnements continus ou longs, oÃ¹ attendre la fin est trop coÃ»teux â€” voire impossible.




# 4 - Exemples simpels de la vraie vie pour illustrer TD Learning (Temporal Difference Learning)

-  **Tu veux apprendre Ã  bien saler tes pÃ¢tes** : tu goÃ»tes une fourchette aprÃ¨s 3 minutes, tu ajoutes un peu de sel, tu regoÃ»tes Ã  5 minutes, tu ajustes... et Ã  la fin, câ€™est (presque) parfait. Câ€™est du TD Learning : **on ajuste Ã  chaque Ã©tape**.

-  **Tu veux devenir bon en karaokÃ©** : Ã  chaque fausse note, tu regardes la tÃªte de ton public et tu baisses un peu le volume. Tu ajustes ton ton couplet aprÃ¨s coupletâ€¦ pas besoin dâ€™attendre la fin de la chanson pour corriger le tir !

-  **Tu veux prendre une douche pas trop froide ni trop chaude** : tu ouvres lâ€™eau, tu testes la tempÃ©rature avec la main, tu ajustes, tu retestes, tu rÃ©ajustesâ€¦ Tu ne restes pas gelÃ© 10 minutes Ã  attendre de voir si Ã§a sâ€™arrange tout seul.

-  **Tu veux apprendre Ã  bien servir au ping-pong** : tu testes un service, lâ€™autre rate complÃ¨tement. Tu tentes avec un peu plus dâ€™effet, Ã§a passe mieux. Tu ajustes Ã  chaque Ã©change â€” pas besoin de perdre 21-0 pour comprendre.

-  **Tu veux laver la vaisselle sans inonder la cuisine** : tu ouvres un peu lâ€™eau, tu vois que Ã§a Ã©clabousse, tu fermes un peu, tu ajustes la pression. Tu nâ€™attends pas la fin de la pile de vaisselle pour comprendre que Ã§a dÃ©borde.



Bref, **TD Learning, câ€™est apprendre Ã  chaque pas**, pas Ã  la fin du chemin. Tu avances, tu observes, tu corriges.

<br/>



# 4 - Nos anciens connaissaient dÃ©jÃ  le principe du TD Learning

Bien avant les algorithmes, nos grands-parents savaient que **la meilleure faÃ§on dâ€™apprendre, câ€™est dâ€™observer, de corriger, et de sâ€™adapter**. Dans les pays arabes, on dit souventâ€¯: **Â« Ê¿Ä«nek mÄ«zÄnek Â»** â€“ Â«â€¯Tes yeux sont ta balanceâ€¯Â». Cela signifie quâ€™au lieu de suivre une rÃ¨gle figÃ©e, on ajuste **en fonction de ce quâ€™on voit, de ce quâ€™on ressent, et de ce qui se passe ici et maintenant**.

Ce proverbe illustre parfaitement lâ€™idÃ©e du **TD Learning** : on apprend Ã  chaque Ã©tape, Ã  partir de lâ€™expÃ©rience immÃ©diate, sans attendre la fin du processus pour tirer des leÃ§ons. Câ€™est lâ€™opposÃ© dâ€™un apprentissage thÃ©orique : **on ajuste en temps rÃ©el, parce quâ€™on fait confiance Ã  ce que la rÃ©alitÃ© nous montre**.



Dans ce sens, **TD Learning** est une formalisation algorithmique de ce principe ancestral :  
> **"Regarde, ajuste, puis apprends. Pas lâ€™inverse."**

D'autres cultures partagent cette philosophie :
- **Proverbe africain** : *Â«â€¯Ce nâ€™est pas au bout du chemin quâ€™on redresse ses sandales.â€¯Â»*
- **Proverbe anglais** : *Â«â€¯Donâ€™t fix it all at once â€” fix it as you go.â€¯Â»*

Toutes ces sagesses expriment la mÃªme idÃ©e :  
ğŸ‘‰ **Le vrai apprentissage ne se fait pas Ã  la fin â€” il se fait Ã  chaque instant, en corrigeant avec les yeux ouverts.**


De la cuisine au jardinage, du bricolage Ã  lâ€™enseignement, nos aÃ®nÃ©s pratiquaient dÃ©jÃ , sans le nommer, un apprentissage par diffÃ©rences temporelles â€” celui de **lâ€™intuition corrigÃ©e par lâ€™expÃ©rience**.

