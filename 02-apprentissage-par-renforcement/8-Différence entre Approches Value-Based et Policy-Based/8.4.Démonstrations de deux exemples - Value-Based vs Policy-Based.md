# **Approches Value-Based vs Policy-Based en Apprentissage par Renforcement**  

<br/>

# **Équations Principales**  

(Équation 1 : Q-Learning Standard)  
$$Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]$$  

(Équation 2 : SARSA Standard)  
$$Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma Q(s', a') - Q(s, a) \right]$$  

(Équation 3 : Gradient de Politique Standard - REINFORCE)  
$$\theta \leftarrow \theta + \alpha \nabla_\theta \log \pi_\theta(a|s) G_t$$  

(Équation 4 : Q-Learning Refactorisé avec $$1 - \alpha$$)  
$$Q(s, a) \leftarrow (1 - \alpha) Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') \right)$$  

(Équation 5 : SARSA Refactorisé avec $$1 - \alpha$$)  
$$Q(s, a) \leftarrow (1 - \alpha) Q(s, a) + \alpha \left( r + \gamma Q(s', a') \right)$$  

(Équation 6 : Gradient de Politique Refactorisé avec $$1 - \alpha$$)  
$$\theta \leftarrow (1 - \alpha) \theta + \alpha \nabla_\theta \log \pi_\theta(a|s) G_t$$  

<br/>

---
# **Introduction**  
---

L'apprentissage par renforcement repose principalement sur deux approches : **Value-Based (Basée sur les Valeurs)** et **Policy-Based (Basée sur les Politiques)**. Chacune de ces approches utilise des équations spécifiques pour définir et optimiser les politiques ou fonctions de valeur.  

<br/>

#  1 - **Démonstration de l’Algorithme Value-Based : Q-Learning**  
### **Objectif :**  
Apprendre une fonction de valeur $$Q(s, a)$$ qui permet de déterminer l’action optimale à entreprendre pour maximiser la récompense cumulative dans un environnement donné.  

### **Principe :**  
L’algorithme Q-Learning est un **algorithme Off-Policy**, c’est-à-dire qu’il apprend indépendamment de la politique utilisée pour explorer l’environnement. Il cherche à apprendre directement la **fonction de valeur d’état-action** $$Q(s, a)$$.  

### **Étapes de l’Algorithme Q-Learning :**  
1. **Initialisation :**  
   - Créer une table $$Q(s, a)$$ avec des valeurs initiales arbitraires (souvent zéro) pour chaque paire $$(s, a)$$.  
   - Choisir un taux d’apprentissage $$\alpha$$, un facteur de réduction $$\gamma$$ (entre 0 et 1), et une stratégie d'exploration (par exemple, $$\epsilon$$-greedy).  

2. **Répéter (pour chaque épisode) :**  
   - Initialiser l’état initial $$s$$.  

3. **Pour chaque étape de l’épisode :**  
   - Choisir une action $$a$$ en fonction de la politique $$\epsilon$$-greedy.  
   - Exécuter l’action $$a$$ et observer la récompense $$r$$ et l’état suivant $$s'$$.  
   - Mettre à jour $$Q(s, a)$$ en utilisant **l'Équation 1** ou **l'Équation 4**.  
   - Mettre à jour l’état courant $$s = s'$$.  

4. **Fin de l'épisode.**  
   - Recommencer un nouvel épisode jusqu'à convergence.  

<br/>

# 2 -  **Démonstration de l’Algorithme Policy-Based : REINFORCE**  
### **Objectif :**  
Apprendre une politique $$\pi(a|s)$$ qui permet de choisir l’action optimale en fonction de l’état actuel, en maximisant la récompense totale attendue.  

### **Principe :**  
L’algorithme **REINFORCE** est un **algorithme On-Policy** qui apprend en suivant directement la politique courante. Il utilise la méthode du **gradient de politique** pour mettre à jour la politique.  

### **Étapes de l’Algorithme REINFORCE :**  
1. **Initialisation :**  
   - Initialiser les paramètres de la politique $$\pi_\theta(a|s)$$.  
   - Choisir un taux d’apprentissage $$\alpha$$.  

2. **Répéter (pour chaque épisode) :**  
   - Initialiser l’état initial $$s$$.  
   - Générer un épisode complet en suivant la politique courante.  
   - Enregistrer chaque transition sous la forme : $$(s_t, a_t, r_t)$$.  

3. **Calcul des récompenses cumulées :**  
   - Calculer $$G_t$$ en utilisant la somme des récompenses actualisées.  

4. **Mise à jour de la politique :**  
   - Ajuster $$\theta$$ en utilisant **l'Équation 3** ou **l'Équation 6**.  

5. **Fin de l'épisode.**  

<br/>

# 3 - **Comparaison : Q-Learning vs REINFORCE**  
| Critère                  | Q-Learning (Value-Based)        | REINFORCE (Policy-Based)                |
|--------------------------|--------------------------------- |-------------------------------------- |
| Ce qui est appris        | Fonction de valeur $$Q(s, a)$$ | Politique $$\pi(a\|s)$$             |
| Type d'apprentissage     | Indirect (Apprentissage par valeurs). | Direct (Apprentissage par politiques). |
| Nature de l'approche     | Off-Policy.                     | On-Policy.                             |
| Adaptation aux actions continues | Mauvais.              | Bon.                                   |
| Convergence              | Plus stable.                    | Plus instable, mais souvent plus efficace en continu. |  


