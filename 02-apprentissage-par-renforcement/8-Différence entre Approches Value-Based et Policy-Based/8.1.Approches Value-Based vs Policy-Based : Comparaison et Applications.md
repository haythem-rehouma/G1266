### **Approches Value-Based vs Policy-Based en Apprentissage par Renforcement**  




(√âquation 1 : Q-Learning Standard)



$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
$$  


(√âquation 2 : SARSA Standard)

$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma Q(s', a') - Q(s, a) \right]
$$  



(√âquation 3 : Gradient de Politique Standard - REINFORCE)


$$
\theta \leftarrow \theta + \alpha \nabla_\theta \log \pi_\theta(a|s) G_t
$$  



(√âquation 4 : Q-Learning Refactoris√© avec $$1 - \alpha$$


$$
Q(s, a) \leftarrow (1 - \alpha) Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') \right)
$$  



(√âquation 5 : SARSA Refactoris√© avec $$1 - \alpha$$


$$
Q(s, a) \leftarrow (1 - \alpha) Q(s, a) + \alpha \left( r + \gamma Q(s', a') \right)
$$  



(√âquation 6 : Gradient de Politique Refactoris√© avec $$1 - \alpha$$

$$
\theta \leftarrow (1 - \alpha) \theta + \alpha \nabla_\theta \log \pi_\theta(a|s) G_t
$$  
























$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
$$  

$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma Q(s', a') - Q(s, a) \right]
$$  

$$
\theta \leftarrow \theta + \alpha \nabla_\theta \log \pi_\theta(a|s) G_t
$$  




$$
Q(s, a) \leftarrow (1 - \alpha) Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') \right)
$$  


$$
Q(s, a) \leftarrow (1 - \alpha) Q(s, a) + \alpha \left( r + \gamma Q(s', a') \right)
$$  


$$
\theta \leftarrow (1 - \alpha) \theta + \alpha \nabla_\theta \log \pi_\theta(a|s) G_t
$$  






---

Ces variantes utilisent toutes la formulation avec \(1 - \alpha\). Voulez-vous que j'int√®gre ces nouvelles √©quations dans votre cours maintenant ? üòä

**√âquation 1 : Q-Learning**  


**√âquation 2 : SARSA**  

**√âquation 3 : Gradient de Politique (REINFORCE)**  



    
## **Introduction**  
L'apprentissage par renforcement repose principalement sur deux approches : **Value-Based (Bas√©e sur les Valeurs)** et **Policy-Based (Bas√©e sur les Politiques)**. Chacune de ces approches poss√®de des caract√©ristiques distinctes, des avantages et des inconv√©nients, ainsi que des algorithmes sp√©cifiques.  

## **1. Approche Value-Based (Bas√©e sur les Valeurs)**

### D√©finition :
L'approche Value-Based consiste √† apprendre une fonction de valeur qui estime combien de r√©compenses un agent peut esp√©rer obtenir en √©tant dans un √©tat particulier (ou en prenant une action sp√©cifique depuis cet √©tat). L'objectif est d'utiliser ces estimations pour d√©river une politique optimale, c'est-√†-dire une strat√©gie qui choisit les actions maximisant les r√©compenses.  

### **Caract√©ristiques :**  
Utilisation d'une fonction de valeur 

$$V(s)$$ : Valeur d'un √©tat.  

$$Q(s, a)$$ : Valeur d'une paire √©tat-action.  

**Apprentissage indirect :**  

1. L'agent apprend une fonction de valeur d'abord, puis d√©duit la meilleure politique en s√©lectionnant les actions ayant la valeur maximale.  
**Optimisation par it√©ration de valeur ou Q-Learning.**
2.**L'approche est typiquement off-policy**, c'est-√†-dire que l'apprentissage peut se faire en observant des actions qui ne sont pas n√©cessairement issues de la politique optimale.  

### **Exemples d‚ÄôAlgorithmes Value-Based :**  

*Q-Learning*

Algorithme qui apprend une fonction $$Q(s, a)$$ en mettant √† jour ses estimations apr√®s chaque interaction.  
  *Formule*  



 56565
    > *Ne vous inqui√©tez pas si cette formule vous para√Æt complexe ! Nous allons la d√©tailler plus tard. Pour l'instant, notez simplement qu'il s'agit d'un processus **it√©ratif** o√π l'agent met √† jour ses connaissances √† chaque √©tape.*  

- **Deep Q-Learning (DQN) :**  
  - Extension de Q-Learning qui utilise un r√©seau de neurones pour approximer la fonction $$Q(s, a)$$.  
  - Permet de g√©rer des espaces d'√©tats de grande dimension.  

- **SARSA :**  
  - Algorithme qui apprend la fonction $$Q(s, a)$$ en suivant la politique actuelle au lieu d'une politique optimale.  

*Formule*  
    $$
    Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma Q(s', a') - Q(s, a) \right]
    $$  

### **Avantages :**  
- Convient pour les environnements √† **espace d‚Äôactions discret**.  
- L‚Äôapprentissage est stable et garantit une convergence vers la politique optimale.  
- Simple √† impl√©menter lorsque l‚Äôespace d‚Äô√©tats est r√©duit.  

### **Inconv√©nients :**  
- Ne fonctionne pas bien pour les **espaces d‚Äôactions continus**.  
- L‚Äôapprentissage indirect peut √™tre lent si l‚Äôagent explore de mani√®re inefficace.  

## **2. Approche Policy-Based (Bas√©e sur les Politiques)**

### **D√©finition :**  
L'approche **Policy-Based** consiste √† **apprendre directement une politique** qui indique √† l‚Äôagent **quelle action entreprendre dans chaque √©tat**, sans passer par une fonction de valeur. L'objectif est d'optimiser cette politique pour maximiser les r√©compenses cumulatives.  

### **Caract√©ristiques :**  
- **Apprentissage direct :**  
  - L‚Äôagent apprend une politique $$\pi(a|s)$$ qui produit une distribution de probabilit√© sur les actions possibles pour chaque √©tat.  
- **Optimisation par Gradient de Politique :**  
  - L'agent ajuste ses param√®tres en maximisant une fonction d‚Äôobjectif qui repr√©sente les r√©compenses attendues.  
- **Adapt√© aux espaces d‚Äôactions continus.**  
- **Approche typiquement on-policy**, c'est-√†-dire que l‚Äôapprentissage se fait en suivant la politique courante.  

### **Exemples d‚ÄôAlgorithmes Policy-Based :**  
- **REINFORCE :**  
  - Algorithme qui met √† jour les param√®tres d'une politique en utilisant la m√©thode du gradient de politique.  
  - Formule g√©n√©rale :  
    $$
    \theta \leftarrow \theta + \alpha \nabla_\theta \log \pi_\theta(a|s) G_t
    $$
    > *Nous allons d√©tailler cette formule plus tard. Pour l'instant, notez simplement que l‚Äôagent ajuste sa politique en fonction des r√©compenses qu‚Äôil re√ßoit.*  

- **PPO (Proximal Policy Optimization) :**  
  - Algorithme qui stabilise l'apprentissage en emp√™chant les mises √† jour trop brutales.  
  - Tr√®s utilis√© dans les environnements complexes comme les jeux ou la robotique.  

- **TRPO (Trust Region Policy Optimization) :**  
  - Algorithme qui contraint la modification de la politique √† chaque √©tape pour √©viter les mises √† jour destructrices.  

### **Avantages :**  
- Fonctionne bien avec les **espaces d‚Äôactions continus**.  
- Permet d‚Äôapprendre des politiques stochastiques.  
- √âvite les approximations instables de la fonction de valeur.  

### **Inconv√©nients :**  
- Convergence moins garantie par rapport aux m√©thodes bas√©es sur les valeurs.  
- Peut n√©cessiter beaucoup d‚Äô√©chantillons pour apprendre une politique efficace.  

## **3. Comparaison Value-Based vs Policy-Based**  

| Crit√®re                        | Approche Value-Based             | Approche Policy-Based                  |
|-------------------------------|---------------------------------|---------------------------------------|
| Ce qui est appris              | Fonction de valeur $$V(s)$$ ou $$Q(s, a)$$ | Politique $$\pi(a|s)$$ (strat√©gie directe) |
| Optimisation                   | Maximiser la fonction de valeur. | Maximiser la r√©compense totale via une politique. |
| Type d'apprentissage           | Indirect (via estimation de la valeur). | Direct (via optimisation de la politique). |
| Adaptation aux actions continues | Mauvais.                       | Bon.                                    |
| Algorithmes courants           | Q-Learning, SARSA, DQN.          | REINFORCE, PPO, TRPO.                   |
| Stabilit√© de l'apprentissage    | Convergence garantie.            | Peut √™tre instable si mal configur√©.     |

## **4. Quand utiliser chaque approche ?**  
- **Approche Value-Based :** Lorsque l'espace d'actions est **discret** et que l'on cherche une m√©thode **simple et efficace** pour trouver la politique optimale.  
- **Approche Policy-Based :** Lorsque l'espace d'actions est **continu** ou que l'on souhaite apprendre une politique **stochastique** capable de s'adapter √† des environnements complexes.
