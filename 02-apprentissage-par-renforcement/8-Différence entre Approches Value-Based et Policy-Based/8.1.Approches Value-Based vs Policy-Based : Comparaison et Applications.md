### **Approches Value-Based vs Policy-Based en Apprentissage par Renforcement**  




(Équation 1 : Q-Learning Standard)



$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
$$  


(Équation 2 : SARSA Standard)

$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma Q(s', a') - Q(s, a) \right]
$$  



(Équation 3 : Gradient de Politique Standard - REINFORCE)


$$
\theta \leftarrow \theta + \alpha \nabla_\theta \log \pi_\theta(a|s) G_t
$$  



(Équation 4 : Q-Learning Refactorisé avec $$1 - \alpha$$)


$$
Q(s, a) \leftarrow (1 - \alpha) Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') \right)
$$  



(Équation 5 : SARSA Refactorisé avec $$1 - \alpha$$)


$$
Q(s, a) \leftarrow (1 - \alpha) Q(s, a) + \alpha \left( r + \gamma Q(s', a') \right)
$$  



(Équation 6 : Gradient de Politique Refactorisé avec $$1 - \alpha$$)

$$
\theta \leftarrow (1 - \alpha) \theta + \alpha \nabla_\theta \log \pi_\theta(a|s) G_t
$$  









## **Approches Value-Based vs Policy-Based en Apprentissage par Renforcement**  

## **Équations Principales**  

(Équation 1 : Q-Learning Standard)  
$$Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]$$  

(Équation 2 : SARSA Standard)  
$$Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma Q(s', a') - Q(s, a) \right]$$  

(Équation 3 : Gradient de Politique Standard - REINFORCE)  
$$\theta \leftarrow \theta + \alpha \nabla_\theta \log \pi_\theta(a|s) G_t$$  

(Équation 4 : Q-Learning Refactorisé avec $$1 - \alpha$$)  
$$Q(s, a) \leftarrow (1 - \alpha) Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') \right)$$  

(Équation 5 : SARSA Refactorisé avec $$1 - \alpha$$)  
$$Q(s, a) \leftarrow (1 - \alpha) Q(s, a) + \alpha \left( r + \gamma Q(s', a') \right)$$  

(Équation 6 : Gradient de Politique Refactorisé avec $$1 - \alpha$$)  
$$\theta \leftarrow (1 - \alpha) \theta + \alpha \nabla_\theta \log \pi_\theta(a|s) G_t$$  

---  

## **Introduction**  
L'apprentissage par renforcement repose principalement sur deux approches : **Value-Based (Basée sur les Valeurs)** et **Policy-Based (Basée sur les Politiques)**. Chacune de ces approches utilise des équations spécifiques pour définir et optimiser les politiques ou fonctions de valeur.  

## **1. Approche Value-Based (Basée sur les Valeurs)**  
### **Définition :**  
L'approche Value-Based se concentre sur l'apprentissage d'une fonction de valeur, qui estime combien de récompenses un agent peut espérer obtenir en étant dans un état particulier ou en prenant une action spécifique. L'objectif est de dériver une politique optimale qui maximise ces récompenses.  

### **Q-Learning (Équation 1 & 4)**  
L'équation (1) présente la mise à jour standard de Q-Learning où l'on ajuste progressivement la valeur $$Q(s, a)$$ avec un taux d'apprentissage $$\alpha$$. L'équation (4) présente une version refactorisée utilisant $$1 - \alpha$$ pour actualiser les anciennes valeurs.  

### **SARSA (Équation 2 & 5)**  
SARSA est une méthode qui suit une politique en cours au lieu d'une politique optimale. L'équation (2) est la version standard tandis que l'équation (5) montre une variante utilisant $$1 - \alpha$$ pour stabiliser l'apprentissage.  

### **Caractéristiques :**  
- Utilisation d'une fonction de valeur : $$Q(s, a)$$.  
- Apprentissage indirect : L'agent apprend une fonction de valeur d'abord, puis dérive une politique optimale.  
- Convient aux environnements discrets et utilise souvent des algorithmes off-policy comme Q-Learning.  

### **Avantages :**  
- Simple à implémenter pour des environnements discrets.  
- Convergence garantie sous certaines conditions.  

### **Inconvénients :**  
- Ne fonctionne pas bien pour les espaces d'actions continus.  
- L'apprentissage peut être lent.  

---  

## **2. Approche Policy-Based (Basée sur les Politiques)**  
### **Définition :**  
L'approche Policy-Based consiste à apprendre directement une politique qui indique à l’agent **quelle action entreprendre dans chaque état**. Contrairement à l'approche Value-Based, elle n’essaie pas d’estimer les valeurs d'état ou d'action.  

### **Gradient de Politique (Équation 3 & 6)**  
L'équation (3) représente l'algorithme standard de REINFORCE qui ajuste les paramètres $$\theta$$ en fonction du gradient de la politique. L'équation (6) montre une variante qui combine $$1 - \alpha$$ pour stabiliser l'apprentissage.  

### **Caractéristiques :**  
- Apprentissage direct d'une politique $$\pi(a|s)$$.  
- Fonctionne bien avec des espaces d'actions continus.  
- Peut apprendre des politiques stochastiques.  

### **Avantages :**  
- S'adapte aux environnements complexes et continus.  
- Évite les approximations de fonction de valeur instables.  

### **Inconvénients :**  
- Convergence moins garantie par rapport aux méthodes Value-Based.  
- Peut nécessiter un grand nombre d'échantillons.  

---  

## **3. Comparaison Value-Based vs Policy-Based**  
| Critère                        | Approche Value-Based          | Approche Policy-Based            |
|------------------------------|-----------------------------|-------------------------------|
| Fonction de valeur            | Oui, estimée par $$Q(s, a)$$. | Non, politique apprise directement. |
| Apprentissage                 | Indirect via fonction de valeur. | Direct par optimisation de politique. |
| Algorithmes courants           | Q-Learning, SARSA.            | REINFORCE, PPO, TRPO.           |
| Stabilité de l'apprentissage    | Convergence garantie.         | Peut être instable.             |

---  

## **4. Quand utiliser chaque approche ?**  
- **Approche Value-Based :** Lorsque l'espace d'actions est **discret** et qu'une méthode stable est nécessaire.  
- **Approche Policy-Based :** Lorsque l'espace d'actions est **continu** ou que l'on souhaite apprendre une politique stochastique capable de s'adapter à des environnements complexes.
