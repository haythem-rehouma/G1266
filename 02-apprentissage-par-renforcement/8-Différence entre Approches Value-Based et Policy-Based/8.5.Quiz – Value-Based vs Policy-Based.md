# **Quiz – Value-Based vs Policy-Based**  

**Objectif** : *Comprendre les différences fondamentales entre les deux grandes approches de l’apprentissage par renforcement*



# **Question 1 — Que cherche à apprendre une méthode Value-Based ?**  
**Enoncé :**  
Explique ce que fait une méthode basée sur les valeurs. Que signifie « apprendre une fonction de valeur » ?

**Réponse :**  
> Une méthode Value-Based cherche à estimer combien "vaut" un état ou une action.  
> Elle construit une fonction mathématique qui dit : « Si je suis dans cet état et que je fais cette action, quelle sera la récompense totale que je peux espérer à long terme ? ».  
> Par exemple, dans un jeu, cela revient à dire : « Si je suis dans telle position et que je saute, vais-je gagner beaucoup de points ? ».  
> Ensuite, l’agent choisit **toujours** l’action qui a la **valeur estimée la plus élevée**.


# **Question 2 — Que cherche à apprendre une méthode Policy-Based ?**  
**Enoncé :**  
Explique ce que fait une méthode basée sur les politiques. Que veut dire « apprendre une politique directement » ?

**Réponse :**  
> Une méthode Policy-Based ne cherche pas à estimer des valeurs.  
> Elle apprend **directement une stratégie** : dans chaque situation, quelle action faut-il faire ?  
> On appelle cette stratégie une **politique**.  
> Par exemple, au lieu de calculer combien vaut chaque coup dans un jeu, l’agent apprend directement à jouer comme un expert : « Dans cette situation, je saute. Dans celle-ci, je recule. »  
> Cela rend l’apprentissage souvent plus rapide et mieux adapté aux environnements complexes.


# **Question 3 — Quelle méthode est la plus adaptée aux actions continues ?**  
**Enoncé :**  
Certaines situations ne permettent pas de choisir entre 3 ou 4 actions fixes. Par exemple : accélérer d’un peu, beaucoup, ou énormément. Quelle méthode est la mieux adaptée à ce genre de problème ?

**Réponse :**  
> Les méthodes **Policy-Based** sont beaucoup mieux adaptées aux actions continues.  
> Pourquoi ? Parce qu’elles peuvent **apprendre une distribution fluide** d’actions, et pas seulement choisir parmi une liste.  
> Par exemple, pour un robot qui contrôle la vitesse de ses bras, il faut pouvoir ajuster les mouvements de manière très fine – ce que ne peut pas faire une méthode Value-Based.



# **Question 4 — Donne un exemple simple d’algorithme basé sur les valeurs.**  
**Réponse :**  
> **Q-Learning** est un algorithme Value-Based classique.  
> Il apprend une table : pour chaque situation et chaque action possible, il stocke une valeur.  
> Ensuite, il choisit l’action avec la meilleure valeur.  
> Par exemple, si tourner à droite dans un labyrinthe donne +10, il va l’apprendre et répéter ce comportement.



# **Question 5 — Donne un exemple simple d’algorithme basé sur les politiques.**  
**Réponse :**  
> **REINFORCE** est un exemple classique de méthode Policy-Based.  
> Il ne stocke pas de valeurs, mais modifie directement la stratégie utilisée par l’agent.  
> Il regarde : « Ai-je obtenu beaucoup de récompenses ? Si oui, renforçons les décisions qui m’y ont mené. »



# **Question 6 — Que signifie le mot "politique" dans les méthodes Policy-Based ?**  
**Réponse :**  
> En apprentissage par renforcement, une **politique** est comme un **mode d’emploi** que l’agent suit pour choisir ses actions.  
> C’est une règle du type : « Si je vois tel état, je fais telle action. »  
> Elle peut être **déterministe** (toujours la même action) ou **stochastique** (choisir une action au hasard selon une probabilité).  
> La méthode Policy-Based apprend cette règle, petit à petit, en la rendant de plus en plus efficace.



# **Question 7 — Quelle est la principale différence entre Value-Based et Policy-Based ?**  
**Réponse :**  
> Les méthodes Value-Based apprennent **indirectement** : elles calculent d’abord combien valent les actions, puis choisissent la meilleure.  
> Les méthodes Policy-Based apprennent **directement** la bonne décision à prendre, sans estimer de valeur.  
> En résumé :  
> - Value-Based = « Combien vaut chaque action ? »  
> - Policy-Based = « Quelle action dois-je faire directement ? »


## **Question 8 — Pourquoi les méthodes Value-Based ne fonctionnent pas bien avec des actions continues ?**  
**Réponse :**  
> Parce qu’on ne peut pas créer une table avec **une infinité d’actions possibles**.  
> Les méthodes Value-Based veulent évaluer chaque action, mais si les actions sont des nombres (par exemple 0.01, 0.02, 0.03, etc.), cela devient impossible à gérer.  
> On dit que ces méthodes ne scalent pas bien avec des espaces d’actions **grands ou infinis**.  
> Policy-Based, elles, peuvent produire directement des actions sous forme de probabilité continue.



# **Question 9 — Pourquoi utilise-t-on des politiques stochastiques ?**  
**Réponse :**  
> Une politique **stochastique** signifie que l’agent **ne fait pas toujours la même action**, même dans la même situation.  
> Cela est très utile pour **explorer** l’environnement.  
> Exemple : si l’agent ne fait que ce qu’il connaît déjà, il peut passer à côté d’une stratégie bien meilleure.  
> En ajoutant de l’aléatoire (choix probabiliste), il découvre de nouvelles possibilités.



# **Question 10 — Si on veut apprendre directement une stratégie sans passer par l’évaluation des actions, quelle approche doit-on utiliser ?**  
**Réponse :**  
> Il faut choisir une méthode **Policy-Based**, car c’est elle qui apprend une stratégie directement.  
> Elle ne se base pas sur une fonction de valeur, mais ajuste **la façon d’agir de l’agent**, en observant ce qui fonctionne bien ou non.  
> Cela peut être plus rapide dans certains cas, et indispensable pour les environnements complexes ou incertains.

