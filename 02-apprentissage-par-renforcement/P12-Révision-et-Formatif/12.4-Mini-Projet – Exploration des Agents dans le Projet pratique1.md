# **Mini-Projet – Exploration des Agents dans le Projet `pratique1`**

## Objectif

L’objectif est de vous familiariser avec les agents d’apprentissage par renforcement du projet `pratique1`, de les manipuler, de comparer leurs comportements et de présenter un rapport structuré en fin de séance.  

Vous allez démarrer à partir d’un **exemple guidé**, puis vous êtes libres d’explorer d’autres agents, layouts, paramètres ou fichiers Python du dossier.

<br/>

## Environnement

Le dossier `pratique1` contient les fichiers suivants :

```
gridworld.py
pacman.py
qlearningAgents.py
valueIterationAgents.py
mdp.py
learningAgents.py
featureExtractors.py
...
```

Vous pouvez utiliser **n’importe quel fichier, agent ou configuration** pour votre expérimentation, tant que cela repose sur les scripts fournis dans le projet.

<br/>


## Exemple d’expérimentation de départ

```bash
python pacman.py -p ValueIterationAgent -a iterations=100 -k 5 -l smallGrid
```

**Analyse :**

- `-p ValueIterationAgent` : agent basé sur l’itération de valeur
- `-a iterations=100` : effectue 100 mises à jour de la fonction de valeur
- `-k 5` : 5 épisodes sont joués
- `-l smallGrid` : layout de type grille simple

<br/>


## Travail à faire

Chaque étudiant (ou groupe) doit :

1. Reprendre l’exemple ci-dessus et l’exécuter
2. Modifier certains paramètres (nombre d’itérations, nombre d’épisodes, layout, agent utilisé, etc.)
3. Tester un autre agent, par exemple `QLearningAgent`, dans un environnement différent


<br/>


## À rendre

Un rapport court (en `.md`, `.pdf`, `.txt`, ou `.html`) contenant :

### 1. Objectif

> Quel comportement avez-vous voulu tester ou comparer ?

### 2. Liste des commandes exécutées

> Liste claire et expliquée de chaque commande testée (agent utilisé, paramètres, résultat attendu ou observé)





## **3. Analyse de votre configuration**

Dans le cadre du problème que vous avez étudié (environnement, agent, paramètres, comportement observé), répondez **rédigé et de manière structurée** aux questions suivantes :


### **3.1 Quel est l’agent que vous avez utilisé ?**

- Nom de l’agent :  
  (ex : `ValueIterationAgent`, `QLearningAgent`, etc.)

- Quel est son rôle général dans l’environnement ?  
  Expliquez en une ou deux phrases comment cet agent prend des décisions.


### **3.2 Quels sont les paramètres utilisés ?**

Pour chaque paramètre, expliquez sa signification et son rôle :

- `iterations = ?` :  
- `epsilon = ?` :  
- `alpha = ?` :  
- `gamma = ?` :  
- `--livingReward = ?` :  

> Si un paramètre ne s’applique pas à votre agent, indiquez-le clairement.



### **3.3 Quelle était la récompense utilisée ?**

- Quelle était la valeur de la récompense de survie (`--livingReward`) ?
- Quelle est la signification de cette valeur ?
- Selon vous, comment cette récompense a-t-elle influencé le comportement de l’agent ?



### **3.4 Votre agent utilise-t-il un apprentissage en ligne ou hors-ligne ?**

- Définissez d’abord ce qu’est un apprentissage **en ligne** et **hors-ligne**.
- Dites ensuite dans **votre cas** ce qui s’applique.
- Justifiez en expliquant **quand et comment** l’agent apprend dans votre configuration.



### **3.5 L’approche est-elle basée sur la valeur, la politique ou est-elle hybride ?**

- Définissez brièvement ce qu’est une approche :
  - **Basée sur la valeur**
  - **Basée sur la politique**
  - **Hybride (Actor-Critic)**

- Identifiez **la catégorie de votre agent**.
- Justifiez avec des exemples précis :
  - Utilise-t-il une fonction $$Q(s,a)$$ ou $$V(s)$$ ?
  - Cherche-t-il directement à optimiser $$\pi(a|s)$$ ?
  - Contient-il deux modules distincts (actor + critic) ?



### **3.6 Pensez-vous que l’approche que vous avez utilisée est adaptée à votre problème ?**

- Pourquoi ce type d’agent (et d’apprentissage) est-il pertinent ?
- Dans quels cas **auriez-vous préféré un autre type d’agent ou d’approche** ?
- Quels avantages ou limites avez-vous observés ?




## Exemple d'analyse expérimentale – Tableau des paramètres et comportements

Complétez le tableau ci-dessous en testant plusieurs configurations **en changeant un ou plusieurs paramètres à la fois**. Après chaque test, **notez brièvement le comportement observé** de l’agent.

> ⚠️ Pour chaque ligne : **vous devez vraiment exécuter la commande correspondante** et **observer attentivement** ce que fait Pacman.

---

### **Tableau à compléter**

| # | Agent utilisé         | Iterations | Epsilon | Alpha | Gamma | LivingReward | Comportement observé |
|---|------------------------|------------|---------|-------|--------|---------------|-----------------------|
| 1 | `ValueIterationAgent` | 10         | —       | —     | —      | 0             | Pacman tourne en rond, hésite, ne prend pas toujours la nourriture. |
| 2 | `ValueIterationAgent` | 100        | —       | —     | —      | 0             | Pacman va plus directement à la nourriture. |
| 3 | `ValueIterationAgent` | 100        | —       | —     | —      | -1            | Pacman essaie d’éviter de trop bouger, arrive plus vite à la fin. |
| 4 | `QLearningAgent`      | —          | 0.3     | 0.5   | 0.9    | 0             | Exploration active, Pacman prend parfois des chemins inattendus. |
| 5 | `QLearningAgent`      | —          | 0.05    | 0.5   | 0.9    | 0             | Pacman reste dans un comportement appris, peu d'exploration. |
| 6 | `QLearningAgent`      | —          | 0.3     | 0.1   | 0.9    | 0             | Apprentissage lent, actions peu efficaces au début. |
| 7 | `QLearningAgent`      | —          | 0.3     | 0.5   | 0.5    | 0             | Ne valorise pas les récompenses futures, comportements plus myopes. |
| 8 | `QLearningAgent`      | —          | 0.3     | 0.5   | 0.9    | -1            | Cherche à finir vite pour éviter la pénalité constante. |
| 9 | `QLearningAgent`      | —          | 0.3     | 0.5   | 0.9    | 2             | Prend des chemins plus longs, profite de la récompense de vivre. |


### Conseils 

- Ne changez **qu’un paramètre à la fois** pour mieux comprendre son impact.
- Essayez au moins **3 à 4 configurations différentes** par agent.
- Vous pouvez aussi tester d'autres layouts : `smallGrid`, `mediumGrid`, `mediumClassic`, etc.











### 4. Observations et analyse

> - Quel comportement Pacman adopte-t-il avec différents paramètres ?  
> - Y a-t-il une différence notable entre les agents ?  
> - Que se passe-t-il si on change la récompense de survie ? ou `epsilon`, `alpha` ?






### 5. Conclusion

> Ce que vous avez appris sur l’apprentissage par renforcement dans ce type d’environnement.

<br/>


### 6. Présentation

Une courte présentation (3 à 5 minutes) pourra être faite à la fin de la séance si le temps le permet. Sinon, les rapports seront lus et évalués après la remise.
