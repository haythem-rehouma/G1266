# **Approches en Apprentissage par Renforcement : Valeur vs Politique**  

## **Concepts et Définitions**

En apprentissage par renforcement, les algorithmes peuvent être classés selon la manière dont ils **apprennent à choisir les meilleures actions**. Trois grandes familles dominent :

- Les **approches basées sur la valeur** cherchent à apprendre la **valeur d’un état** ou d’une **action**.
- Les **approches basées sur la politique** cherchent à **apprendre directement la politique optimale** sans passer par la notion de valeur.
- Les **approches hybrides** combinent les deux pour tirer parti de leurs avantages respectifs.

---

### **1. Approches Basées sur la Valeur (Value-Based Methods)**

#### **Définition :**  
Ces méthodes visent à apprendre une **fonction de valeur** (par exemple $$Q(s, a)$$ ou $$V(s)$$) qui estime la qualité d’un état ou d’une action. L’agent **choisit l’action qui maximise la valeur estimée**.

#### **Caractéristiques :**
- Apprentissage centré sur les récompenses futures attendues.
- Utilise des techniques de mise à jour telles que **Bellman Equation**.
- Nécessite souvent une stratégie d'exploration comme ε-greedy.

#### **Exemples d'algorithmes :**
- **Q-learning**
- **SARSA**
- **Deep Q-Network (DQN)**

> **Exemple d’utilisation :** Environnements discrets comme les jeux Atari, où l’on peut apprendre une Q-table ou une Q-network.

---

### **2. Approches Basées sur la Politique (Policy-Based Methods)**

#### **Définition :**  
Ces méthodes **apprennent directement une politique $$\pi(a|s)$$** qui donne la probabilité de choisir une action dans un état donné, sans passer par une fonction de valeur.

#### **Caractéristiques :**
- Plus adaptées aux environnements avec **actions continues**.
- Permet une **exploration plus fine** grâce à des distributions de probabilité.
- Risque de convergence vers des **politiques sous-optimales** si mal régulé.

#### **Exemples d'algorithmes :**
- **REINFORCE**
- **Policy Gradient**
- **Proximal Policy Optimization (PPO)**

> **Exemple d’utilisation :** Contrôle de robot avec des mouvements continus (bras robotique, drone, etc.).

---

### **3. Approches Hybrides : Actor-Critic**

#### **Définition :**  
Ces méthodes **combinent une fonction de valeur (Critic)** et une **politique (Actor)**. L’acteur choisit les actions et le critique évalue ces actions.

#### **Caractéristiques :**
- Combine la stabilité des approches par valeur avec la flexibilité des politiques.
- Meilleure convergence dans des environnements complexes.
- Très utilisées dans les environnements modernes avec espace d’action continu.

#### **Exemples d'algorithmes :**
- **Actor-Critic**
- **Advantage Actor-Critic (A2C)**
- **Deep Deterministic Policy Gradient (DDPG)**
- **Twin Delayed DDPG (TD3)**

> **Exemple d’utilisation :** Simulations réalistes dans MuJoCo, OpenAI Gym pour tâches locomotrices complexes.

---

### **4. Comparaison des Approches**

| Critère                     | Approches par Valeur          | Approches par Politique      | Approches Hybrides (Actor-Critic) |
|---------------------------- |-------------------------------|------------------------------|-----------------------------------|
| Type d'action               | Discrètes                     | Continues ou discrètes       | Surtout continues                 |
| Représentation              | Fonction de valeur            | Politique paramétrique       | Les deux                         |
| Complexité                 | Moyenne                       | Élevée                       | Élevée                           |
| Convergence                | Parfois instable              | Stable mais lente            | Plus stable                       |
| Explorabilité              | Limitée (ε-greedy)            | Plus naturelle (stochastique) | Contrôlée via l’actor            |
| Exemple d’environnement    | Jeux Atari                    | Drone, robotique             | Simulation locomotion (MuJoCo)   |

---

### **5. Quand utiliser quelle approche ?**

| Situation                                                        | Méthode conseillée               |
|------------------------------------------------------------------|----------------------------------|
| Environnement à **actions discrètes**, faible complexité         | **Q-learning**, **SARSA**        |
| Environnement à **actions continues**, besoin de finesse         | **Policy Gradient**, **PPO**     |
| Environnement **complexe avec dynamique non linéaire**           | **Actor-Critic**, **A2C**, **TD3** |
| Entraînement en **simulation avec transfert dans le réel**       | **DDPG**, **SAC**, **PPO**       |

