# État du parcours "Cycle de Vie d’un Projet RL"

1. **Introduction générale au cycle RL**
2. **Quel est le problème à résoudre ?**
3. **Créer un environnement**
4. **Choisir les actions possibles**
5. **Définir les récompenses**
6. **Laisser l’agent apprendre** (entraînement par essais/erreurs)
7. **Tester ce qu’il a appris**
8. **Ce qu’on peut faire ensuite** (extensions simples, bonus)




# Annexe - Cycle de vie en **apprentissage par renforcement (RL)**

# 1 - Cycle de vie d’un projet en apprentissage **supervisé** / **non supervisé**

### **Apprentissage supervisé**

1. **Collecte des données** (ex. : images de chats/chiens avec étiquettes)
2. **Prétraitement des données** (nettoyage, normalisation)
3. **Séparation en train/test**
4. **Choix du modèle** (régression, SVM, réseau de neurones…)
5. **Entraînement sur des données labellisées**
6. **Évaluation (accuracy, F1, etc.)**
7. **Déploiement du modèle**

### **Apprentissage non supervisé**

1. **Collecte des données** (sans étiquettes)
2. **Prétraitement**
3. **Choix du modèle (k-means, PCA...)**
4. **Entraînement basé sur des patterns**
5. **Évaluation via des métriques indirectes (silhouette, etc.)**
6. **Interprétation des clusters/composantes**
7. **Utilisation/déploiement**



# 2 - **Apprentissage par renforcement (RL)**

Là où ça change radicalement, c’est que **le modèle interagit avec un environnement**, il n’a **aucune donnée au début**, il **explore** par essais-erreurs.

Voici les **7 étapes du cycle RL** illustrées avec l'exemple **CartPole (DQN)** :



### **1. Formulate Problems**

> **Que cherche-t-on à faire ?**
> Ex : Garder un poteau en équilibre sur un chariot (CartPole).



### **2. Create Environment**

> **Où agit l’agent ?**
> Ex : On utilise un environnement comme `CartPole-v1` dans OpenAI Gym.
>
> * Observation : position, vitesse, angle
> * Actions possibles : gauche ou droite


### **3. Define Reward**

> **Qu’est-ce qui est considéré comme "bon" ?**
>
> * +1 à chaque timestep où le poteau est encore debout
> * 0 si le poteau tombe ou sort de l’écran



### **4. Create Agent**

> C’est **l’intelligence artificielle** qui va apprendre.
> Ex : un réseau de neurones (DQN) avec :
>
> * Des couches denses
> * Une fonction de prédiction des Q-valeurs : `Q(s, a)`
> * Un mécanisme d’exploration (`ε-greedy`)



### **5. Train Agent**

> L’agent agit dans l’environnement, reçoit des récompenses, apprend :
>
> * Il stocke ses expériences (états, actions, récompenses)
> * Il rejoue les expériences (experience replay)
> * Il ajuste ses poids avec l'équation de Bellman :
>
> > $Q(s,a) = r + \gamma \max_{a'} Q(s', a')$



### **6. Validate Agent**

> Évalue si l’agent **a vraiment appris** :
>
> * Combien de temps il garde le poteau droit ?
> * Courbes de récompense moyenne
> * Courbe de `ε` (exploration)



### **7. Deploy Policy**

> Si l’agent est performant :
>
> * On **déploie sa politique** dans un robot, un jeu, ou un simulateur.
> * Il n’a **plus besoin d’apprendre**, il **agit** directement.

##  Différences essentielles entre supervisé et RL

| Aspect        | Supervisé                      | Renforcement                         |
| ------------- | ------------------------------ | ------------------------------------ |
| Données       | Disponibles dès le début       | Générées par interaction             |
| Apprentissage | À partir d’exemples labellisés | Par essais-erreurs                   |
| Objectif      | Minimiser une erreur           | Maximiser une récompense             |
| Évaluation    | Sur un dataset de test         | Par performance dans l’environnement |
| Environnement | Aucun                          | Nécessaire (simulation ou réel)      |
| Exploration   | Inexistante                    | Cruciale (ε-greedy, etc.)            |


