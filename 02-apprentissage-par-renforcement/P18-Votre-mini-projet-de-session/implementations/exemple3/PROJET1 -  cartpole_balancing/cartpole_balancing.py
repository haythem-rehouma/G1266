# -*- coding: utf-8 -*-
"""CartPole Balancing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ewX2Uw18Vaw7yKOGN9YNKfL3DFp6UheZ
"""

!pip install gym gymnasium stable-baselines3

import gymnasium as gym
# Imports the Gymnasium library to create and interact with reinforcement learning environments.

from stable_baselines3 import PPO
# Imports the Proximal Policy Optimization (PPO) algorithm from Stable Baselines3.

from stable_baselines3.common.vec_env import DummyVecEnv
# Imports a wrapper to vectorize the environment for parallel processing of environments.

import matplotlib.pyplot as plt
# Imports matplotlib to create plots and display environment frames.

from IPython import display as ipythondisplay
# Imports display functions from IPython for clearing and showing environment frames in Jupyter notebooks.

import time
# Imports the time module to manage sleep intervals and timeouts.

env_name = 'CartPole-v1'
# Defines the name of the environment as 'CartPole-v1'.

env = gym.make(env_name, render_mode='rgb_array')
# Creates the CartPole-v1 environment in Gym with 'rgb_array' render mode for visualization.

# Function to display frames
def show_frame(frame):
    plt.figure(figsize=(8,6))
    # Sets the figure size for displaying the frame.

    plt.imshow(frame)
    # Displays the frame as an image.

    plt.axis('off')
    # Hides axis ticks and labels for a cleaner frame display.

    ipythondisplay.clear_output(wait=True)
    # Clears the previous output to prevent displaying multiple frames at once.

    ipythondisplay.display(plt.gcf())
    # Displays the current figure (frame) in the output.

    plt.close()
    # Closes the plot to avoid memory leaks.

# Random Agent Visualization
for episode in range(1, 10):
    score = 0
    # Initializes the score for each episode.

    state, _ = env.reset()
    # Resets the environment and gets the initial state for a new episode.

    done = False
    truncated = False
    # Flags to track if the episode is over or truncated.

    while not (done or truncated):
        frame = env.render()
        # Renders the current environment frame.

        show_frame(frame)
        # Displays the rendered frame.

        action = env.action_space.sample()
        # Samples a random action from the action space.

        n_state, reward, done, truncated, info = env.step(action)
        # Applies the sampled action to the environment and gets the next state, reward, and done/truncated flags.

        score += reward
        # Updates the score with the received reward.

    # Display the score after the episode ends
    print(f'Episode: {episode}, Score: {score}')
    # Prints the score after the episode finishes.

    time.sleep(2)
    # Pauses for 2 seconds before starting the next episode.

env.close()
# Closes the environment after all episodes are completed.

# Training the PPO agent
env = DummyVecEnv([lambda: gym.make(env_name)])
# Wraps the environment in a DummyVecEnv for parallelization (even if thereâ€™s only one environment).

model = PPO('MlpPolicy', env, verbose=1)
# Initializes a PPO model with a multilayer perceptron (MLP) policy.

model.learn(total_timesteps=20000)
# Trains the PPO agent for 20,000 time steps.

model.save('/content/ppo_model')
# Saves the trained PPO model to a file.

# Evaluate PPO Agent
env = gym.make(env_name, render_mode='rgb_array')
# Recreates the CartPole-v1 environment with rendering for evaluation.

obs, _ = env.reset()
# Resets the environment to get the initial observation.

for episode in range(1, 2):
    score = 0
    # Initializes the score for the evaluation episode.

    done = False
    truncated = False
    # Flags to track if the episode is over or truncated.

    start_time = time.time()
    # Records the start time of the episode to track timeout.

    while not (done or truncated):
        frame = env.render()
        # Renders the current frame of the environment.

        show_frame(frame)
        # Displays the rendered frame.

        action, _ = model.predict(obs)
        # Uses the trained PPO model to predict the best action based on the current observation.

        obs, reward, done, truncated, info = env.step(action)
        # Takes a step in the environment with the chosen action, getting the next observation and other information.

        score += reward
        # Updates the score with the received reward.

        if time.time() - start_time >= 20:
            # If more than 20 seconds have passed since the episode started.

            print(f'Episode: {episode}, Score: {score} (Timed out after 20 seconds)')
            # Prints the score and a message indicating the episode was timed out.

            break
            # Exits the loop.

    if not (done or truncated):
        # If the episode was not done or truncated by the environment.

        print(f'Episode: {episode}, Score: {score} (Completed)')
        # Prints the score and a message indicating the episode was completed normally.

env.close()
# Closes the environment after evaluation.

