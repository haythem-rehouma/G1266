{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## **1. Ce que fait ce code**\n","- Ce programme utilise une intelligence artificielle (IA) pour apprendre à équilibrer un bâton sur un chariot dans un jeu appelé **CartPole-v1**.\n","- On commence par **regarder l’environnement** en action avec un agent qui joue au hasard.\n","- Ensuite, on entraîne une IA (agent PPO) pour **maîtriser le jeu**.\n","- Enfin, on observe l’agent entraîné jouer et réussir à garder le bâton en équilibre.\n","\n","---\n","\n","## **2. Installation des bibliothèques**\n","```python\n","!pip install gym gymnasium stable-baselines3\n","```\n","- **Pourquoi ?**\n","  - `gym` et `gymnasium` : Ces bibliothèques créent des environnements pour entraîner des agents, comme **CartPole**.\n","  - `stable-baselines3` : Une bibliothèque qui contient des algorithmes comme **PPO** pour entraîner des agents en apprentissage par renforcement.\n","\n","---\n","\n","## **3. Importation des bibliothèques**\n","```python\n","import gymnasium as gym\n","from stable_baselines3 import PPO\n","from stable_baselines3.common.vec_env import DummyVecEnv\n","import matplotlib.pyplot as plt\n","from IPython import display as ipythondisplay\n","import time\n","```\n","- **Pourquoi toutes ces bibliothèques ?**\n","  - `gym` : Pour créer et gérer notre environnement de jeu.\n","  - `PPO` : L’algorithme utilisé pour entraîner l’agent.\n","  - `DummyVecEnv` : Permet de gérer des environnements en parallèle (même si ici, on en utilise un seul).\n","  - `matplotlib` et `IPython` : Pour afficher les frames du jeu (images) pendant qu’il est joué.\n","  - `time` : Pour ajouter des pauses ou suivre le temps écoulé.\n","\n","---\n","\n","## **4. Création de l’environnement**\n","```python\n","env_name = 'CartPole-v1'\n","env = gym.make(env_name, render_mode='rgb_array')\n","```\n","- **Qu’est-ce que ça fait ?**\n","  - `env_name = 'CartPole-v1'` : On choisit le jeu **CartPole**, où le but est d’équilibrer un bâton.\n","  - `gym.make(env_name)` : On crée l’environnement du jeu.\n","  - `render_mode='rgb_array'` : On active l’affichage visuel pour voir l’agent jouer.\n","\n","---\n","\n","## **5. Fonction pour afficher les frames**\n","```python\n","def show_frame(frame):\n","    plt.figure(figsize=(8,6))\n","    plt.imshow(frame)\n","    plt.axis('off')\n","    ipythondisplay.clear_output(wait=True)\n","    ipythondisplay.display(plt.gcf())\n","    plt.close()\n","```\n","- **Qu’est-ce que ça fait ?**\n","  - Cette fonction prend une image (frame) et l’affiche joliment sans axes inutiles.\n","  - Elle nettoie l’écran à chaque nouveau frame pour montrer un mouvement fluide.\n","\n","---\n","\n","## **6. Observation d’un agent aléatoire**\n","```python\n","for episode in range(1, 10):\n","    score = 0\n","    state, _ = env.reset()\n","    done = False\n","    truncated = False\n","\n","    while not (done or truncated):\n","        frame = env.render()\n","        show_frame(frame)\n","        action = env.action_space.sample()\n","        n_state, reward, done, truncated, info = env.step(action)\n","        score += reward\n","\n","    print(f'Episode: {episode}, Score: {score}')\n","    time.sleep(2)\n","env.close()\n","```\n","- **Explication des étapes :**\n","  1. **`for episode in range(1, 10):`**  \n","     - On joue 9 parties (épisodes) pour observer l’environnement.\n","  2. **`env.reset()`**  \n","     - On réinitialise l’environnement pour chaque nouvelle partie.\n","  3. **`action = env.action_space.sample()`**  \n","     - On choisit une action **au hasard** (l’agent n’est pas encore intelligent).\n","  4. **`env.step(action)`**  \n","     - On effectue cette action dans le jeu et on récupère :\n","       - Le nouvel état (*n_state*).\n","       - La récompense (*reward*).\n","       - Si la partie est terminée (*done*).\n","       - Si elle a été coupée (*truncated*).\n","  5. **Afficher le score**  \n","     - Une fois la partie terminée, on affiche le score (nombre de frames où le bâton est resté en équilibre).\n","\n","---\n","\n","## **7. Entraînement de l’agent avec PPO**\n","```python\n","env = DummyVecEnv([lambda: gym.make(env_name)])\n","model = PPO('MlpPolicy', env, verbose=1)\n","model.learn(total_timesteps=20000)\n","model.save('/content/ppo_model')\n","```\n","- **Étapes clés :**\n","  1. **`DummyVecEnv`**  \n","     - On enveloppe l’environnement pour qu’il fonctionne bien avec `stable-baselines3` (même avec un seul environnement).\n","  2. **`PPO('MlpPolicy', env, verbose=1)`**  \n","     - On initialise un modèle PPO :\n","       - `MlpPolicy` : Une politique basée sur un réseau de neurones simple.\n","       - `verbose=1` : On affiche des informations pendant l’entraînement.\n","  3. **`model.learn(total_timesteps=20000)`**  \n","     - On entraîne l’agent pour 20 000 étapes de jeu.\n","  4. **`model.save('/content/ppo_model')`**  \n","     - On sauvegarde le modèle pour pouvoir le réutiliser plus tard.\n","\n","---\n","\n","## **8. Évaluation de l’agent entraîné**\n","```python\n","env = gym.make(env_name, render_mode='rgb_array')\n","obs, _ = env.reset()\n","\n","for episode in range(1, 2):\n","    score = 0\n","    done = False\n","    truncated = False\n","    start_time = time.time()\n","\n","    while not (done or truncated):\n","        frame = env.render()\n","        show_frame(frame)\n","        action, _ = model.predict(obs)\n","        obs, reward, done, truncated, info = env.step(action)\n","        score += reward\n","\n","        if time.time() - start_time >= 20:\n","            print(f'Episode: {episode}, Score: {score} (Timed out after 20 seconds)')\n","            break\n","\n","    if not (done or truncated):\n","        print(f'Episode: {episode}, Score: {score} (Completed)')\n","\n","env.close()\n","```\n","- **Qu’est-ce que ça fait ?**\n","  - On recharge l’environnement pour observer notre **agent entraîné**.\n","  - À chaque étape, l’agent choisit une action basée sur ce qu’il a appris avec PPO (**`model.predict(obs)`**).\n","  - On observe si l’agent peut maintenir le bâton en équilibre et on affiche son score.\n","\n","---\n","\n","## **9. Résumé**\n","\n","### **Avant l’entraînement :**  \n","- L’agent joue au hasard (choisit des actions au pif).  \n","- Résultat : Il échoue souvent, car il ne \"sait\" rien.\n","\n","### **Pendant l’entraînement :**  \n","- L’agent utilise **PPO**, un algorithme d’apprentissage, pour apprendre :\n","  - Quels mouvements garderont le bâton en équilibre.\n","  - Ce qu’il doit éviter (faire tomber le bâton).\n","\n","### **Après l’entraînement :**  \n","- L’agent peut jouer **comme un pro**, gardant le bâton en équilibre pendant longtemps.\n","\n"],"metadata":{"id":"tDmdKq7XhvuS"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cSPMH6A8_J4A","outputId":"ef3363d0-cad9-410b-f4c5-ddb66b5b9d27","executionInfo":{"status":"ok","timestamp":1732241472456,"user_tz":300,"elapsed":4621,"user":{"displayName":"REHOUMA Haythem","userId":"15938645019134290079"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n","Collecting gymnasium\n","  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n","Collecting stable-baselines3\n","  Downloading stable_baselines3-2.4.0-py3-none-any.whl.metadata (4.5 kB)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.26.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (3.1.0)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n","Collecting farama-notifications>=0.0.1 (from gymnasium)\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n","Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.5.1+cu121)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.2.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.8.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.16.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13->stable-baselines3) (1.3.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (4.55.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.4.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (24.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (11.0.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable-baselines3) (3.0.2)\n","Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading stable_baselines3-2.4.0-py3-none-any.whl (183 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Installing collected packages: farama-notifications, gymnasium, stable-baselines3\n","Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0 stable-baselines3-2.4.0\n"]}],"source":["!pip install gym gymnasium stable-baselines3\n"]},{"cell_type":"code","source":["import gymnasium as gym\n","# Imports the Gymnasium library to create and interact with reinforcement learning environments.\n","\n","from stable_baselines3 import PPO\n","# Imports the Proximal Policy Optimization (PPO) algorithm from Stable Baselines3.\n","\n","from stable_baselines3.common.vec_env import DummyVecEnv\n","# Imports a wrapper to vectorize the environment for parallel processing of environments.\n","\n","import matplotlib.pyplot as plt\n","# Imports matplotlib to create plots and display environment frames.\n","\n","from IPython import display as ipythondisplay\n","# Imports display functions from IPython for clearing and showing environment frames in Jupyter notebooks.\n","\n","import time\n","# Imports the time module to manage sleep intervals and timeouts."],"metadata":{"id":"F1qOzTu2BiVM","executionInfo":{"status":"ok","timestamp":1732241485925,"user_tz":300,"elapsed":13471,"user":{"displayName":"REHOUMA Haythem","userId":"15938645019134290079"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["\n","env_name = 'CartPole-v1'\n","# Defines the name of the environment as 'CartPole-v1'.\n","\n","env = gym.make(env_name, render_mode='rgb_array')\n","# Creates the CartPole-v1 environment in Gym with 'rgb_array' render mode for visualization."],"metadata":{"id":"TUF5Jyb9BmQy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732241485926,"user_tz":300,"elapsed":10,"user":{"displayName":"REHOUMA Haythem","userId":"15938645019134290079"}},"outputId":"3fed0dde-1c12-441f-8f44-7115b7b934ea"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"code","source":["\n","# Function to display frames\n","def show_frame(frame):\n","    plt.figure(figsize=(8,6))\n","    # Sets the figure size for displaying the frame.\n","\n","    plt.imshow(frame)\n","    # Displays the frame as an image.\n","\n","    plt.axis('off')\n","    # Hides axis ticks and labels for a cleaner frame display.\n","\n","    ipythondisplay.clear_output(wait=True)\n","    # Clears the previous output to prevent displaying multiple frames at once.\n","\n","    ipythondisplay.display(plt.gcf())\n","    # Displays the current figure (frame) in the output.\n","\n","    plt.close()\n","    # Closes the plot to avoid memory leaks."],"metadata":{"id":"59vR9fUyBoQP","executionInfo":{"status":"ok","timestamp":1732241485926,"user_tz":300,"elapsed":4,"user":{"displayName":"REHOUMA Haythem","userId":"15938645019134290079"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["\n","# Random Agent Visualization\n","for episode in range(1, 10):\n","    score = 0\n","    # Initializes the score for each episode.\n","\n","    state, _ = env.reset()\n","    # Resets the environment and gets the initial state for a new episode.\n","\n","    done = False\n","    truncated = False\n","    # Flags to track if the episode is over or truncated.\n","\n","    while not (done or truncated):\n","        frame = env.render()\n","        # Renders the current environment frame.\n","\n","        show_frame(frame)\n","        # Displays the rendered frame.\n","\n","        action = env.action_space.sample()\n","        # Samples a random action from the action space.\n","\n","        n_state, reward, done, truncated, info = env.step(action)\n","        # Applies the sampled action to the environment and gets the next state, reward, and done/truncated flags.\n","\n","        score += reward\n","        # Updates the score with the received reward.\n","\n","    # Display the score after the episode ends\n","    print(f'Episode: {episode}, Score: {score}')\n","    # Prints the score after the episode finishes.\n","\n","    time.sleep(2)\n","    # Pauses for 2 seconds before starting the next episode.\n","\n","env.close()\n","# Closes the environment after all episodes are completed."],"metadata":{"id":"eNUHvQF9Bp3N","colab":{"base_uri":"https://localhost:8080/","height":467},"outputId":"d846288c-b301-4357-d8a2-a9c6394683a8","executionInfo":{"status":"ok","timestamp":1732241532744,"user_tz":300,"elapsed":46822,"user":{"displayName":"REHOUMA Haythem","userId":"15938645019134290079"}}},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 800x600 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Episode: 9, Score: 19.0\n"]}]},{"cell_type":"code","source":["# Training the PPO agent\n","env = DummyVecEnv([lambda: gym.make(env_name)])\n","# Wraps the environment in a DummyVecEnv for parallelization (even if there’s only one environment).\n","\n","model = PPO('MlpPolicy', env, verbose=1)\n","# Initializes a PPO model with a multilayer perceptron (MLP) policy.\n","\n","model.learn(total_timesteps=20000)\n","# Trains the PPO agent for 20,000 time steps.\n","\n","model.save('/content/ppo_model')\n","# Saves the trained PPO model to a file."],"metadata":{"id":"hu227JsIBtkg","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8d23c9bc-118b-4004-a16e-a744207c21f4","executionInfo":{"status":"ok","timestamp":1732241587878,"user_tz":300,"elapsed":55140,"user":{"displayName":"REHOUMA Haythem","userId":"15938645019134290079"}}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]},{"output_type":"stream","name":"stdout","text":["Using cuda device\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["-----------------------------\n","| time/              |      |\n","|    fps             | 379  |\n","|    iterations      | 1    |\n","|    time_elapsed    | 5    |\n","|    total_timesteps | 2048 |\n","-----------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 383         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 10          |\n","|    total_timesteps      | 4096        |\n","| train/                  |             |\n","|    approx_kl            | 0.008243034 |\n","|    clip_fraction        | 0.0884      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.687      |\n","|    explained_variance   | -0.000748   |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 5.29        |\n","|    n_updates            | 10          |\n","|    policy_gradient_loss | -0.0141     |\n","|    value_loss           | 49.7        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 388         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 15          |\n","|    total_timesteps      | 6144        |\n","| train/                  |             |\n","|    approx_kl            | 0.008972422 |\n","|    clip_fraction        | 0.0649      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.666      |\n","|    explained_variance   | 0.0435      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 14.9        |\n","|    n_updates            | 20          |\n","|    policy_gradient_loss | -0.0181     |\n","|    value_loss           | 44.1        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 406         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 20          |\n","|    total_timesteps      | 8192        |\n","| train/                  |             |\n","|    approx_kl            | 0.007101617 |\n","|    clip_fraction        | 0.0648      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.635      |\n","|    explained_variance   | 0.173       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 18.8        |\n","|    n_updates            | 30          |\n","|    policy_gradient_loss | -0.0175     |\n","|    value_loss           | 56.7        |\n","-----------------------------------------\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 415        |\n","|    iterations           | 5          |\n","|    time_elapsed         | 24         |\n","|    total_timesteps      | 10240      |\n","| train/                  |            |\n","|    approx_kl            | 0.00694094 |\n","|    clip_fraction        | 0.0576     |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -0.617     |\n","|    explained_variance   | 0.285      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 26.1       |\n","|    n_updates            | 40         |\n","|    policy_gradient_loss | -0.0145    |\n","|    value_loss           | 64.7       |\n","----------------------------------------\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 411        |\n","|    iterations           | 6          |\n","|    time_elapsed         | 29         |\n","|    total_timesteps      | 12288      |\n","| train/                  |            |\n","|    approx_kl            | 0.00889156 |\n","|    clip_fraction        | 0.0642     |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -0.605     |\n","|    explained_variance   | 0.395      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 38.7       |\n","|    n_updates            | 50         |\n","|    policy_gradient_loss | -0.0128    |\n","|    value_loss           | 65.3       |\n","----------------------------------------\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 418        |\n","|    iterations           | 7          |\n","|    time_elapsed         | 34         |\n","|    total_timesteps      | 14336      |\n","| train/                  |            |\n","|    approx_kl            | 0.00762836 |\n","|    clip_fraction        | 0.0957     |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -0.596     |\n","|    explained_variance   | 0.637      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 24.3       |\n","|    n_updates            | 60         |\n","|    policy_gradient_loss | -0.0137    |\n","|    value_loss           | 52.1       |\n","----------------------------------------\n","------------------------------------------\n","| time/                   |              |\n","|    fps                  | 420          |\n","|    iterations           | 8            |\n","|    time_elapsed         | 38           |\n","|    total_timesteps      | 16384        |\n","| train/                  |              |\n","|    approx_kl            | 0.0062172096 |\n","|    clip_fraction        | 0.078        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -0.576       |\n","|    explained_variance   | 0.571        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 30.7         |\n","|    n_updates            | 70           |\n","|    policy_gradient_loss | -0.0125      |\n","|    value_loss           | 71.3         |\n","------------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 418         |\n","|    iterations           | 9           |\n","|    time_elapsed         | 44          |\n","|    total_timesteps      | 18432       |\n","| train/                  |             |\n","|    approx_kl            | 0.009070322 |\n","|    clip_fraction        | 0.076       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.571      |\n","|    explained_variance   | 0.701       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 6.76        |\n","|    n_updates            | 80          |\n","|    policy_gradient_loss | -0.00939    |\n","|    value_loss           | 46.2        |\n","-----------------------------------------\n","------------------------------------------\n","| time/                   |              |\n","|    fps                  | 422          |\n","|    iterations           | 10           |\n","|    time_elapsed         | 48           |\n","|    total_timesteps      | 20480        |\n","| train/                  |              |\n","|    approx_kl            | 0.0036165232 |\n","|    clip_fraction        | 0.0416       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -0.564       |\n","|    explained_variance   | 0.705        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 14           |\n","|    n_updates            | 90           |\n","|    policy_gradient_loss | -0.00497     |\n","|    value_loss           | 40.5         |\n","------------------------------------------\n"]}]},{"cell_type":"code","source":["# Evaluate PPO Agent\n","env = gym.make(env_name, render_mode='rgb_array')\n","# Recreates the CartPole-v1 environment with rendering for evaluation.\n","\n","obs, _ = env.reset()\n","# Resets the environment to get the initial observation.\n","\n","for episode in range(1, 2):\n","    score = 0\n","    # Initializes the score for the evaluation episode.\n","\n","    done = False\n","    truncated = False\n","    # Flags to track if the episode is over or truncated.\n","\n","    start_time = time.time()\n","    # Records the start time of the episode to track timeout.\n","\n","    while not (done or truncated):\n","        frame = env.render()\n","        # Renders the current frame of the environment.\n","\n","        show_frame(frame)\n","        # Displays the rendered frame.\n","\n","        action, _ = model.predict(obs)\n","        # Uses the trained PPO model to predict the best action based on the current observation.\n","\n","        obs, reward, done, truncated, info = env.step(action)\n","        # Takes a step in the environment with the chosen action, getting the next observation and other information.\n","\n","        score += reward\n","        # Updates the score with the received reward.\n","\n","        if time.time() - start_time >= 20:\n","            # If more than 20 seconds have passed since the episode started.\n","\n","            print(f'Episode: {episode}, Score: {score} (Timed out after 20 seconds)')\n","            # Prints the score and a message indicating the episode was timed out.\n","\n","            break\n","            # Exits the loop.\n","\n","    if not (done or truncated):\n","        # If the episode was not done or truncated by the environment.\n","\n","        print(f'Episode: {episode}, Score: {score} (Completed)')\n","        # Prints the score and a message indicating the episode was completed normally.\n","\n","env.close()\n","# Closes the environment after evaluation.\n","\n"],"metadata":{"id":"2vbYWpKrBtn2","colab":{"base_uri":"https://localhost:8080/","height":485},"outputId":"ea11b09f-3169-4959-c5db-2be55a74e848","executionInfo":{"status":"ok","timestamp":1732242326571,"user_tz":300,"elapsed":20482,"user":{"displayName":"REHOUMA Haythem","userId":"15938645019134290079"}}},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 800x600 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAoAAAAGxCAYAAADyAwe2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAN4UlEQVR4nO3dv49l513H8e8598541xvb8SZZBeRgVyaQFBCnwwU90CCq1CBaF1vzD2zjggK5jkSJhBKliAQFEhThl4ITRIQFCuAo2SjGOJP1ztx7HgpLIHtnZu9mzvje53xeL2m7R7vf6tm3nuecc4fWWisAAGKM+x4AAICPlwAEAAgjAAEAwghAAIAwAhAAIIwABAAIIwABAMIIQACAMAIQACCMAAQACCMAAQDCCEAAgDACEAAgjAAEAAgjAAEAwghAAIAwAhAAIIwABAAIIwABAMIIQACAMAIQACCMAAQACCMAAQDCCEAAgDACEAAgjAAEAAgjAAEAwghAAIAwAhAAIIwABAAIIwABAMIIQACAMAIQACCMAAQACCMAAQDCCEAAgDACEAAgjAAEAAgjAAEAwghAAIAwAhAAIIwABAAIIwABAMIIQACAMAIQACCMAAQACCMAAQDCCEAAgDACEAAgjAAEAAgjAAEAwghAAIAwAhAAIIwABAAIIwABAMIIQACAMAIQACCMAAQACCMAAQDCCEAAgDACEAAgjAAEAAgjAAEAwghAAIAwAhAAIIwABAAIIwABAMIIQACAMAIQACCMAAQACCMAAQDCCEAAgDACEAAgjAAEAAgjAAEAwghAAIAwAhAAIIwABAAIIwABAMIIQACAMAIQACCMAAQACCMAAQDCCEAAgDACEAAgjAAEAAgjAAEAwghAAIAwAhAAIIwABAAIIwABAMIIQACAMAIQACCMAAQACCMAAQDCCEAAgDACEAAgjAAEAAgjAAEAwghAAIAwAhAAIIwABAAIIwABAMIIQACAMAIQACCMAAQACCMAAQDCCEAAgDACEAAgjAAEAAgjAAEAwghAAIAwAhAAIIwABAAIIwABAMIIQACAMAIQACCMAAQACCMAAQDCCEAAgDACEAAgjAAEAAgjAAEAwghAAIAwAhAAIIwABAAIIwABAMIIQACAMAIQACCMAAQACCMAAQDCCEAAgDACEAAgjAAEAAgjAAEAwghAAIAwAhAAIIwABAAIIwABAMIIQACAMAIQACCMAAQACCMAAQDCCEAAgDACEAAgjAAEAAgjAAEAwghAAIAwAhAAIIwABAAIIwABAMIIQACAMAIQACCMAAQACCMAAQDCCEAAgDACEAAgjAAEAAgjAAEAwghAAIAwAhAAIIwABAAIIwABAMIIQACAMAIQACCMAAQACCMAAQDCCEAAgDACEAAgjAAEAAgjAAEAwghAAIAwAhAAIIwABAAIIwABAMIIQACAMAIQACCMAAQACCMAAQDCCEAAgDACEAAgjAAEAAgjAAEAwghAAIAwAhAAIIwABAAIs973AABc7nvf+ON69/vf3mntzU+9UF/8vT+65omA3jkBBDh47QmWtpo2p9c3CrAIAhBgQVqbatqe7XsM4MAJQIAlaa2mjQAELicAAZbEFTCwAwEIsCCtmitg4LEEIMCStFbNFTDwGAIQYEmaE0Dg8QQgwIK01mrabvY9BnDgBCDAorRqTgCBxxCAAAvSfAYG2IEABFgSzwACOxCAAIviO4DA4wlAgAN36zMv1tHTz+20dnv6oN57+1+ueSKgdwIQ4MDdvP1CrW8+s9PaaXNaJ/f//XoHAronAAEO3Lha1zDYroH52FEADtwgAIGZ2VEADtw4rqtG2zUwHzsKwIFzAgjMzY4CcOAEIDA3OwrAgXMFDMzNjgJw4JwAAnOzowAcuGF9VIMTQGBGdhSAAzeO6yongMCM7CgAB86HoIG52VEADtywWrsCBmZlRwE4cOPq6IlPAFtr1zQNsAQCEGCB2vZs3yMAB0wAAixMa62mjQAELiYAAZamtdpuT/c9BXDABCDA4rSazgQgcDEBCLAwrTXPAAKXEoAAi+MZQOByAhBgaVqryTOAwCUEIMDStFbNCSBwCQEIsDCtqibPAAKXEIAAS9OaAAQuJQABFsdLIMDlBCDAwnzwSyBeAgEuJgABOvDML/5yPfXcnZ3WTmfv10/e+tY1TwT0TAACdOD41vO1furWTmvbtK2H7/34micCeiYAATowrNY1DLZsYB52E4AODOO6arRlA/OwmwB0YFytnAACs7GbAHRgGNc1jKt9jwEshAAE6IBnAIE52U0AOuAZQGBOdhOADoxOAIEZ2U0AOjCsPAMIzEcAAnRgGJ0AAvOxmwB0YFytavAMIDATuwlABz44AXQFDMxDAAJ0YFw9+VvArbVrmgbonQAEWKJW1bZn+54COFACEGCRWk2b030PARwoAQiwRK3VduMEEDifAARYoFZV0+bhvscADpQABFii1mpyAghcQAACLFKrtvUMIHA+AQiwUE4AgYsIQIAFaq6AgUsIQIBFajW5AgYuIAABlqi5AgYuJgABFsmHoIGLCUCABWqt1eSn4IALCECATjz3uS/U059+cae109nD+tGbf3HNEwG9EoAAnVgd36zx6HjH1a22pw+udR6gXwIQoBPDuK5hWO17DGABBCBAJ8ZxVcNo2wauzk4C0Ilhta5hdAIIXJ0ABOjEMK5qGGzbwNXZSQA6MYwrJ4DALAQgQCc+uAK2bQNXZycB6MQwrqucAAIzEIAAnRhXK5+BAWYhAAE6MYyugIF52EkAOuElEGAuAhCgE+Nq7TMwwCzsJACdGH6OXwJpbbqmaYCeCUCApWqtps3ZvqcADpAABFioVlXT5nTfYwAHSAACLFaraesEEHiUAARYqlY1nT3c9xTAARKAAIvlGUDgfAIQYMGmrWcAgUcJQICl8hYwcAEBCLBQ3gIGLiIAARaseQsYOIcABFis5gQQOJcABFiq5goYOJ8ABFisVlsvgQDnEIAAHXn2c1+sT/zCyzutnTan9YN/+Po1TwT0SAACdGS1Oqpxtd55fZuma5wG6JUABOjIMK5qGFf7HgPonAAE6IgABOYgAAE6MqzWAhC4MgEI0BEngMAcBCBARwQgMAcBCNCRYVzVMNi6gauxiwB0ZFita3iCz8AAnEcAAnTEFTAwBwEI0JFxXNUw2rqBq7GLAHRkGNc1DE4AgasRgAAdcQUMzEEAAnRkGMcnfgu4TdtrmgbolQAEWLhpc7bvEYADIwABFq3VtD3d9xDAgRGAAAu3dQIIfIQABFiyVjVtnAACHyYAARatVTt7uO8hgAMjAAEWbtq6AgY+TAACLJwrYOCjBCDAkrXmMzDAIwQgwIK1Kp+BAR4hAAEWzhUw8FHrfQ8AsGSttdpu5/0ptmmanmSC2pw+rM1mM9u/PwxDrVZ+jxh6NrTW2r6HAFiq+/fv1507d2b9O//gt79Uv/9bX6phGB679vRsU3/y539bX/3mP83279+9e7fu3bs3298HfPxcAQN05lv//F/119/5j53WHh+t6w9/58vXPBHQGwEI0JnNdqrN5kmugQE+zDOAAJ3ZbKfabD8IwP8++0y99eDX6p2zz9a2HdXN1f/UZ4//rV6+9Xd7nhI4ZAIQoDObaaqz7VTff/D5+u7Jq9Xq/58FPNk+X289eL7efvhy/ebtP93jlMAhcwUM0JnNZqp//emv1ndPfqNajVU1PPLnwfSJ+suffGWvcwKHSwACdObFX/ndeukLX6lWl32KZaj3p1siEDiXAATozDge1Wp1vMPKoTZtl3VAGgEIABBGAAIAhBGAAABhBCBAZ9789tfrH//+zx67bj08rC8/+42PYSKgNwIQoDPvvPODOv7pX9Uv3XjzwjVHw/v16898s54/+uHHOBnQCx+CBujMdjvVcb1bL918s54af1Y/PH2pTjafrKnWdTz+rD65/lG98NT36tPHb+97VOBACUCAzrSq+pvv/Ge9e/K1Op1u1HubT9XD9nRNbaz1cFpPr96rZ9c//r/109T2NyxwkIbW2k47w2uvvXbNowAsz4MHD+qNN97Y9xizeuWVV+rVV1/d9xjAOV5//fWd1u18Anj79u2fdxaAWCcnJ/seYXY3btzwfwJ0bucTQACe3P379+vOnTv7HmNWd+/erXv37u17DOAKvAUMABBGAAIAhBGAAABhBCAAQBgBCAAQRgACAIQRgAAAYQQgAEAYAQgAEEYAAgCEEYAAAGEEIABAGAEIABBGAAIAhBlaa23fQwAs1VK32GEY9j0CcAXrfQ8AsGRCCThEroABAMIIQACAMAIQACCMAAQACCMAAQDCCEAAgDACEAAgjAAEAAgjAAEAwghAAIAwAhAAIIwABAAIIwABAMIIQACAMAIQACCMAAQACCMAAQDCCEAAgDACEAAgjAAEAAgjAAEAwghAAIAwAhAAIIwABAAIIwABAMIIQACAMAIQACCMAAQACCMAAQDCCEAAgDACEAAgjAAEAAgjAAEAwghAAIAwAhAAIIwABAAIIwABAMIIQACAMAIQACCMAAQACCMAAQDCCEAAgDACEAAgjAAEAAgjAAEAwghAAIAwAhAAIIwABAAIIwABAMIIQACAMAIQACCMAAQACCMAAQDCCEAAgDACEAAgjAAEAAgjAAEAwghAAIAwAhAAIIwABAAIIwABAMIIQACAMAIQACCMAAQACCMAAQDCCEAAgDACEAAgjAAEAAgjAAEAwghAAIAwAhAAIIwABAAIIwABAMIIQACAMAIQACCMAAQACCMAAQDCCEAAgDACEAAgjAAEAAgjAAEAwghAAIAwAhAAIIwABAAIIwABAMIIQACAMAIQACCMAAQACCMAAQDCCEAAgDACEAAgjAAEAAgjAAEAwghAAIAwAhAAIIwABAAI87+00f1nDymXOwAAAABJRU5ErkJggg==\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Episode: 1, Score: 125.0 (Timed out after 20 seconds)\n","Episode: 1, Score: 125.0 (Completed)\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"SXLNfCSFiEFF"}},{"cell_type":"markdown","source":["------------------------\n","------------------------"],"metadata":{"id":"NanVNDIliLWd"}},{"cell_type":"markdown","source":["# **Programme complet commenté ligne par ligne : CartPole avec PPO**\n","\n","\n","#### **1. Installation des bibliothèques**\n","```python\n","!pip install gym gymnasium stable-baselines3\n","```\n","- **Pourquoi ?**\n","  - Ce programme utilise trois bibliothèques essentielles :\n","    - **`gym` et `gymnasium`** : Ces bibliothèques permettent de créer des environnements standards pour entraîner un agent d’apprentissage par renforcement (AR).  \n","    Exemple : CartPole, où un chariot doit équilibrer un bâton.\n","    - **`stable-baselines3`** : Une bibliothèque qui contient des algorithmes d’apprentissage par renforcement avancés comme PPO.\n","\n","- **Que fait cette ligne ?**\n","  - Installe ces bibliothèques sur l’ordinateur ou dans Colab pour qu’on puisse les utiliser dans le programme.\n","\n","---\n","\n","#### **2. Importation des bibliothèques**\n","```python\n","import gymnasium as gym\n","```\n","- **Pourquoi ?**\n","  - `gym` permet de créer et gérer un **environnement** dans lequel un agent peut interagir.  \n","  Exemple : CartPole.\n","\n","```python\n","from stable_baselines3 import PPO\n","```\n","- **Pourquoi ?**\n","  - On importe l’algorithme **Proximal Policy Optimization (PPO)**. C’est un algorithme d’apprentissage par renforcement qui aide un agent à apprendre en testant différentes actions tout en s’assurant de ne pas trop explorer les mauvaises.\n","\n","```python\n","from stable_baselines3.common.vec_env import DummyVecEnv\n","```\n","- **Pourquoi ?**\n","  - Stable-Baselines3 nécessite que les environnements soient vectorisés, même si on n’utilise qu’un seul environnement.  \n","  **DummyVecEnv** est une enveloppe (wrapper) pour convertir un environnement simple en un environnement vectorisé.\n","\n","```python\n","import matplotlib.pyplot as plt\n","from IPython import display as ipythondisplay\n","import time\n","```\n","- **Pourquoi ?**\n","  - **`matplotlib`** : Pour afficher les images des frames du jeu.\n","  - **`IPython.display`** : Pour afficher chaque image dans un notebook Jupyter ou Colab.\n","  - **`time`** : Pour gérer les pauses entre les épisodes ou chronométrer une exécution.\n","\n","---\n","\n","#### **3. Création de l’environnement**\n","```python\n","env_name = 'CartPole-v1'\n","```\n","- **Que fait cette ligne ?**\n","  - On stocke le nom de l’environnement que l’on veut utiliser.  \n","  **CartPole-v1** est un environnement classique où un chariot doit équilibrer un bâton.\n","\n","```python\n","env = gym.make(env_name, render_mode='rgb_array')\n","```\n","- **Que fait cette ligne ?**\n","  - On utilise `gym.make()` pour créer l’environnement CartPole.\n","  - **Paramètre `render_mode='rgb_array'`** :\n","    - Permet d’afficher visuellement l’environnement sous forme d’images (frames).\n","\n","---\n","\n","#### **4. Fonction pour afficher les frames**\n","```python\n","def show_frame(frame):\n","    plt.figure(figsize=(8,6))  # Définit la taille de l’image.\n","    plt.imshow(frame)          # Affiche l’image du jeu (frame actuelle).\n","    plt.axis('off')            # Supprime les axes pour rendre l’affichage plus propre.\n","    ipythondisplay.clear_output(wait=True)  # Efface l’ancienne frame (affichage propre).\n","    ipythondisplay.display(plt.gcf())       # Affiche la frame actuelle.\n","    plt.close()               # Ferme l’image pour éviter d’utiliser trop de mémoire.\n","```\n","- **Pourquoi ?**\n","  - Cette fonction prend une **image** en entrée et l’affiche joliment, sans axes ou distractions.\n","  - Elle rend le jeu fluide en remplaçant chaque image précédente par la nouvelle.\n","\n","---\n","\n","#### **5. Visualisation d’un agent aléatoire**\n","```python\n","for episode in range(1, 10):\n","```\n","- **Pourquoi ?**\n","  - On joue 9 épisodes pour **observer** comment l’environnement réagit à des actions aléatoires.\n","\n","```python\n","    score = 0  # Initialisation du score (nombre de frames où le bâton reste en équilibre).\n","    state, _ = env.reset()  # Réinitialise l’environnement à l’état initial.\n","    done = False  # Flag pour suivre si la partie est terminée.\n","    truncated = False  # Flag si la partie est coupée (par exemple, dépassement du temps).\n","```\n","\n","```python\n","    while not (done or truncated):  # Tant que la partie n’est pas terminée...\n","        frame = env.render()  # Récupère l’image actuelle (frame).\n","        show_frame(frame)  # Affiche la frame.\n","        action = env.action_space.sample()  # Choisit une action **au hasard**.\n","        n_state, reward, done, truncated, info = env.step(action)  # Exécute l’action dans l’environnement.\n","        score += reward  # Ajoute la récompense reçue au score.\n","```\n","- **Détails étape par étape :**\n","  - **`env.action_space.sample()`** : L’agent choisit une action au hasard parmi celles disponibles.\n","  - **`env.step(action)`** : L’environnement :\n","    1. Exécute l’action.\n","    2. Renvoie :\n","       - L’état suivant (*n_state*).\n","       - Une récompense (*reward*).\n","       - Si la partie est terminée (*done*).\n","       - D’autres informations (*info*).\n","\n","```python\n","    print(f'Episode: {episode}, Score: {score}')  # Affiche le score de l’épisode.\n","    time.sleep(2)  # Pause de 2 secondes entre les épisodes.\n","```\n","\n","---\n","\n","#### **6. Entraînement de l’agent avec PPO**\n","```python\n","env = DummyVecEnv([lambda: gym.make(env_name)])\n","model = PPO('MlpPolicy', env, verbose=1)\n","model.learn(total_timesteps=20000)\n","model.save('/content/ppo_model')\n","```\n","- **Étapes expliquées :**\n","  1. **`DummyVecEnv([lambda: gym.make(env_name)])`** :  \n","     - Convertit l’environnement en une version compatible avec PPO.\n","  2. **`PPO('MlpPolicy', env, verbose=1)`** :  \n","     - Initialise un modèle PPO :\n","       - **`MlpPolicy`** : Utilise un réseau de neurones simple (Multilayer Perceptron).\n","       - **`verbose=1`** : Affiche les détails pendant l’entraînement.\n","  3. **`model.learn(total_timesteps=20000)`** :  \n","     - Entraîne l’agent pendant 20 000 étapes (frames).\n","  4. **`model.save()`** :  \n","     - Sauvegarde le modèle entraîné.\n","\n","---\n","\n","#### **7. Évaluation de l’agent entraîné**\n","```python\n","env = gym.make(env_name, render_mode='rgb_array')\n","obs, _ = env.reset()\n","\n","for episode in range(1, 2):  # On évalue l’agent pour 1 épisode.\n","    score = 0\n","    done = False\n","    truncated = False\n","    start_time = time.time()  # Chronomètre pour limiter le temps d’un épisode.\n","\n","    while not (done or truncated):\n","        frame = env.render()  # Récupère l’image actuelle.\n","        show_frame(frame)  # Affiche la frame.\n","        action, _ = model.predict(obs)  # L’agent choisit la meilleure action selon son modèle.\n","        obs, reward, done, truncated, info = env.step(action)  # Exécute l’action choisie.\n","        score += reward  # Ajoute la récompense.\n","\n","        if time.time() - start_time >= 20:  # Si l’épisode dure plus de 20 secondes.\n","            print(f'Episode: {episode}, Score: {score} (Timed out after 20 seconds)')\n","            break  # Arrête l’épisode.\n","\n","    if not (done or truncated):\n","        print(f'Episode: {episode}, Score: {score} (Completed)')\n","\n","env.close()  # Ferme l’environnement après évaluation.\n","```\n","\n","---\n","\n","### **Résumé**\n","1. **Agent aléatoire** :\n","   - Joue au hasard et échoue souvent (ne comprend rien).\n","2. **Entraînement avec PPO** :\n","   - L’agent apprend progressivement quelles actions garderont le bâton en équilibre.\n","3. **Agent entraîné** :\n","   - Joue comme un pro et maintient le bâton en équilibre pendant longtemps.\n"],"metadata":{"id":"tUvmI3fxiWWG"}},{"cell_type":"code","source":[],"metadata":{"id":"AfrRk3viDHVm","executionInfo":{"status":"ok","timestamp":1732241608655,"user_tz":300,"elapsed":7,"user":{"displayName":"REHOUMA Haythem","userId":"15938645019134290079"}}},"execution_count":8,"outputs":[]}]}