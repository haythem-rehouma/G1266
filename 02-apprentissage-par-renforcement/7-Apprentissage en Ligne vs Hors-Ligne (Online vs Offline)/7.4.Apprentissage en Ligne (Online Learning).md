
 # Apprentissage en Ligne (Online Learning)



## D√©finition   
L'apprentissage en ligne est un paradigme o√π le mod√®le ou l'agent apprend en temps r√©el, en mettant √† jour ses connaissances imm√©diatement apr√®s chaque nouvelle donn√©e re√ßue ou apr√®s chaque interaction avec l'environnement. L'apprentissage se fait de mani√®re incr√©mentale, souvent sans conserver un historique complet des donn√©es.


---
# 1. Apprentissage en Ligne en Apprentissage par Renforcement (Reinforcement Learning)
---

En Apprentissage par Renforcement (RL), l'apprentissage en ligne est particuli√®rement courant car l'agent apprend par l'interaction directe avec l'environnement. L'agent prend des d√©cisions, re√ßoit des r√©compenses ou des p√©nalit√©s, et ajuste sa strat√©gie en fonction de ces exp√©riences.


### Caract√©ristiques 
- Mises √† jour instantan√©es   
  L'agent ajuste ses param√®tres imm√©diatement apr√®s avoir re√ßu une r√©compense pour une action. Cela permet une adaptation rapide aux changements de l'environnement.  
- Exploration et Exploitation simultan√©es   
  L'agent doit trouver un √©quilibre entre l'exploration (essayer de nouvelles actions pour d√©couvrir de meilleures strat√©gies) et l'exploitation (utiliser ce qu'il a d√©j√† appris pour maximiser les r√©compenses).  
- Adaptation continue   
  L'apprentissage se poursuit tant que l'agent interagit avec l'environnement.  
- Mod√®les √† m√©moire courte 
  L'apprentissage peut √™tre r√©alis√© sans stocker l'ensemble des donn√©es historiques, mais en utilisant uniquement les donn√©es les plus r√©centes.  



### Exemples d‚ÄôAlgorithmes en Ligne en Apprentissage par Renforcement   

*Q-Learning*

  Mise √† jour de la fonction Q apr√®s chaque interaction via la r√®gle d'actualisation


  $$
  Q(s, a) \leftarrow (1 - \alpha) Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') \right]
  $$

  
  Ici, chaque r√©compense re√ßue est imm√©diatement utilis√©e pour ajuster l'estimation de la qualit√© de chaque action.

*SARSA (State-Action-Reward-State-Action)*

 
Contrairement au Q-Learning qui choisit toujours l'action optimale, SARSA met √† jour la fonction de valeur en fonction de l'action r√©ellement choisie par la politique actuelle
SARSA est dit "On-Policy" car il apprend en suivant la politique courante, ce qui le rend plus stable dans certains environnements dynamiques.

  $$
  Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma Q(s', a') - Q(s, a) \right]
  $$
  



*Deep Q-Learning (DQN) en ligne* 

Les r√©seaux de neurones profonds peuvent aussi apprendre en ligne lorsque les poids sont mis √† jour apr√®s chaque interaction ou petit lot de transitions.  
Dans cette approche, l'agent ajuste progressivement son r√©seau neuronal au fur et √† mesure de son interaction avec l'environnement.  

---
# 2. Apprentissage en Ligne en dehors de l‚ÄôApprentissage par Renforcement (Autres Types d‚ÄôApprentissage)
---

L'apprentissage en ligne ne se limite pas au RL. Il est √©galement pr√©sent dans d'autres paradigmes d'apprentissage, notamment   



## 2.1. Apprentissage Supervis√© en Ligne   
Il est possible d'entra√Æner des mod√®les supervis√©s de mani√®re en ligne lorsque les donn√©es arrivent en continu. Par exemple   

- Perceptron en ligne   
  Un mod√®le simple de r√©seau de neurones o√π chaque exemple est trait√© un par un, et les poids sont mis √† jour imm√©diatement apr√®s chaque pr√©diction incorrecte.  

- Algorithmes d'apprentissage adaptatif   
  Par exemple, l‚Äôalgorithme Stochastic Gradient Descent (SGD) en mode en ligne traite une seule instance de donn√©e √† chaque √©tape au lieu de calculer la moyenne d'un lot entier. Cela permet de s'adapter rapidement aux nouvelles donn√©es.  

> Exemple  Un syst√®me de d√©tection de spam qui ajuste son mod√®le √† chaque nouvel e-mail re√ßu.  



## 2.2. Apprentissage Non-Supervis√© en Ligne   
Certaines m√©thodes non-supervis√©es peuvent √™tre adapt√©es √† l'apprentissage en ligne   

- K-Means Incr√©mental   
  Les centres de clusters peuvent √™tre mis √† jour en temps r√©el au fur et √† mesure que de nouvelles donn√©es arrivent, au lieu de recalculer les clusters sur un ensemble complet.  

- M√©thodes de r√©duction de dimension adaptatives   
  Comme les Auto-encodeurs en ligne, qui ajustent leurs poids progressivement en fonction des nouvelles donn√©es.  

> Exemple  Un syst√®me de clustering qui regroupe en continu les donn√©es d'utilisateurs sur un site Web en fonction de leurs comportements r√©cents.  

---
# 3. Comparaison  Apprentissage en Ligne vs Apprentissage Hors-Ligne
---

| Crit√®re                  | Apprentissage en Ligne (Online)         | Apprentissage Hors-Ligne (Offline) |
|--------------------------|-----------------------------------------|-----------------------------------|
| Interaction avec l'environnement | En temps r√©el, au fur et √† mesure des interactions.  | Pas d'interaction pendant l'entra√Ænement. |
| Mises √† jour             | Imm√©diates apr√®s chaque interaction.   | Diff√©r√©es, apr√®s traitement complet des donn√©es. |
| Adaptabilit√©             | Tr√®s adaptatif aux changements.        | Peu adaptable si les donn√©es ne sont pas mises √† jour. |
| Co√ªt d'apprentissage      | √âlev√© si chaque interaction est co√ªteuse.  | Moins co√ªteux si les donn√©es sont disponibles en grand nombre. |
| Utilisation courante      | Jeux, robotique, syst√®mes dynamiques, recommandation en temps r√©el.  | Conduite autonome simul√©e, diagnostic m√©dical, d√©tection de fraudes. |
| Algorithmes principaux    | Q-Learning, SARSA, DQN, SGD en ligne, Perceptron en ligne. | Algorithmes bas√©s sur des batchs complets (Batch Learning). |



### Alors, est-ce que l'apprentissage en ligne existe en dehors du RL ?  
Oui, absolument.  
En apprentissage supervis√© et non-supervis√©, les algorithmes peuvent √™tre adapt√©s pour fonctionner en mode en ligne. L'apprentissage en ligne est principalement utilis√© lorsque   
- Les donn√©es arrivent en continu.  
- L'environnement change constamment.  
- Il est n√©cessaire d'adapter le mod√®le en temps r√©el.  



---
# Annexe 1 - Exemples d‚ÄôAlgorithmes en Ligne en Apprentissage par Renforcement   
----


*SARSA (State-Action-Reward-State-Action)* 
  Contrairement au Q-Learning qui choisit toujours l'action optimale, SARSA met √† jour la fonction de valeur en fonction de l'action r√©ellement choisie par la politique actuelle 


  $$
  Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma Q(s', a') - Q(s, a) \right]
  $$

  ou 

  $$
  Q(s, a) \leftarrow (1 - \alpha) Q(s, a) + \alpha \left[ r + \gamma Q(s', a') \right]
  $$

  > *Encore une fois, ne vous inqui√©tez pas si cette formule semble complexe. L'essentiel √† comprendre ici est que SARSA met √† jour la connaissance √† chaque interaction en suivant une politique courante, ce qui le rend l√©g√®rement diff√©rent de Q-Learning.*  



*Deep Q-Learning (DQN) en ligne*

  Ici, un r√©seau de neurones est utilis√© pour approximer la fonction Q. Contrairement au Q-Learning traditionnel qui stocke une table Q, DQN permet de g√©rer des environnements avec un tr√®s grand nombre d'√©tats.  
  L'apprentissage se fait en ligne lorsque les poids du r√©seau sont mis √† jour apr√®s chaque interaction ou apr√®s un petit lot de transitions appel√© mini-batch.  
  > *Ne vous inqui√©tez pas, nous allons voir en d√©tail comment cela fonctionne plus tard. Pour l'instant, retenez simplement que cette m√©thode permet d'apprendre m√™me lorsque l'espace d'√©tats est √©norme.*  





---
# Annexe 2 - Exemples d‚ÄôAlgorithmes qui peuvent √™tre consid√©r√© comme hors-ligne et aussi en Ligne en Apprentissage par Renforcement   
----



*Q-Learning*   
  Mise √† jour de la fonction Q apr√®s chaque interaction via la r√®gle d'actualisation 


$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)
$$



ou 

  
 $$
 Q(s, a) \leftarrow (1 - \alpha) Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') \right]
 $$



- $$\alpha $$ : Taux d'apprentissage (*learning rate*), qui d√©termine l'importance donn√©e aux nouvelles informations.  
- $$\gamma $$ : Facteur de discount (*discount factor*), qui contr√¥le l'importance des r√©compenses futures.  
- $$r$$ : R√©compense imm√©diate obtenue apr√®s avoir pris l'action $$a$$ dans l'√©tat $$s$$.  
- $$s'$$ : Nouvel √©tat atteint apr√®s l'action $$ a $$.  
- $$a'$$ : Action future choisie par l'agent pour estimer la r√©compense maximale potentielle.  


Cette √©quation correspond √† l'algorithme de **Q-Learning**, une m√©thode d'apprentissage par renforcement hors-ligne (*Off-Policy*). L'objectif est d'estimer la **fonction de valeur d'action** $$Q(s, a)$$ de mani√®re it√©rative en ajustant les valeurs selon l'erreur de pr√©diction entre la valeur actuelle et la valeur cible calcul√©e. La r√®gle de mise √† jour est donn√©e par :  





üí° Remarque :
Il est important de noter que Q-Learning est g√©n√©ralement consid√©r√© comme un algorithme d'apprentissage hors-ligne (Off-Policy) car il apprend une politique optimale ind√©pendamment de la politique suivie par l'agent.  
Cependant, il peut √©galement √™tre utilis√© en ligne, c'est-√†-dire que la mise √† jour de la fonction $$Q(s, a)$$ peut se faire apr√®s chaque interaction avec l'environnement.  
Ainsi, Q-Learning peut √™tre appliqu√© en ligne ou hors-ligne, selon la mani√®re dont les exp√©riences sont collect√©es et trait√©es.  


### Pourquoi c'est **Off-Policy** ?  
L'algorithme apprend une politique optimale $$\pi^*$$ ind√©pendamment de la politique suivie par l'agent. Par exemple, m√™me si l'agent explore l'environnement de mani√®re al√©atoire (via une politique $$\epsilon$$-greedy), l'estimation de la valeur $$Q(s', a')$$ se base sur l'action qui maximise cette valeur ‚Äî c'est pourquoi on utilise le terme $$\max$$.  




  > *Ne vous inqui√©tez pas si cette √©quation vous para√Æt complexe ! Nous allons la d√©tailler plus tard de mani√®re progressive. Pour l'instant, remarquez simplement qu'il s'agit d'un processus it√©ratif o√π l'on met √† jour progressivement la connaissance sur l'environnement apr√®s chaque interaction.*  









