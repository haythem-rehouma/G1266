
 # Apprentissage en Ligne (Online Learning)



## Définition   
L'apprentissage en ligne est un paradigme où le modèle ou l'agent apprend en temps réel, en mettant à jour ses connaissances immédiatement après chaque nouvelle donnée reçue ou après chaque interaction avec l'environnement. L'apprentissage se fait de manière incrémentale, souvent sans conserver un historique complet des données.


---
# 1. Apprentissage en Ligne en Apprentissage par Renforcement (Reinforcement Learning)
---

En Apprentissage par Renforcement (RL), l'apprentissage en ligne est particulièrement courant car l'agent apprend par l'interaction directe avec l'environnement. L'agent prend des décisions, reçoit des récompenses ou des pénalités, et ajuste sa stratégie en fonction de ces expériences.


### Caractéristiques 
- Mises à jour instantanées   
  L'agent ajuste ses paramètres immédiatement après avoir reçu une récompense pour une action. Cela permet une adaptation rapide aux changements de l'environnement.  
- Exploration et Exploitation simultanées   
  L'agent doit trouver un équilibre entre l'exploration (essayer de nouvelles actions pour découvrir de meilleures stratégies) et l'exploitation (utiliser ce qu'il a déjà appris pour maximiser les récompenses).  
- Adaptation continue   
  L'apprentissage se poursuit tant que l'agent interagit avec l'environnement.  
- Modèles à mémoire courte 
  L'apprentissage peut être réalisé sans stocker l'ensemble des données historiques, mais en utilisant uniquement les données les plus récentes.  



### Exemples d’Algorithmes en Ligne en Apprentissage par Renforcement   

*Q-Learning*

  Mise à jour de la fonction Q après chaque interaction via la règle d'actualisation


  $$
  Q(s, a) \leftarrow (1 - \alpha) Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') \right]
  $$

  
  Ici, chaque récompense reçue est immédiatement utilisée pour ajuster l'estimation de la qualité de chaque action.

*SARSA (State-Action-Reward-State-Action)*

 
Contrairement au Q-Learning qui choisit toujours l'action optimale, SARSA met à jour la fonction de valeur en fonction de l'action réellement choisie par la politique actuelle
SARSA est dit "On-Policy" car il apprend en suivant la politique courante, ce qui le rend plus stable dans certains environnements dynamiques.

  $$
  Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma Q(s', a') - Q(s, a) \right]
  $$
  



*Deep Q-Learning (DQN) en ligne* 

Les réseaux de neurones profonds peuvent aussi apprendre en ligne lorsque les poids sont mis à jour après chaque interaction ou petit lot de transitions.  
Dans cette approche, l'agent ajuste progressivement son réseau neuronal au fur et à mesure de son interaction avec l'environnement.  

---
# 2. Apprentissage en Ligne en dehors de l’Apprentissage par Renforcement (Autres Types d’Apprentissage)
---

L'apprentissage en ligne ne se limite pas au RL. Il est également présent dans d'autres paradigmes d'apprentissage, notamment   

---

### Apprentissage Supervisé en Ligne   
Il est possible d'entraîner des modèles supervisés de manière en ligne lorsque les données arrivent en continu. Par exemple   

- Perceptron en ligne   
  Un modèle simple de réseau de neurones où chaque exemple est traité un par un, et les poids sont mis à jour immédiatement après chaque prédiction incorrecte.  

- Algorithmes d'apprentissage adaptatif   
  Par exemple, l’algorithme Stochastic Gradient Descent (SGD) en mode en ligne traite une seule instance de donnée à chaque étape au lieu de calculer la moyenne d'un lot entier. Cela permet de s'adapter rapidement aux nouvelles données.  

> Exemple  Un système de détection de spam qui ajuste son modèle à chaque nouvel e-mail reçu.  

---

### Apprentissage Non-Supervisé en Ligne   
Certaines méthodes non-supervisées peuvent être adaptées à l'apprentissage en ligne   

- K-Means Incrémental   
  Les centres de clusters peuvent être mis à jour en temps réel au fur et à mesure que de nouvelles données arrivent, au lieu de recalculer les clusters sur un ensemble complet.  

- Méthodes de réduction de dimension adaptatives   
  Comme les Auto-encodeurs en ligne, qui ajustent leurs poids progressivement en fonction des nouvelles données.  

> Exemple  Un système de clustering qui regroupe en continu les données d'utilisateurs sur un site Web en fonction de leurs comportements récents.  

---
# 3. Comparaison  Apprentissage en Ligne vs Apprentissage Hors-Ligne
---

| Critère                  | Apprentissage en Ligne (Online)         | Apprentissage Hors-Ligne (Offline) |
|--------------------------|-----------------------------------------|-----------------------------------|
| Interaction avec l'environnement | En temps réel, au fur et à mesure des interactions.  | Pas d'interaction pendant l'entraînement. |
| Mises à jour             | Immédiates après chaque interaction.   | Différées, après traitement complet des données. |
| Adaptabilité             | Très adaptatif aux changements.        | Peu adaptable si les données ne sont pas mises à jour. |
| Coût d'apprentissage      | Élevé si chaque interaction est coûteuse.  | Moins coûteux si les données sont disponibles en grand nombre. |
| Utilisation courante      | Jeux, robotique, systèmes dynamiques, recommandation en temps réel.  | Conduite autonome simulée, diagnostic médical, détection de fraudes. |
| Algorithmes principaux    | Q-Learning, SARSA, DQN, SGD en ligne, Perceptron en ligne. | Algorithmes basés sur des batchs complets (Batch Learning). |



### Alors, est-ce que l'apprentissage en ligne existe en dehors du RL ?  
Oui, absolument.  
En apprentissage supervisé et non-supervisé, les algorithmes peuvent être adaptés pour fonctionner en mode en ligne. L'apprentissage en ligne est principalement utilisé lorsque   
- Les données arrivent en continu.  
- L'environnement change constamment.  
- Il est nécessaire d'adapter le modèle en temps réel.  




### Exemples d’Algorithmes en Ligne en Apprentissage par Renforcement   

- Q-Learning   
  Mise à jour de la fonction Q après chaque interaction via la règle d'actualisation 
  
  $$
  Q(s, a) \leftarrow (1 - \alpha) Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') \right]
  $$
  
  > *Ne vous inquiétez pas si cette équation vous paraît complexe ! Nous allons la détailler plus tard de manière progressive. Pour l'instant, remarquez simplement qu'il s'agit d'un processus itératif où l'on met à jour progressivement la connaissance sur l'environnement après chaque interaction.*  

---

- SARSA (State-Action-Reward-State-Action)   
  Contrairement au Q-Learning qui choisit toujours l'action optimale, SARSA met à jour la fonction de valeur en fonction de l'action réellement choisie par la politique actuelle 


  $$
  Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma Q(s', a') - Q(s, a) \right]
  $$
  
  > *Encore une fois, ne vous inquiétez pas si cette formule semble complexe. L'essentiel à comprendre ici est que SARSA met à jour la connaissance à chaque interaction en suivant une politique courante, ce qui le rend légèrement différent de Q-Learning.*  

---

- Deep Q-Learning (DQN) en ligne   
  Ici, un réseau de neurones est utilisé pour approximer la fonction Q. Contrairement au Q-Learning traditionnel qui stocke une table Q, DQN permet de gérer des environnements avec un très grand nombre d'états.  
  L'apprentissage se fait en ligne lorsque les poids du réseau sont mis à jour après chaque interaction ou après un petit lot de transitions appelé mini-batch.  
  > *Ne vous inquiétez pas, nous allons voir en détail comment cela fonctionne plus tard. Pour l'instant, retenez simplement que cette méthode permet d'apprendre même lorsque l'espace d'états est énorme.*  

