

| Bloc | Sujet | Fait ? |
|:----:|:-----------------------------|:-------:|
| 0 | Installer MLflow + d√©marrer serveur  |
| 1 | Premi√®re exp√©rience (`set_experiment`, `start_run`)  |
| 2 | Ajouter `requirements.txt` |
| 3 | Utiliser `set_tracking_uri`, `get_tracking_uri`  |
| 4 | Cr√©er une exp√©rience avec `create_experiment`  |
| 5 | Utiliser `active_run`, `last_active_run`  |
| 6 | Logger artefacts avec `log_artifacts`  |
| 7 | Ajouter des tags `set_tags` |
| 8 | Boucles : plusieurs `start_run()` dans le m√™me script  |
| 9 | G√©rer plusieurs exp√©riences diff√©rentes  |
| 10 | `mlflow.autolog` |
| 11 | D√©marrer en production (PostgreSQL + S3)  |
| 12 ‚Üí 24 | Plus avanc√© : evaluation, artefacts visuels, promotion de mod√®le, baselines, metrics customis√©es  |
| 25 | Promotion automatique (`MlflowClient`, `transition_model_version_stage`) |
| 26 | R√©cup√©ration automatique de la version | Ne plus fixer `version=1`, d√©tecter dynamiquement |
| 27 | Ajouter un **commentaire automatique** √† la version | Avec `set_model_version_tag` |
| 28 | D√©ploiement local simple (`mlflow models serve`) | Tester ton mod√®le via une API locale Flask-like |
| 29 | R√©sum√© visuel final du pipeline complet | (le "poster" final pour bien fixer tout) |




# Bloc 0 ‚Äì Code initial

```python
# 1. Importation des biblioth√®ques
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import ElasticNet

# 2. Chargement des donn√©es
data = pd.read_csv("red-wine-quality.csv")

# 3. S√©paration en variables explicatives et variable cible
X = data.drop(["quality"], axis=1)
y = data["quality"]

# 4. Division en train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# 5. Entra√Ænement d'un mod√®le ElasticNet
model = ElasticNet(alpha=0.5, l1_ratio=0.5)
model.fit(X_train, y_train)

# 6. Pr√©dictions
predictions = model.predict(X_test)

# 7. Affichage simple
print(predictions[:5])
```

---

# Bloc 1 ‚Äì Ajout 1 : Importer mlflow

**Instruction** :  
Ajoutez juste apr√®s les imports standards (**apr√®s la ligne 3**).

```python
import mlflow
import mlflow.sklearn
```

**Pourquoi** :  
Pr√©parer l'int√©gration avec MLflow pour tracer les exp√©riences.

**Code complet apr√®s Ajout 1** :

```python
# 1. Importation des biblioth√®ques
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import ElasticNet
import mlflow
import mlflow.sklearn

# 2. Chargement des donn√©es
data = pd.read_csv("red-wine-quality.csv")

# 3. S√©paration en variables explicatives et variable cible
X = data.drop(["quality"], axis=1)
y = data["quality"]

# 4. Division en train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# 5. Entra√Ænement d'un mod√®le ElasticNet
model = ElasticNet(alpha=0.5, l1_ratio=0.5)
model.fit(X_train, y_train)

# 6. Pr√©dictions
predictions = model.predict(X_test)

# 7. Affichage simple
print(predictions[:5])
```

---

# Bloc 2 ‚Äì Ajout 2 : D√©finir une exp√©rience

**Instruction** :  
Ajoutez juste avant de charger les donn√©es (**avant la ligne 8**).

```python
mlflow.set_experiment("exp_wine_quality")
```

**Pourquoi** :  
D√©finir l'exp√©rience MLflow √† laquelle le run sera rattach√©.

**Code complet apr√®s Ajout 2** :

```python
# 1. Importation des biblioth√®ques
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import ElasticNet
import mlflow
import mlflow.sklearn

# 2. D√©finir l'exp√©rience MLflow
mlflow.set_experiment("exp_wine_quality")

# 3. Chargement des donn√©es
data = pd.read_csv("red-wine-quality.csv")

# 4. S√©paration en variables explicatives et variable cible
X = data.drop(["quality"], axis=1)
y = data["quality"]

# 5. Division en train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# 6. Entra√Ænement d'un mod√®le ElasticNet
model = ElasticNet(alpha=0.5, l1_ratio=0.5)
model.fit(X_train, y_train)

# 7. Pr√©dictions
predictions = model.predict(X_test)

# 8. Affichage simple
print(predictions[:5])
```

---

# Bloc 3 ‚Äì Ajout 3 : D√©marrer un run

**Instruction** :  
Ajoutez juste avant l'entra√Ænement du mod√®le (**avant la ligne 17**).

```python
mlflow.start_run()
```

**Pourquoi** :  
D√©marrer la session de tracking MLflow pour le run courant.

**Code complet apr√®s Ajout 3** :

```python
# 1. Importation des biblioth√®ques
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import ElasticNet
import mlflow
import mlflow.sklearn

# 2. D√©finir l'exp√©rience MLflow
mlflow.set_experiment("exp_wine_quality")

# 3. Chargement des donn√©es
data = pd.read_csv("red-wine-quality.csv")

# 4. S√©paration en variables explicatives et variable cible
X = data.drop(["quality"], axis=1)
y = data["quality"]

# 5. Division en train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# 6. D√©marrer un run MLflow
mlflow.start_run()

# 7. Entra√Ænement d'un mod√®le ElasticNet
model = ElasticNet(alpha=0.5, l1_ratio=0.5)
model.fit(X_train, y_train)

# 8. Pr√©dictions
predictions = model.predict(X_test)

# 9. Affichage simple
print(predictions[:5])
```






















# Bloc 4 ‚Äì Ajout 4 : Logger un param√®tre

**Instruction** :  
Ajoutez juste apr√®s la cr√©ation du mod√®le (**apr√®s la ligne 21**).

```python
mlflow.log_param("alpha", 0.5)
mlflow.log_param("l1_ratio", 0.5)
```

**Pourquoi** :  
Sauvegarder les hyperparam√®tres utilis√©s pour cet entra√Ænement.

**Code complet apr√®s Ajout 4** :

```python
# 1. Importation des biblioth√®ques
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import ElasticNet
import mlflow
import mlflow.sklearn

# 2. D√©finir l'exp√©rience MLflow
mlflow.set_experiment("exp_wine_quality")

# 3. Chargement des donn√©es
data = pd.read_csv("red-wine-quality.csv")

# 4. S√©paration en variables explicatives et variable cible
X = data.drop(["quality"], axis=1)
y = data["quality"]

# 5. Division en train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# 6. D√©marrer un run MLflow
mlflow.start_run()

# 7. Entra√Ænement d'un mod√®le ElasticNet
model = ElasticNet(alpha=0.5, l1_ratio=0.5)
mlflow.log_param("alpha", 0.5)
mlflow.log_param("l1_ratio", 0.5)
model.fit(X_train, y_train)

# 8. Pr√©dictions
predictions = model.predict(X_test)

# 9. Affichage simple
print(predictions[:5])
```

---

# Bloc 5 ‚Äì Ajout 5 : Logger les m√©triques d'√©valuation

**Instruction** :  
Ajoutez juste apr√®s les pr√©dictions (**apr√®s la ligne 26**).

```python
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

rmse = np.sqrt(mean_squared_error(y_test, predictions))
mae = mean_absolute_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

mlflow.log_metric("rmse", rmse)
mlflow.log_metric("mae", mae)
mlflow.log_metric("r2", r2)
```

**Pourquoi** :  
Enregistrer les performances du mod√®le (RMSE, MAE, R2).

**Code complet apr√®s Ajout 5** :

```python
# 1. Importation des biblioth√®ques
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import ElasticNet
import mlflow
import mlflow.sklearn
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# 2. D√©finir l'exp√©rience MLflow
mlflow.set_experiment("exp_wine_quality")

# 3. Chargement des donn√©es
data = pd.read_csv("red-wine-quality.csv")

# 4. S√©paration en variables explicatives et variable cible
X = data.drop(["quality"], axis=1)
y = data["quality"]

# 5. Division en train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# 6. D√©marrer un run MLflow
mlflow.start_run()

# 7. Entra√Ænement d'un mod√®le ElasticNet
model = ElasticNet(alpha=0.5, l1_ratio=0.5)
mlflow.log_param("alpha", 0.5)
mlflow.log_param("l1_ratio", 0.5)
model.fit(X_train, y_train)

# 8. Pr√©dictions
predictions = model.predict(X_test)

# 9. Log des m√©triques
rmse = np.sqrt(mean_squared_error(y_test, predictions))
mae = mean_absolute_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

mlflow.log_metric("rmse", rmse)
mlflow.log_metric("mae", mae)
mlflow.log_metric("r2", r2)

# 10. Affichage simple
print(predictions[:5])
```

---

# Bloc 6 ‚Äì Ajout 6 : Logger le mod√®le

**Instruction** :  
Ajoutez apr√®s avoir logg√© les m√©triques (**apr√®s la ligne 34**).

```python
mlflow.sklearn.log_model(model, "model")
```

**Pourquoi** :  
Sauvegarder le mod√®le entra√Æn√© dans le stockage MLflow pour futur d√©ploiement.

**Code complet apr√®s Ajout 6** :

```python
# 1. Importation des biblioth√®ques
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import ElasticNet
import mlflow
import mlflow.sklearn
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# 2. D√©finir l'exp√©rience MLflow
mlflow.set_experiment("exp_wine_quality")

# 3. Chargement des donn√©es
data = pd.read_csv("red-wine-quality.csv")

# 4. S√©paration en variables explicatives et variable cible
X = data.drop(["quality"], axis=1)
y = data["quality"]

# 5. Division en train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# 6. D√©marrer un run MLflow
mlflow.start_run()

# 7. Entra√Ænement d'un mod√®le ElasticNet
model = ElasticNet(alpha=0.5, l1_ratio=0.5)
mlflow.log_param("alpha", 0.5)
mlflow.log_param("l1_ratio", 0.5)
model.fit(X_train, y_train)

# 8. Pr√©dictions
predictions = model.predict(X_test)

# 9. Log des m√©triques
rmse = np.sqrt(mean_squared_error(y_test, predictions))
mae = mean_absolute_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

mlflow.log_metric("rmse", rmse)
mlflow.log_metric("mae", mae)
mlflow.log_metric("r2", r2)

# 10. Log du mod√®le
mlflow.sklearn.log_model(model, "model")

# 11. Affichage simple
print(predictions[:5])
```






# Bloc 7 ‚Äì Ajout 7 : Terminer proprement le run

**Instruction** :  
Ajoutez juste **√† la fin du script** (**apr√®s la ligne 37**).

```python
mlflow.end_run()
```

**Pourquoi** :  
Toujours fermer proprement un run MLflow pour √©viter des runs "orphan" (incomplets) et assurer la bonne synchronisation avec le serveur MLflow.

---

**Code complet apr√®s Ajout 7** :

```python
# 1. Importation des biblioth√®ques
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import ElasticNet
import mlflow
import mlflow.sklearn
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# 2. D√©finir l'exp√©rience MLflow
mlflow.set_experiment("exp_wine_quality")

# 3. Chargement des donn√©es
data = pd.read_csv("red-wine-quality.csv")

# 4. S√©paration en variables explicatives et variable cible
X = data.drop(["quality"], axis=1)
y = data["quality"]

# 5. Division en train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# 6. D√©marrer un run MLflow
mlflow.start_run()

# 7. Entra√Ænement d'un mod√®le ElasticNet
model = ElasticNet(alpha=0.5, l1_ratio=0.5)
mlflow.log_param("alpha", 0.5)
mlflow.log_param("l1_ratio", 0.5)
model.fit(X_train, y_train)

# 8. Pr√©dictions
predictions = model.predict(X_test)

# 9. Log des m√©triques
rmse = np.sqrt(mean_squared_error(y_test, predictions))
mae = mean_absolute_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

mlflow.log_metric("rmse", rmse)
mlflow.log_metric("mae", mae)
mlflow.log_metric("r2", r2)

# 10. Log du mod√®le
mlflow.sklearn.log_model(model, "model")

# 11. Affichage simple
print(predictions[:5])

# 12. Finir proprement le run
mlflow.end_run()
```

---

# Bloc 8 ‚Äì Ajout 8 : Ajouter des tags personnalis√©s

**Instruction** :  
Ajoutez apr√®s le `start_run()` (**apr√®s la ligne 18**).

```python
mlflow.set_tags({
    "version": "v1",
    "model_type": "ElasticNet",
    "dataset": "wine_quality"
})
```

**Pourquoi** :  
Ajouter des m√©tadonn√©es sur le contexte de l'exp√©rience pour faciliter les recherches et la documentation.

---

**Code complet apr√®s Ajout 8** :

```python
# 1. Importation des biblioth√®ques
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import ElasticNet
import mlflow
import mlflow.sklearn
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# 2. D√©finir l'exp√©rience MLflow
mlflow.set_experiment("exp_wine_quality")

# 3. Chargement des donn√©es
data = pd.read_csv("red-wine-quality.csv")

# 4. S√©paration en variables explicatives et variable cible
X = data.drop(["quality"], axis=1)
y = data["quality"]

# 5. Division en train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# 6. D√©marrer un run MLflow
mlflow.start_run()

# 7. Ajouter des tags
mlflow.set_tags({
    "version": "v1",
    "model_type": "ElasticNet",
    "dataset": "wine_quality"
})

# 8. Entra√Ænement d'un mod√®le ElasticNet
model = ElasticNet(alpha=0.5, l1_ratio=0.5)
mlflow.log_param("alpha", 0.5)
mlflow.log_param("l1_ratio", 0.5)
model.fit(X_train, y_train)

# 9. Pr√©dictions
predictions = model.predict(X_test)

# 10. Log des m√©triques
rmse = np.sqrt(mean_squared_error(y_test, predictions))
mae = mean_absolute_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

mlflow.log_metric("rmse", rmse)
mlflow.log_metric("mae", mae)
mlflow.log_metric("r2", r2)

# 11. Log du mod√®le
mlflow.sklearn.log_model(model, "model")

# 12. Affichage simple
print(predictions[:5])

# 13. Finir proprement le run
mlflow.end_run()
```









# Bloc 9 ‚Äì Ajout 9 : Logger des artefacts (sauvegarde d‚Äôun fichier)

**Instruction** :  
Ajoutez **juste apr√®s** `mlflow.sklearn.log_model(model, "model")`, c‚Äôest-√†-dire **apr√®s la ligne 40**.

```python
# Cr√©er un fichier de r√©sultats
with open("results.txt", "w") as f:
    f.write(f"RMSE: {rmse}\n")
    f.write(f"MAE: {mae}\n")
    f.write(f"R2: {r2}\n")

# Logger ce fichier dans MLflow
mlflow.log_artifact("results.txt")
```

**Pourquoi** :  
- Sauvegarder des fichiers manuellement g√©n√©r√©s.
- Stocker des rapports, figures, datasets ou r√©sultats interm√©diaires dans l‚Äôinterface MLflow.

---

# Code complet apr√®s Ajout 9 :

```python
# 1. Importation des biblioth√®ques
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import ElasticNet
import mlflow
import mlflow.sklearn
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# 2. D√©finir l'exp√©rience MLflow
mlflow.set_experiment("exp_wine_quality")

# 3. Chargement des donn√©es
data = pd.read_csv("red-wine-quality.csv")

# 4. S√©paration en variables explicatives et variable cible
X = data.drop(["quality"], axis=1)
y = data["quality"]

# 5. Division en train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# 6. D√©marrer un run MLflow
mlflow.start_run()

# 7. Ajouter des tags
mlflow.set_tags({
    "version": "v1",
    "model_type": "ElasticNet",
    "dataset": "wine_quality"
})

# 8. Entra√Ænement d'un mod√®le ElasticNet
model = ElasticNet(alpha=0.5, l1_ratio=0.5)
mlflow.log_param("alpha", 0.5)
mlflow.log_param("l1_ratio", 0.5)
model.fit(X_train, y_train)

# 9. Pr√©dictions
predictions = model.predict(X_test)

# 10. Log des m√©triques
rmse = np.sqrt(mean_squared_error(y_test, predictions))
mae = mean_absolute_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

mlflow.log_metric("rmse", rmse)
mlflow.log_metric("mae", mae)
mlflow.log_metric("r2", r2)

# 11. Log du mod√®le
mlflow.sklearn.log_model(model, "model")

# 12. Cr√©er un fichier de r√©sultats et le logger comme artefact
with open("results.txt", "w") as f:
    f.write(f"RMSE: {rmse}\n")
    f.write(f"MAE: {mae}\n")
    f.write(f"R2: {r2}\n")
mlflow.log_artifact("results.txt")

# 13. Affichage simple
print(predictions[:5])

# 14. Finir proprement le run
mlflow.end_run()
```







# Bloc 10 ‚Äì Ajout 10 : Activer `mlflow.autolog()` pour tout logger automatiquement

**Instruction** :  
Ajoutez cette ligne **imm√©diatement apr√®s** `import mlflow.sklearn` (c‚Äôest-√†-dire **apr√®s la ligne 6**).

```python
mlflow.autolog(log_input_examples=False, log_model_signatures=False)
```

**Pourquoi** :  
- Plus besoin d‚Äô√©crire manuellement `log_param`, `log_metric`, `log_model`.
- MLflow d√©tecte automatiquement les entra√Ænements et enregistre les m√©triques, les param√®tres, et le mod√®le.

---

# Code complet apr√®s Ajout 10 :

```python
# 1. Importation des biblioth√®ques
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import ElasticNet
import mlflow
import mlflow.sklearn

# Activation de mlflow.autolog
mlflow.autolog(log_input_examples=False, log_model_signatures=False)

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# 2. D√©finir l'exp√©rience MLflow
mlflow.set_experiment("exp_wine_quality")

# 3. Chargement des donn√©es
data = pd.read_csv("red-wine-quality.csv")

# 4. S√©paration en variables explicatives et variable cible
X = data.drop(["quality"], axis=1)
y = data["quality"]

# 5. Division en train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# 6. D√©marrer un run MLflow
mlflow.start_run()

# 7. Ajouter des tags
mlflow.set_tags({
    "version": "v1",
    "model_type": "ElasticNet",
    "dataset": "wine_quality"
})

# 8. Entra√Ænement du mod√®le (les param√®tres seront auto-logg√©s)
model = ElasticNet(alpha=0.5, l1_ratio=0.5)
model.fit(X_train, y_train)

# 9. Pr√©dictions
predictions = model.predict(X_test)

# 10. Calcul manuel des m√©triques (mais plus besoin de les logger manuellement)
rmse = np.sqrt(mean_squared_error(y_test, predictions))
mae = mean_absolute_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

# 11. Cr√©er un fichier de r√©sultats et le logger comme artefact
with open("results.txt", "w") as f:
    f.write(f"RMSE: {rmse}\n")
    f.write(f"MAE: {mae}\n")
    f.write(f"R2: {r2}\n")
mlflow.log_artifact("results.txt")

# 12. Affichage simple
print(predictions[:5])

# 13. Finir proprement le run
mlflow.end_run()
```

---

# R√©sum√© Bloc 10 :

| Num√©ro d'√©tape | Changement |
|:--------------:|:-----------|
| Ligne 6 | `mlflow.autolog(log_input_examples=False, log_model_signatures=False)` ajout√© |

**Notes p√©dagogiques** :  
- Le `log_param`, `log_metric` et `log_model` sont maintenant inutiles √† √©crire manuellement.
- Il reste utile de logger des artefacts manuellement (`log_artifact`) si on veut sauvegarder des fichiers sp√©ciaux (comme `results.txt`).





# Bloc 11 ‚Äì Ajout 11 : Configurer MLflow pour production (PostgreSQL + S3)

**Instruction** :  
Cette partie est **hors script Python**.  
Elle concerne **comment lancer le serveur MLflow** pour production (pas juste en local avec SQLite).

**Commandes Terminal** (pas dans le code Python) :

```bash
mlflow server \
  --backend-store-uri postgresql://<user>:<password>@<hostname>:5432/<dbname> \
  --default-artifact-root s3://<bucket-name>/mlflow-artifacts \
  --host 0.0.0.0 \
  --port 5000
```

**Remplacer** :
- `<user>`, `<password>`, `<hostname>`, `<dbname>` par les acc√®s PostgreSQL r√©els.
- `<bucket-name>` par le nom du bucket S3 o√π stocker les artefacts.

---

# Pourquoi ce changement ?

- **PostgreSQL** = base solide pour les m√©tadonn√©es MLflow (experiences, runs, m√©triques...).
- **S3** = stockage scalable des fichiers lourds (artefacts mod√®les, images, datasets...).
- **Server accessible √† distance** (`--host 0.0.0.0`).

---

# Ce qu‚Äôon NE touche PAS dans le script Python :

Dans ton script existant, seul **le tracking URI** doit pointer vers ce serveur distant :

```python
mlflow.set_tracking_uri("http://<ip-serveur>:5000")
```

*(ex: `http://192.168.1.100:5000` ou `http://mon-serveur-mlflow.com:5000`)*

---

# R√©sum√© Bloc 11 :

| Action | O√π ? |
|:------:|:----:|
| Lancer MLflow avec PostgreSQL + S3 | Terminal (pas dans Python) |
| Modifier `set_tracking_uri` | D√©but du script Python |

---

# Code Python inchang√©, sauf une ligne importante :

```python
mlflow.set_tracking_uri("http://<ip-serveur>:5000")
```

**√Ä mettre imm√©diatement avant** :

```python
mlflow.set_experiment("exp_wine_quality")
```

(vers la ligne 13 de ton code actuel).

---

# üö® Attention

- La base PostgreSQL doit √™tre existante et accessible.
- Le bucket S3 doit √™tre pr√™t et les permissions configur√©es.
- Si ce n‚Äôest pas encore pr√™t, continue en local avec SQLite pour l‚Äôinstant (comme au Bloc 0).









# Bloc 12 ‚Äì Ajout 12 : Signature du mod√®le et Exemple d'entr√©e (ModelSignature + input_example)

## Objectif de ce bloc

- Ajouter une **signature** (`ModelSignature`) pour documenter formellement ce que prend et produit le mod√®le.
- Ajouter un **exemple d'entr√©e** (`input_example`) pour aider les utilisateurs √† comprendre les donn√©es attendues.

---

## Instructions d√©taill√©es

**√Ä faire dans ton script existant :**

### 1. Importer les bons modules

**Ajout √† faire en haut du fichier**, dans la section des `import` existants, **apr√®s `import mlflow.sklearn`** :

```python
from mlflow.models.signature import ModelSignature, infer_signature
from mlflow.types.schema import Schema, ColSpec
```

**Position** : environ ligne 12.

---

### 2. Cr√©er les sch√©mas manuellement

**√Ä ajouter juste apr√®s l'entra√Ænement du mod√®le**, c‚Äôest-√†-dire **apr√®s :**

```python
lr.fit(train_x, train_y)
```

**Ajout :**

```python
input_schema = Schema([
    ColSpec("double", "fixed acidity"),
    ColSpec("double", "volatile acidity"),
    ColSpec("double", "citric acid"),
    ColSpec("double", "residual sugar"),
    ColSpec("double", "chlorides"),
    ColSpec("double", "free sulfur dioxide"),
    ColSpec("double", "total sulfur dioxide"),
    ColSpec("double", "density"),
    ColSpec("double", "pH"),
    ColSpec("double", "sulphates"),
    ColSpec("double", "alcohol")
])

output_schema = Schema([
    ColSpec("double")
])

signature = ModelSignature(inputs=input_schema, outputs=output_schema)
```

**Position** : juste apr√®s `lr.fit(train_x, train_y)`, donc autour de la ligne 50.

---

### 3. Cr√©er un input_example

Toujours **imm√©diatement apr√®s** la signature :

```python
input_example = train_x.iloc[:5]
```

---

### 4. Logguer le mod√®le avec signature et input_example

**Remplacer ton `mlflow.sklearn.log_model` existant** (probablement autour de la ligne 70)  
par la nouvelle version suivante :

```python
mlflow.sklearn.log_model(
    sk_model=lr,
    artifact_path="model",
    signature=signature,
    input_example=input_example
)
```

---

# R√©sum√© Bloc 12

| √âtape | O√π ? |
|:-----:|:----:|
| Import `ModelSignature`, `Schema`, `ColSpec` | Ligne 12 |
| D√©finir `input_schema`, `output_schema`, `signature` | Apr√®s `fit()` |
| D√©finir `input_example` | Apr√®s `signature` |
| Modifier `log_model()` | √Ä la place du `log_model` existant |

---

# Code complet ajout√© √† ce stade

```python
# Haut du fichier
from mlflow.models.signature import ModelSignature, infer_signature
from mlflow.types.schema import Schema, ColSpec

# Apr√®s entrainement
input_schema = Schema([
    ColSpec("double", "fixed acidity"),
    ColSpec("double", "volatile acidity"),
    ColSpec("double", "citric acid"),
    ColSpec("double", "residual sugar"),
    ColSpec("double", "chlorides"),
    ColSpec("double", "free sulfur dioxide"),
    ColSpec("double", "total sulfur dioxide"),
    ColSpec("double", "density"),
    ColSpec("double", "pH"),
    ColSpec("double", "sulphates"),
    ColSpec("double", "alcohol")
])

output_schema = Schema([
    ColSpec("double")
])

signature = ModelSignature(inputs=input_schema, outputs=output_schema)

input_example = train_x.iloc[:5]

# Enregistrement du mod√®le
mlflow.sklearn.log_model(
    sk_model=lr,
    artifact_path="model",
    signature=signature,
    input_example=input_example
)
```












# Bloc 13 ‚Äì Ajout 13 : Utilisation de `infer_signature` pour g√©n√©rer automatiquement la signature du mod√®le

## Objectif de ce bloc

- Ne plus √©crire les sch√©mas d'entr√©e/sortie manuellement.
- Utiliser `infer_signature` pour analyser automatiquement les entr√©es (`test_x`) et les sorties (`predicted_qualities`).

---

## Instructions d√©taill√©es

**√Ä faire dans ton script existant :**

### 1. Remplacer la d√©finition manuelle de la signature

**Supprimer compl√®tement** cette partie que nous avions ajout√©e dans Bloc 12 :

```python
input_schema = Schema([...])
output_schema = Schema([...])
signature = ModelSignature(inputs=input_schema, outputs=output_schema)
```

**√Ä remplacer par :**

```python
signature = infer_signature(test_x, predicted_qualities)
```

**Position** : √Ä mettre juste **apr√®s** la ligne o√π tu fais les pr√©dictions :

```python
predicted_qualities = lr.predict(test_x)
signature = infer_signature(test_x, predicted_qualities)
```

Autour de la ligne 55 environ.

---

### 2. Garder l'input_example existant

L'input_example reste exactement :

```python
input_example = train_x.iloc[:5]
```

Pas de modification ici.

---

### 3. Garder la nouvelle version de `log_model`

Le `mlflow.sklearn.log_model` reste celui du Bloc 12 :

```python
mlflow.sklearn.log_model(
    sk_model=lr,
    artifact_path="model",
    signature=signature,
    input_example=input_example
)
```

---

# R√©sum√© Bloc 13

| √âtape | O√π ? |
|:-----:|:----:|
| Remplacer le Schema manuel par `infer_signature(test_x, predicted_qualities)` | Apr√®s la pr√©diction |
| Garder `input_example = train_x.iloc[:5]` | Inchang√© |
| Garder `mlflow.sklearn.log_model(...)` avec signature et input_example | Inchang√© |

---

# Code modifi√© ajout√© √† ce stade

```python
# Apr√®s les pr√©dictions
predicted_qualities = lr.predict(test_x)
signature = infer_signature(test_x, predicted_qualities)

input_example = train_x.iloc[:5]

# Enregistrement du mod√®le
mlflow.sklearn.log_model(
    sk_model=lr,
    artifact_path="model",
    signature=signature,
    input_example=input_example
)
```









# Bloc 14 ‚Äì Ajout 14 : Activation de `mlflow.autolog` pour automatiser la capture des param√®tres, m√©triques et mod√®les

## Objectif de ce bloc

- √âviter d‚Äô√©crire manuellement `mlflow.log_param`, `mlflow.log_metric`, et `mlflow.log_model`.
- Capturer automatiquement toutes les m√©triques et artefacts g√©n√©r√©s par Scikit-learn.

---

## Instructions d√©taill√©es

**√Ä faire dans ton script existant :**

### 1. Ajouter `mlflow.sklearn.autolog()` **avant** l'entra√Ænement du mod√®le

**Juste avant** de cr√©er et d‚Äôentra√Æner ton mod√®le `ElasticNet`, ajoute la ligne suivante :

```python
mlflow.sklearn.autolog()
```

**Position** :  
Juste avant cette ligne existante :

```python
lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)
```

Donc environ **ligne 45-48** dans ton script.

---

### 2. Supprimer les appels manuels √† `log_param`, `log_metric`, `log_model`

**Supprime** les lignes suivantes car elles deviendront inutiles :

```python
mlflow.log_params({...})
mlflow.log_metrics({...})
mlflow.sklearn.log_model(...)
```

√Ä partir d‚Äôici, tout sera fait **automatiquement**.

---

## Important

- Le `autolog()` doit √™tre appel√© **avant** l'entra√Ænement du mod√®le.
- Le tracking automatique s'occupe aussi de :
  - Hyperparam√®tres du mod√®le
  - M√©triques d‚Äô√©valuation (MSE, RMSE, MAE, etc.)
  - S√©rialisation du mod√®le
  - Signature et input example (si possible)

---

# R√©sum√© Bloc 14

| √âtape | O√π ? |
|:-----:|:----:|
| Ajouter `mlflow.sklearn.autolog()` | Avant la cr√©ation du mod√®le ElasticNet |
| Supprimer tous les appels manuels √† `log_params`, `log_metrics`, `log_model` | Apr√®s |

---

# Code modifi√© ajout√© √† ce stade

```python
# Avant de cr√©er le mod√®le
mlflow.sklearn.autolog()

# Cr√©ation et entra√Ænement
lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)
lr.fit(train_x, train_y)
predicted_qualities = lr.predict(test_x)

# Plus besoin de log_params, log_metrics, log_model manuellement
signature = infer_signature(test_x, predicted_qualities)
input_example = train_x.iloc[:5]
```












# Bloc 15 ‚Äì Ajout 15 : Encapsulation du mod√®le Scikit-learn dans un `PythonModel` personnalis√© (wrapper)

## Objectif de ce bloc

- Encapsuler ton mod√®le Scikit-learn dans une classe `PythonModel`.
- Pr√©parer ton mod√®le pour un d√©ploiement plus flexible via `mlflow.pyfunc`.
- Pouvoir personnaliser le comportement du mod√®le √† l‚Äôinf√©rence si besoin.

---

## Instructions d√©taill√©es

**√Ä faire dans ton script existant :**

### 1. D√©finir une classe `SklearnWrapper`

**Ajoute** cette classe **apr√®s** tes imports, mais **avant** le `if __name__ == "__main__":`.

```python
class SklearnWrapper(mlflow.pyfunc.PythonModel):
    def load_context(self, context):
        import joblib
        self.model = joblib.load(context.artifacts["sklearn_model"])

    def predict(self, context, model_input):
        return self.model.predict(model_input)
```

**Position** :  
- Imm√©diatement **apr√®s les imports** (`import pandas as pd`, etc.)
- Vers **ligne 20-25** selon ton script.

---

### 2. Sauvegarder ton mod√®le Scikit-learn avec `joblib`

**Ajoute** avant la fin du `start_run()` :

```python
import joblib

# Sauvegarder le mod√®le entrain√©
model_path = "sklearn_model.pkl"
joblib.dump(lr, model_path)
```

**Position** :  
Juste apr√®s `lr.fit(...)` et `predicted_qualities = lr.predict(...)`.

---

### 3. Utiliser `mlflow.pyfunc.log_model` pour enregistrer ton mod√®le encapsul√©

**Ajoute** apr√®s la sauvegarde du mod√®le :

```python
mlflow.pyfunc.log_model(
    artifact_path="sklearn_pyfunc_model",
    python_model=SklearnWrapper(),
    artifacts={"sklearn_model": model_path}
)
```

**Position** :  
√Ä la place o√π tu faisais `mlflow.sklearn.log_model` (si existant).

---

## Important

- Le fichier `sklearn_model.pkl` sera enregistr√© en tant qu‚Äôartefact de run.
- L‚ÄôAPI d'inf√©rence passera par ton `predict()` du `SklearnWrapper`.
- Ce mod√®le est compatible avec les API Serveur de MLflow (`mlflow models serve`).

---

# R√©sum√© Bloc 15

| √âtape | O√π ? |
|:-----:|:----:|
| Cr√©er `SklearnWrapper` | Apr√®s les imports |
| Sauvegarder `sklearn_model.pkl` | Apr√®s entra√Ænement mod√®le |
| Utiliser `mlflow.pyfunc.log_model` | √Ä la place de `mlflow.sklearn.log_model` |

---

# Code modifi√© ajout√© √† ce stade

```python
# D√©finition du Wrapper
class SklearnWrapper(mlflow.pyfunc.PythonModel):
    def load_context(self, context):
        import joblib
        self.model = joblib.load(context.artifacts["sklearn_model"])

    def predict(self, context, model_input):
        return self.model.predict(model_input)

# Entra√Ænement
lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)
lr.fit(train_x, train_y)
predicted_qualities = lr.predict(test_x)

# Sauvegarde du mod√®le Scikit-learn
import joblib
model_path = "sklearn_model.pkl"
joblib.dump(lr, model_path)

# Logging du mod√®le encapsul√© avec Pyfunc
mlflow.pyfunc.log_model(
    artifact_path="sklearn_pyfunc_model",
    python_model=SklearnWrapper(),
    artifacts={"sklearn_model": model_path}
)
```








# Bloc 16 ‚Äì Ajout 16 : D√©finir un environnement Conda pour le mod√®le

---

## Objectif de ce bloc

- Cr√©er **manuellement** un environnement `conda.yaml` **li√© au mod√®le**.
- Permettre que ton mod√®le soit **ex√©cutable partout** (serveur, production).
- S'assurer que toutes les **d√©pendances** n√©cessaires sont pr√©sentes.

---

## Instructions d√©taill√©es

**√Ä faire dans ton script existant :**

---

### 1. D√©finir un dictionnaire `conda_env`

**Ajoute** ce bloc **juste avant** `mlflow.pyfunc.log_model(...)`  
(autrement dit, **avant l'enregistrement du mod√®le** encapsul√© par `SklearnWrapper`).

```python
# D√©finir l'environnement Conda
conda_env = {
    "channels": ["defaults"],
    "dependencies": [
        "python=3.10",
        "pip",
        {
            "pip": [
                f"mlflow=={mlflow.__version__}",
                f"scikit-learn=={sklearn.__version__}",
                f"cloudpickle=={cloudpickle.__version__}"
            ]
        }
    ],
    "name": "sklearn_env"
}
```

---

### 2. Modifier ton appel √† `mlflow.pyfunc.log_model`

**Ajoute le param√®tre** `conda_env=conda_env` dans l'appel √† `log_model`, comme ceci :

```python
mlflow.pyfunc.log_model(
    artifact_path="sklearn_pyfunc_model",
    python_model=SklearnWrapper(),
    artifacts={"sklearn_model": model_path},
    conda_env=conda_env
)
```

---

## Position exacte dans ton script :

| Partie | Emplacement |
|:------:|:------------|
| D√©finir `conda_env` | Avant `mlflow.pyfunc.log_model(...)` |
| Ajouter `conda_env=conda_env` | Dans l'appel √† `log_model` |

---

## Pourquoi on fait √ßa ?

- Ton mod√®le sera **portable** avec un fichier `conda.yaml` g√©n√©r√© automatiquement.
- Tu pourras **servir** (`mlflow models serve`) ton mod√®le sur n'importe quelle machine sans surprises.
- Bonne **pratique professionnelle** : toujours versionner l'environnement.

---

# Code modifi√© apr√®s ce Bloc

```python
# D√©finir l'environnement Conda
conda_env = {
    "channels": ["defaults"],
    "dependencies": [
        "python=3.10",
        "pip",
        {
            "pip": [
                f"mlflow=={mlflow.__version__}",
                f"scikit-learn=={sklearn.__version__}",
                f"cloudpickle=={cloudpickle.__version__}"
            ]
        }
    ],
    "name": "sklearn_env"
}

# Logging du mod√®le encapsul√© avec Pyfunc + Conda env
mlflow.pyfunc.log_model(
    artifact_path="sklearn_pyfunc_model",
    python_model=SklearnWrapper(),
    artifacts={"sklearn_model": model_path},
    conda_env=conda_env
)
```

---

# R√©sum√© du Bloc 16

| √âtape | O√π l'ajouter | Pourquoi |
|:-----:|:------------:|:--------:|
| Ajouter `conda_env` | Avant `log_model` | D√©finir un environnement reproductible |
| Ajouter `conda_env=conda_env` | Dans `log_model` | Lier l'environnement au mod√®le enregistr√© |














# Bloc 17 ‚Äì Ajout 17 : Charger le mod√®le sauvegard√© avec `load_model` et pr√©dire

---

## Objectif de ce bloc

- **V√©rifier** que le mod√®le enregistr√© fonctionne en le **rechargeant** depuis MLflow.
- **Faire une pr√©diction** sur `test_x` apr√®s rechargement.
- **√âvaluer** les m√©triques de pr√©diction apr√®s rechargement (RMSE, MAE, R¬≤).

---

## Instructions d√©taill√©es

**√Ä faire dans ton script existant :**

---

### 1. Charger le mod√®le depuis MLflow

**Ajoute** ce bloc **apr√®s** avoir fait `mlflow.end_run()`.

Juste apr√®s `mlflow.end_run()`, ajoute :

```python
# Chargement du mod√®le sauvegard√© depuis MLflow
loaded_model = mlflow.pyfunc.load_model(model_uri="runs:/"+run.info.run_id+"/sklearn_pyfunc_model")
```

---

### 2. Utiliser le mod√®le recharg√© pour pr√©dire

Ajoute imm√©diatement apr√®s :

```python
# Faire des pr√©dictions avec le mod√®le recharg√©
reloaded_predictions = loaded_model.predict(test_x)
```

---

### 3. R√©√©valuer les m√©triques sur les pr√©dictions recharg√©es

Ajoute :

```python
# R√©√©valuer les m√©triques sur les pr√©dictions reload√©es
rmse_reload, mae_reload, r2_reload = eval_metrics(test_y, reloaded_predictions)

print("√âvaluation du mod√®le recharg√© depuis MLflow :")
print(f"  RMSE (Reloaded): {rmse_reload}")
print(f"  MAE  (Reloaded): {mae_reload}")
print(f"  R2   (Reloaded): {r2_reload}")
```

---

## Position exacte dans ton script :

| Partie | Emplacement |
|:------:|:------------|
| Charger mod√®le | Imm√©diatement apr√®s `mlflow.end_run()` |
| Pr√©dire et √©valuer | Apr√®s le chargement |

---

## Pourquoi on fait √ßa ?

- **Valider** que ton mod√®le est bien sauvegard√© **correctement** dans MLflow.
- **S‚Äôassurer** que les performances sont **identiques** apr√®s sauvegarde et rechargement.
- **Pr√©parer** la future utilisation de ton mod√®le en production.

---

# Code ajout√© apr√®s ce Bloc

```python
mlflow.end_run()

# Chargement du mod√®le sauvegard√© depuis MLflow
loaded_model = mlflow.pyfunc.load_model(model_uri="runs:/"+run.info.run_id+"/sklearn_pyfunc_model")

# Faire des pr√©dictions avec le mod√®le recharg√©
reloaded_predictions = loaded_model.predict(test_x)

# R√©√©valuer les m√©triques sur les pr√©dictions reload√©es
rmse_reload, mae_reload, r2_reload = eval_metrics(test_y, reloaded_predictions)

print("√âvaluation du mod√®le recharg√© depuis MLflow :")
print(f"  RMSE (Reloaded): {rmse_reload}")
print(f"  MAE  (Reloaded): {mae_reload}")
print(f"  R2   (Reloaded): {r2_reload}")
```

---

# R√©sum√© du Bloc 17

| √âtape | O√π l'ajouter | Pourquoi |
|:-----:|:------------:|:--------:|
| Charger mod√®le MLflow | Apr√®s `end_run()` | R√©cup√©rer mod√®le |
| Pr√©dire avec mod√®le charg√© | Suite directe | V√©rifier la sauvegarde correcte |
| R√©√©valuer m√©triques | Suite directe | Confirmer qu'il n'y a pas de perte de performance |















# Bloc 18 ‚Äì Ajout 18 : Utiliser `mlflow.evaluate()` pour √©valuer automatiquement

---

## Objectif de ce bloc

- **Automatiser** l‚Äô√©valuation du mod√®le **recharg√©**.
- **G√©n√©rer un rapport complet** de m√©triques d‚Äô√©valuation (RMSE, MAE, R¬≤‚Ä¶).
- **Visualiser** facilement les r√©sultats et les comparer plus tard.

---

## Instructions d√©taill√©es

**√Ä faire apr√®s avoir charg√© et utilis√© ton mod√®le.**

---

### 1. √âvaluer automatiquement avec `mlflow.evaluate()`

Ajoute **imm√©diatement apr√®s** le bloc pr√©c√©dent (apr√®s avoir r√©√©valu√© manuellement).

```python
import mlflow.models

# Utiliser mlflow.evaluate pour √©valuer automatiquement le mod√®le
eval_result = mlflow.evaluate(
    model=loaded_model,
    data=test_x.assign(quality=test_y),
    targets="quality",
    model_type="regressor",
    evaluators="default"
)

print("R√©sultats d'√©valuation automatique avec mlflow.evaluate :")
print(eval_result.metrics)
```

---

### 2. Points tr√®s importants :

- `model=loaded_model` : C‚Äôest ton mod√®le recharg√©.
- `data=test_x.assign(quality=test_y)` : On fournit X **et** y sous forme d‚Äôun m√™me DataFrame.
- `targets="quality"` : On pr√©cise la colonne cible.
- `model_type="regressor"` : C‚Äôest un mod√®le de r√©gression, pas de classification.
- `evaluators="default"` : Utiliser les √©valuateurs standards de MLflow.

---

## Position exacte dans ton script

| Partie | Emplacement |
|:------:|:------------|
| Appel √† `evaluate` | Juste apr√®s avoir r√©√©valu√© manuellement le mod√®le |

---

## Pourquoi on fait √ßa ?

- **√âviter** de coder manuellement le calcul des m√©triques.
- **Avoir** un **rapport standardis√©** compatible avec d‚Äôautres outils MLflow.
- **Automatiser** compl√®tement la validation des mod√®les.

---

# Code ajout√© apr√®s ce Bloc

```python
import mlflow.models

# Utiliser mlflow.evaluate pour √©valuer automatiquement le mod√®le
eval_result = mlflow.evaluate(
    model=loaded_model,
    data=test_x.assign(quality=test_y),
    targets="quality",
    model_type="regressor",
    evaluators="default"
)

print("R√©sultats d'√©valuation automatique avec mlflow.evaluate :")
print(eval_result.metrics)
```

---

# R√©sum√© du Bloc 18

| √âtape | O√π l'ajouter | Pourquoi |
|:-----:|:------------:|:--------:|
| Appeler `mlflow.evaluate()` | Apr√®s la r√©√©valuation manuelle | Avoir un rapport automatique complet |

---

# **R√©sum√© de ta progression actuelle**

| Bloc | Fonction ajout√©e | Pourquoi |
|:----:|:-----------------|:---------|
| 17 | Charger mod√®le MLflow | Tester le mod√®le sauvegard√© |
| 18 | √âvaluer automatiquement | Avoir toutes les m√©triques standardis√©es |

---

# Ce que tu as maintenant dans ton projet

- Un mod√®le qui est :
  - entra√Æn√©,
  - logg√© dans MLflow,
  - recharg√© depuis MLflow,
  - √©valu√© **manuellement**,
  - √©valu√© **automatiquement** par MLflow.

Tu arrives √† **un script semi-professionnel**, tr√®s proche d‚Äôun **workflow de production**.














# Bloc 19 ‚Äì Ajout 19 : Ajouter des m√©triques personnalis√©es avec `make_metric`

---

## Objectif de ce bloc

- **D√©finir** des m√©triques personnalis√©es sp√©cifiques aux besoins du projet.
- **√âtendre** les m√©triques disponibles par d√©faut dans MLflow.

---

## Instructions d√©taill√©es

**√Ä faire juste apr√®s** l'utilisation de `mlflow.evaluate()` (Bloc 18).

---

### 1. D√©finir des m√©triques personnalis√©es

Ajoute ce code **imm√©diatement apr√®s l‚Äô√©valuation automatique** :

```python
from mlflow.models import make_metric
import numpy as np

# D√©finir une m√©trique personnalis√©e : somme des erreurs au carr√© plus 1
def squared_diff_plus_one(eval_df, _builtin_metrics):
    return np.sum(np.abs(eval_df["prediction"] - eval_df["target"] + 1) ** 2)

# D√©finir une autre m√©trique personnalis√©e : somme des cibles divis√©e par deux
def sum_on_target_divided_by_two(eval_df, builtin_metrics):
    return builtin_metrics["sum_on_target"] / 2

# Cr√©er les objets m√©triques pour MLflow
squared_diff_plus_one_metric = make_metric(
    eval_fn=squared_diff_plus_one,
    greater_is_better=False,
    name="squared_diff_plus_one"
)

sum_on_target_divided_by_two_metric = make_metric(
    eval_fn=sum_on_target_divided_by_two,
    greater_is_better=True,
    name="sum_on_target_divided_by_two"
)
```

---

### 2. Pourquoi ce code est important ?

- **`squared_diff_plus_one`** : Modifie l'erreur quadratique pour tester des variantes.
- **`sum_on_target_divided_by_two`** : Utilise une m√©trique simple d√©riv√©e d‚Äôune somme int√©gr√©e.
- **`make_metric`** : Standardise l'int√©gration de m√©triques personnalis√©es dans MLflow.

---

## Position exacte dans ton script

| Partie | Emplacement |
|:------:|:------------|
| D√©finitions de m√©triques personnalis√©es | Juste apr√®s `mlflow.evaluate()` |

---

## Ce que ces m√©triques vont faire :

- **`squared_diff_plus_one`** : Donne une id√©e de la robustesse aux petites erreurs.
- **`sum_on_target_divided_by_two`** : Teste une propri√©t√© du dataset (pas du mod√®le).

---

# Code ajout√© apr√®s ce Bloc

```python
from mlflow.models import make_metric
import numpy as np

def squared_diff_plus_one(eval_df, _builtin_metrics):
    return np.sum(np.abs(eval_df["prediction"] - eval_df["target"] + 1) ** 2)

def sum_on_target_divided_by_two(eval_df, builtin_metrics):
    return builtin_metrics["sum_on_target"] / 2

squared_diff_plus_one_metric = make_metric(
    eval_fn=squared_diff_plus_one,
    greater_is_better=False,
    name="squared_diff_plus_one"
)

sum_on_target_divided_by_two_metric = make_metric(
    eval_fn=sum_on_target_divided_by_two,
    greater_is_better=True,
    name="sum_on_target_divided_by_two"
)
```

---

# R√©sum√© du Bloc 19

| √âtape | O√π l'ajouter | Pourquoi |
|:-----:|:------------:|:--------:|
| D√©finir des m√©triques personnalis√©es | Apr√®s `mlflow.evaluate()` standard | √âtendre l‚Äô√©valuation de mod√®le |

---

# **R√©sum√© actuel du projet**

| Bloc | Fonction ajout√©e | Pourquoi |
|:----:|:-----------------|:---------|
| 17 | Charger mod√®le | Tester apr√®s sauvegarde |
| 18 | √âvaluation automatique | Obtenir des m√©triques standard |
| 19 | Ajouter m√©triques custom | Enrichir l‚Äô√©valuation |

---

# Ce que tu as maintenant dans ton projet

- Mod√®le recharg√©.
- √âvalu√© automatiquement.
- Avec des **m√©triques personnalis√©es pr√™tes √† √™tre utilis√©es**.












# Bloc 20 ‚Äì Ajout 20 : G√©n√©rer des artefacts visuels lors de l‚Äô√©valuation

---

## Objectif de ce bloc

- **Cr√©er automatiquement** un **graphe visuel** dans MLflow (scatter plot entre pr√©dictions et vraies valeurs).
- **Ajouter cet artefact** comme preuve visuelle de la qualit√© du mod√®le.

---

## Instructions d√©taill√©es

**√Ä faire juste apr√®s** la d√©finition des m√©triques personnalis√©es (Bloc 19).

---

### 1. D√©finir une fonction de cr√©ation d'artefact

Ajoute ce code :

```python
import matplotlib.pyplot as plt
import os

# Fonction pour cr√©er un graphique de dispersion
def prediction_target_scatter(eval_df, _builtin_metrics, artifacts_dir):
    plt.scatter(eval_df["prediction"], eval_df["target"])
    plt.xlabel("Predictions")
    plt.ylabel("True Values")
    plt.title("Scatter Plot: Predictions vs True Values")
    plot_path = os.path.join(artifacts_dir, "scatter_plot.png")
    plt.savefig(plot_path)
    plt.close()
    return {"scatter_plot_artifact": plot_path}
```

---

### 2. Pourquoi ce code est important ?

- **Visualisation imm√©diate** dans MLflow de la qualit√© de la pr√©diction.
- **Artefact** : Fichier `.png` sauvegard√© automatiquement dans l‚Äôonglet Artefacts du run MLflow.

---

## Position exacte dans ton script

| Partie | Emplacement |
|:------:|:------------|
| D√©finir `prediction_target_scatter()` | Apr√®s la d√©finition des m√©triques personnalis√©es |

---

### 3. Int√©grer la fonction d'artefact dans `mlflow.evaluate`

Quand tu appelles `mlflow.evaluate`, ajoute :

```python
mlflow.evaluate(
    model=artifacts_uri,
    data=test,
    targets="quality",
    model_type="regressor",
    evaluators=["default"],
    custom_metrics=[
        squared_diff_plus_one_metric,
        sum_on_target_divided_by_two_metric
    ],
    custom_artifacts=[prediction_target_scatter]
)
```

Regarde bien :  
`custom_artifacts=[prediction_target_scatter]` est **la partie ajout√©e**.

---

# Code ajout√© apr√®s ce Bloc

```python
import matplotlib.pyplot as plt
import os

def prediction_target_scatter(eval_df, _builtin_metrics, artifacts_dir):
    plt.scatter(eval_df["prediction"], eval_df["target"])
    plt.xlabel("Predictions")
    plt.ylabel("True Values")
    plt.title("Scatter Plot: Predictions vs True Values")
    plot_path = os.path.join(artifacts_dir, "scatter_plot.png")
    plt.savefig(plot_path)
    plt.close()
    return {"scatter_plot_artifact": plot_path}
```

Et modification de `mlflow.evaluate` :

```python
mlflow.evaluate(
    model=artifacts_uri,
    data=test,
    targets="quality",
    model_type="regressor",
    evaluators=["default"],
    custom_metrics=[
        squared_diff_plus_one_metric,
        sum_on_target_divided_by_two_metric
    ],
    custom_artifacts=[prediction_target_scatter]
)
```

---

# R√©sum√© du Bloc 20

| √âtape | O√π l'ajouter | Pourquoi |
|:-----:|:------------:|:--------:|
| D√©finir `prediction_target_scatter()` | Apr√®s d√©finition des m√©triques personnalis√©es | Ajouter un visuel √† l‚Äô√©valuation |
| Modifier `mlflow.evaluate()` | Ajouter `custom_artifacts` | Voir l‚Äôalignement pr√©dictions / valeurs r√©elles |

---

# Ce que tu as maintenant dans ton projet

- En plus des m√©triques, **un artefact visuel** est automatiquement g√©n√©r√© pour chaque √©valuation.
- Le **scatter plot** permet **de juger visuellement la performance** sans lire uniquement les chiffres.











# Bloc 21 ‚Äì Ajout 21 : Int√©grer un mod√®le de base (`DummyRegressor`)

---

## Objectif de ce bloc

- **Ajouter un mod√®le de base na√Øf** (DummyRegressor) qui sert de **r√©f√©rence minimale** pour √©valuer si ElasticNet est **r√©ellement meilleur**.
- **Logger les m√©triques** du DummyRegressor pour comparer avec ElasticNet.

---

## Instructions d√©taill√©es

**√Ä faire juste apr√®s** l'entra√Ænement du mod√®le ElasticNet.

---

### 1. Importer le DummyRegressor

Ajoute cette ligne **tout en haut du fichier** (pr√®s des imports) :

```python
from sklearn.dummy import DummyRegressor
```

---

### 2. Entra√Æner le mod√®le Dummy

Ajoute ce bloc de code :

```python
# Entra√Æner un mod√®le de base (DummyRegressor)
dummy = DummyRegressor(strategy="mean")
dummy.fit(train_x, train_y)

# Pr√©dictions du mod√®le de base
dummy_predicted = dummy.predict(test_x)

# Calculer les m√©triques du DummyRegressor
(dummy_rmse, dummy_mae, dummy_r2) = eval_metrics(test_y, dummy_predicted)

print("DummyRegressor model:")
print("  RMSE: %s" % dummy_rmse)
print("  MAE: %s" % dummy_mae)
print("  R2: %s" % dummy_r2)
```

---

### 3. Logger les m√©triques Dummy dans MLflow

Apr√®s l‚Äôentra√Ænement du mod√®le de base, ajoute :

```python
mlflow.log_metrics({
    "dummy_rmse": dummy_rmse,
    "dummy_mae": dummy_mae,
    "dummy_r2": dummy_r2
})
```

---

## Position exacte dans ton script

| Partie | Emplacement |
|:------:|:------------|
| Importer DummyRegressor | Ligne des imports |
| Entra√Æner et √©valuer DummyRegressor | Apr√®s l'entra√Ænement et le logging ElasticNet |
| Logger les m√©triques Dummy dans MLflow | Apr√®s le logging ElasticNet |

---

# Code ajout√© apr√®s ce Bloc

```python
from sklearn.dummy import DummyRegressor

# Apr√®s entra√Ænement ElasticNet
dummy = DummyRegressor(strategy="mean")
dummy.fit(train_x, train_y)
dummy_predicted = dummy.predict(test_x)

(dummy_rmse, dummy_mae, dummy_r2) = eval_metrics(test_y, dummy_predicted)

print("DummyRegressor model:")
print("  RMSE: %s" % dummy_rmse)
print("  MAE: %s" % dummy_mae)
print("  R2: %s" % dummy_r2)

mlflow.log_metrics({
    "dummy_rmse": dummy_rmse,
    "dummy_mae": dummy_mae,
    "dummy_r2": dummy_r2
})
```

---

# R√©sum√© du Bloc 21

| √âtape | O√π l'ajouter | Pourquoi |
|:-----:|:------------:|:--------:|
| Importer DummyRegressor | En haut du fichier | Utiliser un mod√®le de base |
| Entra√Æner / pr√©dire Dummy | Apr√®s ElasticNet | Avoir un benchmark minimum |
| Logger les m√©triques Dummy | Apr√®s m√©triques ElasticNet | Comparaison objective dans MLflow |

---

# Ce que tu as maintenant dans ton projet

- Tu as **deux mod√®les √©valu√©s** : ElasticNet **et** DummyRegressor.
- Tu pourras **prouver objectivement** que ElasticNet apporte une **valeur ajout√©e r√©elle** (ou pas).
















# Bloc 22 ‚Äì Ajout 22 : Ajouter des seuils de validation automatique avec `MetricThreshold`

---

## Objectif de ce bloc

- **D√©finir des crit√®res** qui valident **automatiquement** si ElasticNet est **vraiment meilleur que le mod√®le de base**.
- **Utiliser `MetricThreshold`** pour comparer les m√©triques entre ElasticNet et Dummy.

---

## Instructions d√©taill√©es

**√Ä faire apr√®s** l'entra√Ænement de DummyRegressor et apr√®s avoir logg√© ses m√©triques.

---

### 1. Importer MetricThreshold

Ajoute cette ligne **dans la partie des imports** :

```python
from mlflow.models import MetricThreshold
```

---

### 2. D√©finir les seuils de validation

Ajoute ce bloc **apr√®s le logging du mod√®le ElasticNet et Dummy** :

```python
# D√©finir un seuil sur la m√©trique "rmse"
thresholds = {
    "rmse": MetricThreshold(
        threshold=dummy_rmse,  # On exige que RMSE soit meilleur que celui du Dummy
        min_absolute_change=0.1,  # Minimum 0.1 de mieux
        min_relative_change=0.05,  # Minimum 5% de mieux
        greater_is_better=False  # Un RMSE plus petit est meilleur
    )
}
```

---

### 3. Pr√©parer le chemin des mod√®les pour √©valuation

Ajoute :

```python
# R√©cup√©rer l'URI du mod√®le ElasticNet
elasticnet_uri = mlflow.get_artifact_uri("model")
```

---

### 4. √âvaluer et appliquer les seuils avec mlflow.evaluate

Ajoute ce bloc :

```python
mlflow.evaluate(
    model=elasticnet_uri,
    data=test,
    targets="quality",
    model_type="regressor",
    validation_thresholds=thresholds
)
```

---

## Position exacte dans ton script

| Partie | Emplacement |
|:------:|:------------|
| Importer `MetricThreshold` | Partie imports |
| D√©finir seuils | Apr√®s logging ElasticNet et Dummy |
| Obtenir ElasticNet URI | Apr√®s entra√Ænement et logging ElasticNet |
| √âvaluer avec mlflow.evaluate | Juste apr√®s d√©finition des seuils |

---

# Code ajout√© apr√®s ce Bloc

```python
from mlflow.models import MetricThreshold

# D√©finir un seuil sur la m√©trique "rmse"
thresholds = {
    "rmse": MetricThreshold(
        threshold=dummy_rmse,
        min_absolute_change=0.1,
        min_relative_change=0.05,
        greater_is_better=False
    )
}

# Obtenir l'URI du mod√®le ElasticNet
elasticnet_uri = mlflow.get_artifact_uri("model")

# √âvaluer le mod√®le ElasticNet avec les seuils d√©finis
mlflow.evaluate(
    model=elasticnet_uri,
    data=test,
    targets="quality",
    model_type="regressor",
    validation_thresholds=thresholds
)
```

---

# R√©sum√© du Bloc 22

| √âtape | O√π l'ajouter | Pourquoi |
|:-----:|:------------:|:--------:|
| Importer MetricThreshold | En haut du fichier | Pouvoir d√©finir des r√®gles d'acceptation |
| D√©finir des seuils sur RMSE | Apr√®s log ElasticNet/Dummy | V√©rifier que ElasticNet est utile |
| √âvaluer avec mlflow.evaluate | Fin du script | Automatiser la validation des mod√®les |

---

# Ce que tu as maintenant dans ton projet

- Tu n'as **plus besoin d'inspecter manuellement** si ElasticNet est meilleur :  
Le script **refusera automatiquement** les mod√®les qui **ne surpassent pas** le mod√®le de base Dummy selon les crit√®res d√©finis.

















# Bloc 23 ‚Äì Ajout 23 : G√©n√©rer un graphique de comparaison pr√©dictions vs cibles et l‚Äôenregistrer dans MLflow

---

## Objectif de ce bloc

- **Cr√©er une figure** comparant visuellement les **pr√©dictions** √† la **r√©alit√©** (les vraies valeurs cibles).
- **Sauvegarder automatiquement** cette figure comme **artefact** attach√© au run dans MLflow.

---

## Instructions d√©taill√©es

---

### 1. Importer `matplotlib.pyplot`

Ajoute cette ligne **dans la section imports** si ce n‚Äôest pas encore fait :

```python
import matplotlib.pyplot as plt
```

---

### 2. Cr√©er une fonction pour g√©n√©rer le graphique

Ajoute cette fonction **juste avant la fin du script**, **avant `mlflow.end_run()`** :

```python
def plot_predictions_vs_targets(predictions, targets, save_path="scatter_plot.png"):
    plt.figure(figsize=(8, 6))
    plt.scatter(targets, predictions, alpha=0.5)
    plt.plot([targets.min(), targets.max()], [targets.min(), targets.max()], 'r--')
    plt.xlabel("Valeurs r√©elles (Target)")
    plt.ylabel("Pr√©dictions")
    plt.title("Comparaison : Pr√©dictions vs Cibles")
    plt.grid(True)
    plt.savefig(save_path)
    plt.close()
```

---

### 3. Appeler la fonction pour cr√©er le graphique

Ajoute ce bloc **imm√©diatement apr√®s avoir √©valu√© ElasticNet sur `test_x` et obtenu `predicted_qualities`** :

```python
# G√©n√©rer le graphique scatter plot
plot_predictions_vs_targets(predicted_qualities, test_y.values.flatten())
```

---

### 4. Logger le graphique comme artefact dans MLflow

Ajoute juste apr√®s avoir g√©n√©r√© le fichier `scatter_plot.png` :

```python
mlflow.log_artifact("scatter_plot.png", artifact_path="plots")
```

---

## Position exacte dans ton script

| Partie | Emplacement pr√©cis |
|:------:|:--------------------|
| Importer matplotlib | Dans la partie imports |
| Fonction plot_predictions_vs_targets | Avant `mlflow.end_run()` |
| G√©n√©ration du graphique | Apr√®s pr√©dictions ElasticNet |
| Log du graphique dans MLflow | Juste apr√®s avoir g√©n√©r√© le PNG |

---

# Code ajout√© dans ce Bloc

```python
import matplotlib.pyplot as plt

def plot_predictions_vs_targets(predictions, targets, save_path="scatter_plot.png"):
    plt.figure(figsize=(8, 6))
    plt.scatter(targets, predictions, alpha=0.5)
    plt.plot([targets.min(), targets.max()], [targets.min(), targets.max()], 'r--')
    plt.xlabel("Valeurs r√©elles (Target)")
    plt.ylabel("Pr√©dictions")
    plt.title("Comparaison : Pr√©dictions vs Cibles")
    plt.grid(True)
    plt.savefig(save_path)
    plt.close()

# Apr√®s pr√©dictions ElasticNet
plot_predictions_vs_targets(predicted_qualities, test_y.values.flatten())

# Log du graphique
mlflow.log_artifact("scatter_plot.png", artifact_path="plots")
```

---

# R√©sum√© du Bloc 23

| √âtape | O√π l'ajouter | Pourquoi |
|:-----:|:------------:|:--------:|
| Importer matplotlib | En haut du script | Pouvoir cr√©er des figures |
| D√©finir fonction de scatter plot | Fin du script | G√©n√©rer un graphique clair |
| G√©n√©rer graphique apr√®s pr√©dictions | Juste apr√®s pr√©diction | Visualiser la qualit√© du mod√®le |
| Log du PNG dans MLflow | Apr√®s g√©n√©ration | Sauvegarder la figure dans les artefacts |

---

# Ce que ton script sait faire apr√®s ce Bloc

- Il **entra√Æne** ElasticNet.
- Il **√©value** et **compare** les performances contre Dummy.
- Il **applique des r√®gles de validation automatique** avec des seuils.
- Il **g√©n√®re une figure** et **la sauvegarde** automatiquement dans MLflow comme artefact.
















# Bloc 24 ‚Äì Ajout 24 : Inscription automatique du mod√®le ElasticNet dans le registre de mod√®les MLflow

---

## Objectif de ce bloc

- **Enregistrer** automatiquement notre mod√®le ElasticNet dans le **registre de mod√®les MLflow**.
- Permettre **une meilleure gestion** : historique des versions, promotion en staging/production, etc.

---

## Instructions d√©taill√©es

---

### 1. Modifier `mlflow.pyfunc.log_model`

Actuellement, tu utilises une commande comme :

```python
mlflow.pyfunc.log_model(
    artifact_path="sklear_mlflow_pyfunc",
    python_model=SklearnWrapper(),
    artifacts=artifacts,
    code_path=["main.py"],
    conda_env=conda_env
)
```

Tu dois **ajouter** le param√®tre suivant :

```python
registered_model_name="ElasticNet-Production-Model"
```

Cela devient :

```python
mlflow.pyfunc.log_model(
    artifact_path="sklear_mlflow_pyfunc",
    python_model=SklearnWrapper(),
    artifacts=artifacts,
    code_path=["main.py"],
    conda_env=conda_env,
    registered_model_name="ElasticNet-Production-Model"
)
```

---

### 2. O√π faire cette modification

| Partie | Emplacement pr√©cis |
|:------:|:--------------------|
| Log du mod√®le pyfunc | Lors de l'appel `mlflow.pyfunc.log_model` |

**Attention** : Ne pas confondre avec `mlflow.sklearn.log_model`, ici on est sur `mlflow.pyfunc.log_model`.

---

### 3. R√©sultat attendu

- Le mod√®le sera automatiquement enregistr√© dans **le registre de mod√®les** MLflow.
- Tu verras ton mod√®le dans l'interface web, dans **"Models"** avec le nom **ElasticNet-Production-Model**.

---

# Code ajout√© dans ce Bloc

Version corrig√©e :

```python
mlflow.pyfunc.log_model(
    artifact_path="sklear_mlflow_pyfunc",
    python_model=SklearnWrapper(),
    artifacts=artifacts,
    code_path=["main.py"],
    conda_env=conda_env,
    registered_model_name="ElasticNet-Production-Model"
)
```

---

# R√©sum√© du Bloc 24

| √âtape | O√π l'ajouter | Pourquoi |
|:-----:|:------------:|:--------:|
| Ajouter `registered_model_name` | Dans l'appel `log_model` | Inscrire le mod√®le ElasticNet dans MLflow Model Registry |

---

# Ce que ton script sait faire apr√®s ce Bloc

- Il **entra√Æne** un mod√®le.
- Il **g√©n√®re** un graphique d'√©valuation.
- Il **valide** automatiquement les performances.
- Il **log** tout dans MLflow (params, m√©triques, artefacts).
- Il **inscrit** automatiquement le mod√®le dans **Model Registry** pr√™t pour √™tre promu.











# Bloc 25 ‚Äì Ajout 25 : Promotion automatique du mod√®le dans le registre MLflow (Staging ou Production)

---

## Objectif de ce bloc

- **Automatiser** la promotion du mod√®le dans **Model Registry** selon sa performance.
- Si le **RMSE** est **inf√©rieur** √† un seuil fix√© (par exemple 0.6), le mod√®le est directement promu vers **Staging** ou **Production**.

---

## Instructions d√©taill√©es

---

### 1. Importer les biblioth√®ques n√©cessaires pour manipuler le registre

Ajoute **en haut de ton script** dans la section importations, √† c√¥t√© de `import mlflow`, **cette ligne** :

```python
from mlflow import MlflowClient
```

**O√π** :  
Juste apr√®s les autres imports MLflow (`import mlflow`, `import mlflow.sklearn`, etc.)

---

### 2. D√©finir un seuil de validation

Juste **apr√®s** avoir √©valu√© ton mod√®le avec `eval_metrics`, ajoute :

```python
SEUIL_RMSE = 0.6
```

**O√π** :  
Apr√®s ce bloc existant :

```python
rmse, mae, r2 = eval_metrics(test_y, predicted_qualities)
```

---

### 3. Ajouter la logique de promotion

Apr√®s avoir **logg√©** ton mod√®le dans le registre avec `mlflow.pyfunc.log_model(...)`, **imm√©diatement apr√®s**, ajoute ce bloc :

```python
client = MlflowClient()
run_id = mlflow.active_run().info.run_id

if rmse < SEUIL_RMSE:
    # Promotion vers "Staging"
    client.transition_model_version_stage(
        name="ElasticNet-Production-Model",
        version=1,
        stage="Staging"
    )
    print(f"Mod√®le promu vers Staging car RMSE ({rmse:.3f}) est inf√©rieur √† {SEUIL_RMSE}")
else:
    print(f"Mod√®le NON promu (RMSE = {rmse:.3f} >= {SEUIL_RMSE})")
```

---

### 4. Attention sur le **version number**

Ici on a mis :

```python
version=1
```

**Important** :
- Cela suppose que c'est le premier mod√®le inscrit.
- Si plusieurs mod√®les sont enregistr√©s, il faudrait r√©cup√©rer dynamiquement la bonne version.  
  (Je te donnerai la m√©thode plus avanc√©e si besoin.)

---

## Code ajout√© dans ce Bloc

Version ajout√©e :

```python
from mlflow import MlflowClient

SEUIL_RMSE = 0.6

client = MlflowClient()
run_id = mlflow.active_run().info.run_id

if rmse < SEUIL_RMSE:
    client.transition_model_version_stage(
        name="ElasticNet-Production-Model",
        version=1,
        stage="Staging"
    )
    print(f"Mod√®le promu vers Staging car RMSE ({rmse:.3f}) est inf√©rieur √† {SEUIL_RMSE}")
else:
    print(f"Mod√®le NON promu (RMSE = {rmse:.3f} >= {SEUIL_RMSE})")
```

---

## R√©sultat attendu

- Si ton mod√®le est **meilleur que le seuil** (par exemple RMSE < 0.6), **MLflow passe directement** la version du mod√®le dans l‚Äô√©tat **Staging**.
- Sinon, le mod√®le reste en **None** (non promu).

---

# R√©sum√© du Bloc 25

| √âtape | O√π l'ajouter | Pourquoi |
|:-----:|:------------:|:--------:|
| Importer `MlflowClient` | En haut, avec les imports MLflow | Permet d'interagir avec Model Registry |
| D√©finir `SEUIL_RMSE` | Apr√®s l‚Äô√©valuation `eval_metrics` | Fixe un seuil de promotion automatique |
| Logique de promotion | Apr√®s `log_model` | Passe automatiquement en **Staging** si le mod√®le est performant |

---

# Ce que ton script sait faire apr√®s ce Bloc

- Il **entra√Æne** un mod√®le ElasticNet.
- Il **√©value** les performances du mod√®le.
- Il **logge** tout (params, m√©triques, mod√®le).
- Il **inscrit** automatiquement le mod√®le dans le registre MLflow.
- Il **promeut** automatiquement le mod√®le si les performances sont au niveau attendu.



